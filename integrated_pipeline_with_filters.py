# integrated_pipeline_with_filters.py - å¸¦ç­›é€‰ç³»ç»Ÿçš„æ™ºèƒ½å²—ä½å¤„ç†æµæ°´çº¿ï¼ˆä¿®å¤ç‰ˆï¼‰
"""
ğŸ¯ åˆ†å±‚ç­›é€‰æ™ºèƒ½å²—ä½å¤„ç†æµæ°´çº¿ - ä¿®å¤ç‰ˆ

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ğŸ“Š åˆ†å±‚ç­›é€‰ç³»ç»Ÿï¼š
   - æ­¥éª¤2åï¼šåŸºç¡€ç­›é€‰ï¼ˆç¡¬æ€§æ¡ä»¶è¿‡æ»¤ï¼‰ 
   - æ­¥éª¤3åï¼šé«˜çº§ç­›é€‰ï¼ˆæ™ºèƒ½è¯„åˆ†æ’åºï¼‰
2. ä¼˜åŒ–Notionå­—æ®µç»“æ„ï¼ˆ14ä¸ªæ ¸å¿ƒå­—æ®µï¼‰
3. å¢å¼ºç”¨æˆ·ä½“éªŒå’Œç­›é€‰åé¦ˆ
4. ğŸ”§ ä¿®å¤: æ­£ç¡®å¤„ç†"æ— æ–°æ•°æ®"æƒ…å†µï¼Œé¿å…è¯¯å¯¼æ€§çš„"å¤±è´¥"çŠ¶æ€

ç­›é€‰ç‰¹ç‚¹ï¼š
- ğŸš« ç¡¬æ€§ç­›é€‰ï¼šè‡ªåŠ¨è¿‡æ»¤æ¯•ä¸šæ—¶é—´ä¸ç¬¦ã€æ‹›è˜å·²æˆªæ­¢çš„å²—ä½
- ğŸ“ˆ æ™ºèƒ½è¯„åˆ†ï¼š0-100åˆ†ç»¼åˆè¯„åˆ†ï¼ŒåŸºäºç»éªŒåŒ¹é…ã€è–ªèµ„ã€å…¬å¸ç­‰çº§ç­‰
- â­ æ¨èç­‰çº§ï¼šğŸŒŸ å¼ºçƒˆæ¨èã€âœ… æ¨èã€âš ï¸ ä¸€èˆ¬ã€âŒ ä¸æ¨è
- ğŸ’ å­—æ®µä¼˜åŒ–ï¼šä»24ä¸ªå­—æ®µç²¾ç®€åˆ°14ä¸ªæ ¸å¿ƒå­—æ®µ

ä½¿ç”¨æ–¹å¼ï¼š
1. å®Œæ•´æµæ°´çº¿ï¼špython integrated_pipeline_with_filters.py
2. è°ƒè¯•æ¨¡å¼ï¼špython integrated_pipeline_with_filters.py --skip-crawl --log-level debug
3. ç¦ç”¨ç­›é€‰ï¼špython integrated_pipeline_with_filters.py --no-filters

ä¿®å¤å†…å®¹ï¼š
- âœ… ä¿®å¤äº† step2_deduplicate_and_filter_jobs æ–¹æ³•çš„è¿”å›é€»è¾‘
- âœ… æ·»åŠ äº†æ— æ•°æ®æ£€æŸ¥åˆ° step3_extract_and_advanced_filter æ–¹æ³•
- âœ… æ·»åŠ äº†æ— æ•°æ®æ£€æŸ¥åˆ° step4_write_to_notion_optimized æ–¹æ³•  
- âœ… æ”¹è¿›äº†ä¸»æµç¨‹ä¸­çš„æœ€ç»ˆæ¶ˆæ¯æ˜¾ç¤º
- âœ… åŒºåˆ†æœ‰æ•°æ®å’Œæ— æ•°æ®çš„æˆåŠŸæƒ…å†µ
"""
import asyncio
import json
import os
import glob
import argparse
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path

# å¯¼å…¥åŸæœ‰ç»„ä»¶
from src.logger_config import LogLevel, init_logger, get_logger, cleanup_logger
from src.data_snapshot import create_snapshot_manager
from src.enhanced_pipeline_fixed import EnhancedNotionJobPipelineWithLogging

# å¯¼å…¥ç»Ÿä¸€ç­›é€‰ç³»ç»Ÿ
from src.unified_filter_system import (
    UnifiedJobFilterManager, 
    get_unified_filter_config,
    create_optimized_notion_properties,
    get_optimized_notion_fields
)

try:
    from dotenv import load_dotenv
    # åŠ è½½ç¯å¢ƒå˜é‡
    for env_path in [".env", "../.env", "../../.env"]:
        if os.path.exists(env_path):
            load_dotenv(env_path)
            break
    
    from src.crawler_registry import crawler_registry
    from src.enhanced_extractor import EnhancedNotionExtractor
    from src.optimized_notion_writer import OptimizedNotionJobWriter
    DEPENDENCIES_OK = True
except ImportError as e:
    print(f"âŒ ä¾èµ–å¯¼å…¥å¤±è´¥: {e}")
    DEPENDENCIES_OK = False

class FilteredJobPipeline(EnhancedNotionJobPipelineWithLogging):
    """é›†æˆåˆ†å±‚ç­›é€‰ç³»ç»Ÿçš„å¢å¼ºç‰ˆæµæ°´çº¿ - ä¿®å¤ç‰ˆ"""
    
    def __init__(self, config=None, skip_crawl=False, data_file=None, 
                 skip_notion_load=False, notion_cache_file=None, enable_filters=True):
        """åˆå§‹åŒ–å¸¦ç­›é€‰çš„æµæ°´çº¿"""
        super().__init__(config, skip_crawl, data_file, skip_notion_load, notion_cache_file)
        
        self.enable_filters = enable_filters
        self.filter_manager = None
        
        # ç­›é€‰ç»Ÿè®¡
        self.filter_stats = {
            "input_jobs": 0,
            "basic_filtered": 0,
            "advanced_filtered": 0,
            "hard_rejected": 0,
            "soft_rejected": 0,
            "final_passed": 0,
            "recommended_jobs": 0,
            "not_recommended": 0
        }
        
        if enable_filters:
            self._init_filter_system()
        
        self.logger.debug("å¸¦ç­›é€‰çš„æµæ°´çº¿åˆå§‹åŒ–å®Œæˆ", {
            "enable_filters": enable_filters,
            "filter_manager_loaded": self.filter_manager is not None
        })
    
    def _init_filter_system(self):
        """åˆå§‹åŒ–ç­›é€‰ç³»ç»Ÿ"""
        try:
            # åŠ è½½ç­›é€‰é…ç½®
            filter_config = get_unified_filter_config()
            
            # å¯ä»¥ä»ç¯å¢ƒå˜é‡è¦†ç›–ç”¨æˆ·é…ç½®
            if os.getenv("USER_GRADUATION"):
                filter_config["basic"]["common"]["user_graduation"] = os.getenv("USER_GRADUATION")
            if os.getenv("USER_EXPERIENCE_YEARS"):
                filter_config["basic"]["common"]["user_experience_years"] = float(os.getenv("USER_EXPERIENCE_YEARS"))
            if os.getenv("TARGET_SALARY"):
                filter_config["basic"]["salary"]["target_salary"] = int(os.getenv("TARGET_SALARY"))
            
            self.filter_manager = UnifiedJobFilterManager(filter_config)
            
            self.logger.success("ç­›é€‰ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ", {
                "user_graduation": filter_config["basic"]["common"]["user_graduation"],
                "user_experience": filter_config["basic"]["common"]["user_experience_years"],
                "target_salary": filter_config["basic"]["salary"]["target_salary"],
                "basic_filters": len(self.filter_manager.basic_filters),
                "advanced_filters": len(self.filter_manager.advanced_filters)
            })
            
        except Exception as e:
            self.logger.error("ç­›é€‰ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥", {"error": str(e)}, e)
            self.filter_manager = None

    async def _post_deduplication_processing(self, deduplicated_jobs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡åå¤„ç†ï¼šæ·»åŠ åŸºç¡€ç­›é€‰é€»è¾‘"""
        
        # å¦‚æœç­›é€‰è¢«ç¦ç”¨ï¼Œç›´æ¥è¿”å›
        if not self.enable_filters or not self.filter_manager:
            return deduplicated_jobs
        
        self.logger.info("å¼€å§‹åŸºç¡€ç­›é€‰å¤„ç†", {
            "input_count": len(deduplicated_jobs),
            "filters_enabled": list(self.filter_manager.basic_filters.keys())
        })
        
        # ä¿å­˜åŸºç¡€ç­›é€‰å‰å¿«ç…§
        self.snapshot.capture("before_basic_filter", deduplicated_jobs, {
            "stage": "åŸºç¡€ç­›é€‰å‰",
            "job_count": len(deduplicated_jobs)
        })
        
        # æ‰§è¡ŒåŸºç¡€ç­›é€‰
        filtered_results = self.filter_manager.apply_basic_filters(deduplicated_jobs)
        
        input_count = len(deduplicated_jobs)
        passed_count = len(filtered_results)
        rejected_count = input_count - passed_count
        
        # æ›´æ–°ç­›é€‰ç»Ÿè®¡
        self.filter_stats.update({
            "input_jobs": input_count,
            "basic_filtered": passed_count,
            "hard_rejected": rejected_count
        })
        
        # ä¿å­˜åŸºç¡€ç­›é€‰åå¿«ç…§
        self.snapshot.capture("after_basic_filter", filtered_results, {
            "stage": "åŸºç¡€ç­›é€‰å",
            "passed_count": passed_count,
            "hard_rejected_count": rejected_count
        })
        
        if passed_count == 0:
            self.logger.info_no_data("åŸºç¡€ç­›é€‰å®Œæˆ - æ— å²—ä½é€šè¿‡ç­›é€‰")
        else:
            self.logger.success(f"åŸºç¡€ç­›é€‰å®Œæˆ - {passed_count}ä¸ªå²—ä½é€šè¿‡")
        
        return filtered_results
    
    def _generate_dedup_stats(self, deduplicated_jobs: List[Dict], final_jobs: List[Dict]) -> Dict[str, Any]:
        """ç”ŸæˆåŒ…å«ç­›é€‰ä¿¡æ¯çš„ç»Ÿè®¡"""
        base_stats = super()._generate_dedup_stats(deduplicated_jobs, final_jobs)
        
        # æ·»åŠ ç­›é€‰ç»Ÿè®¡
        if self.enable_filters:
            base_stats.update({
                "åŸºç¡€ç­›é€‰é€šè¿‡": self.filter_stats.get("basic_filtered", len(final_jobs)),
                "ç¡¬æ€§è¢«æ‹’": self.filter_stats.get("hard_rejected", 0),
                "é€šè¿‡ç‡": f"{(len(final_jobs)/len(self.raw_jobs)*100):.1f}%" if self.raw_jobs else "0%"
            })
        
        return base_stats
    
    def get_total_steps(self) -> int:
        """å­ç±»æœ‰5ä¸ªæ­¥éª¤"""
        return 5

    async def step3_extract_and_advanced_filter(self) -> bool:
        """æ­¥éª¤3: ä¿¡æ¯æå– + é«˜çº§ç­›é€‰ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        self.logger.step_start("ä¿¡æ¯æå– + é«˜çº§ç­›é€‰", 3, 5)
        
        # ğŸ”§ ä¿®å¤: æ·»åŠ æ— æ•°æ®æ£€æŸ¥
        if not self.deduplicated_jobs:
            self.logger.info("æ²¡æœ‰æ–°å²—ä½éœ€è¦æå–ï¼Œè·³è¿‡ä¿¡æ¯æå–æ­¥éª¤")
            self.logger.step_end("ä¿¡æ¯æå– + é«˜çº§ç­›é€‰", True, {
                "çŠ¶æ€": "è·³è¿‡(æ— æ–°æ•°æ®)",
                "åŸå› ": "æ‰€æœ‰å²—ä½å·²å­˜åœ¨äºæ•°æ®åº“æˆ–è¢«ç­›é€‰è¿‡æ»¤",
                "å»ºè®®": "é‡æ–°çˆ¬å–æˆ–ä½¿ç”¨ä¸åŒæœç´¢æ¡ä»¶è·å–æ–°å²—ä½",
                "æ•°æ®è´¨é‡": "âœ… é¿å…é‡å¤å¤„ç†"
            })
            # ç¡®ä¿ç»Ÿè®¡æ•°æ®æ­£ç¡®
            self.extracted_jobs = []
            self.stats["extracted"] = 0
            self.stats["failed"] = 0
            return True  # ğŸ”§ è¿”å›Trueè€Œä¸æ˜¯False
        
        try:
            # ç¬¬ä¸€é˜¶æ®µï¼šä¿¡æ¯æå–ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
            extraction_success = await self._extract_job_info()
            if not extraction_success:
                return False
            
            # ç¬¬äºŒé˜¶æ®µï¼šé«˜çº§ç­›é€‰å’Œä¼˜åŒ–å­—æ®µ
            if self.enable_filters and self.filter_manager and self.extracted_jobs:
                self.logger.info("å¼€å§‹é«˜çº§ç­›é€‰å¤„ç†", {
                    "input_count": len(self.extracted_jobs),
                    "filters_enabled": list(self.filter_manager.advanced_filters.keys())
                })
                
                # ä¿å­˜é«˜çº§ç­›é€‰å‰å¿«ç…§
                self.snapshot.capture("before_advanced_filter", self.extracted_jobs, {
                    "stage": "é«˜çº§ç­›é€‰å‰",
                    "job_count": len(self.extracted_jobs)
                })
                
                # æ‰§è¡Œé«˜çº§ç­›é€‰ï¼ˆæ™ºèƒ½è¯„åˆ†ï¼‰
                advanced_results = self.filter_manager.apply_advanced_filters(self.extracted_jobs)

                input_count = len(self.extracted_jobs)
                scored_count = len(advanced_results)
                
                # æ›´æ–°ç»Ÿè®¡
                self.filter_stats["advanced_filtered"] = scored_count
                self.filter_stats["soft_rejected"] = 0
                
                # ç»Ÿè®¡æ¨èç­‰çº§
                for job in advanced_results:
                    if "æ¨è" in job.get("æ¨èç­‰çº§", ""):
                        self.filter_stats["recommended_jobs"] += 1
                    else:
                        self.filter_stats["not_recommended"] += 1
                
                # ä¼˜åŒ–å­—æ®µç»“æ„ï¼ˆ14ä¸ªæ ¸å¿ƒå­—æ®µï¼‰
                self.extracted_jobs = self._optimize_job_fields(advanced_results)
                
                # ä¿å­˜é«˜çº§ç­›é€‰åå¿«ç…§
                self.snapshot.capture("after_advanced_filter", self.extracted_jobs, {
                    "stage": "é«˜çº§ç­›é€‰å",
                    "scored_count": scored_count,
                    "soft_rejected_count": 0,
                    "recommended_count": self.filter_stats["recommended_jobs"]
                })
                
                step_stats = {
                    "æå–æˆåŠŸ": len(self.extracted_jobs),
                    "æ¨èå²—ä½": self.filter_stats["recommended_jobs"],
                    "ä¸€èˆ¬å²—ä½": self.filter_stats["not_recommended"],
                    "è½¯æ‹’ç»": 0,
                    "æ¨èç‡": f"{(self.filter_stats['recommended_jobs']/len(self.extracted_jobs)*100):.1f}%" if self.extracted_jobs else "0%"
                }
                
                self.logger.success("é«˜çº§ç­›é€‰å®Œæˆ", step_stats)
                self.logger.step_end("ä¿¡æ¯æå– + é«˜çº§ç­›é€‰", True, step_stats)
                
            else:
                self.logger.info("ç­›é€‰åŠŸèƒ½å·²ç¦ç”¨ï¼Œè·³è¿‡é«˜çº§ç­›é€‰")
                # ä»ç„¶éœ€è¦ä¼˜åŒ–å­—æ®µ
                self.extracted_jobs = self._optimize_job_fields(self.extracted_jobs)
                self.logger.step_end("ä¿¡æ¯æå– + é«˜çº§ç­›é€‰", True, {
                    "æå–æˆåŠŸ": len(self.extracted_jobs),
                    "å­—æ®µä¼˜åŒ–": "å®Œæˆï¼ˆ14ä¸ªæ ¸å¿ƒå­—æ®µï¼‰"
                })
            
            return True
            
        except Exception as e:
            self.logger.error("ä¿¡æ¯æå–+é«˜çº§ç­›é€‰å¤±è´¥", {"error": str(e)}, e)
            self.logger.step_end("ä¿¡æ¯æå– + é«˜çº§ç­›é€‰", False, {"é”™è¯¯": str(e)})
            return False
    
    async def step4_write_to_notion_optimized(self) -> bool:
        """æ­¥éª¤4: å†™å…¥ä¼˜åŒ–çš„Notionæ•°æ®åº“ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        self.logger.step_start("å†™å…¥Notionæ•°æ®åº“", 4, 5)
        
        # ğŸ”§ ä¿®å¤: æ·»åŠ æ— æ•°æ®æ£€æŸ¥
        if not self.extracted_jobs:
            self.logger.info("æ²¡æœ‰æ–°å²—ä½éœ€è¦å†™å…¥ï¼Œè·³è¿‡å†™å…¥æ­¥éª¤")
            self.logger.step_end("å†™å…¥Notionæ•°æ®åº“", True, {
                "çŠ¶æ€": "è·³è¿‡(æ— æ–°æ•°æ®)",
                "åŸå› ": "æ²¡æœ‰é€šè¿‡ç­›é€‰çš„æ–°å²—ä½",
                "æ•°æ®åº“çŠ¶æ€": "âœ… ä¿æŒæœ€æ–°",
                "å»ºè®®": "è°ƒæ•´ç­›é€‰æ¡ä»¶æˆ–è·å–æ–°æ•°æ®"
            })
            return True  # ğŸ”§ è¿”å›Trueè€Œä¸æ˜¯False
        
        try:
            # æ‰¹é‡å†™å…¥ä¼˜åŒ–çš„å²—ä½æ•°æ®
            write_stats = await self._batch_write_optimized_jobs(self.extracted_jobs)
            
            # æ›´æ–°æœ€ç»ˆç»Ÿè®¡
            self.filter_stats["final_passed"] = write_stats["success"]
            
            step_stats = {
                "å†™å…¥æˆåŠŸ": write_stats["success"],
                "å†™å…¥å¤±è´¥": write_stats["failed"],
                "æ¨èå²—ä½": write_stats.get("strongly_recommended", 0) + write_stats.get("recommended", 0),
                "ä¸€èˆ¬å²—ä½": write_stats.get("considerable", 0) + write_stats.get("not_recommended", 0),
                "æˆåŠŸç‡": f"{(write_stats['success']/write_stats['total']*100):.1f}%" if write_stats['total'] > 0 else "0%"
            }
            
            if write_stats["success"] > 0:
                self.logger.success("Notionå†™å…¥å®Œæˆ", step_stats)
            else:
                self.logger.warning("æ²¡æœ‰å²—ä½æˆåŠŸå†™å…¥Notion", step_stats)
            
            self.logger.step_end("å†™å…¥Notionæ•°æ®åº“", True, step_stats)
            return True
            
        except Exception as e:
            self.logger.error("Notionå†™å…¥å¤±è´¥", {"error": str(e)}, e)
            self.logger.step_end("å†™å…¥Notionæ•°æ®åº“", False, {"é”™è¯¯": str(e)})
            return False
    
    async def step5_generate_final_report(self) -> bool:
        """æ­¥éª¤5: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        self.logger.step_start("ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š", 5, 5)
        
        try:
            # ä¿å­˜å¿«ç…§æ‘˜è¦
            self.snapshot.save_summary()
            
            self.logger.step_end("ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š", True, {
                "å¿«ç…§æ•°é‡": len(self.snapshot.snapshots),
                "æŠ¥å‘ŠçŠ¶æ€": "å·²ç”Ÿæˆ"
            })
            
            return True
            
        except Exception as e:
            self.logger.error("ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šå¤±è´¥", {"error": str(e)}, e)
            self.logger.step_end("ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š", False, {"é”™è¯¯": str(e)})
            # å³ä½¿å¤±è´¥ä¹Ÿä¸å½±å“æ•´ä½“æµæ°´çº¿
            return True

    async def run_filtered_pipeline(self) -> bool:
        """è¿è¡Œå¸¦ç­›é€‰çš„å®Œæ•´æµæ°´çº¿ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        self.logger.info("ğŸš€ å¯åŠ¨å¸¦ç­›é€‰çš„æ™ºèƒ½å²—ä½å¤„ç†æµæ°´çº¿ï¼ˆä¿®å¤ç‰ˆï¼‰")
        
        if self.enable_filters:
            if self.filter_manager:
                self.logger.info("âœ… ç­›é€‰ç³»ç»Ÿå·²å¯ç”¨")
            else:
                self.logger.warning("âš ï¸ ç­›é€‰ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨åŸå§‹æµæ°´çº¿")
                self.enable_filters = False
        else:
            self.logger.info("â„¹ï¸ ç­›é€‰ç³»ç»Ÿå·²ç¦ç”¨ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰")
        
        try:
            # æ­¥éª¤1: çˆ¬å–æˆ–åŠ è½½æ•°æ®
            if not await self.step1_load_or_crawl_jobs():
                return False
            
            # æ­¥éª¤2: å»é‡ + åŸºç¡€ç­›é€‰
            if not await self.step2_deduplicate_jobs():
                return False
            
            # æ­¥éª¤3: ä¿¡æ¯æå– + é«˜çº§ç­›é€‰
            if not await self.step3_extract_and_advanced_filter():
                return False
            
            # æ­¥éª¤4: å†™å…¥ä¼˜åŒ–çš„Notionæ•°æ®åº“
            if not await self.step4_write_to_notion_optimized():
                return False
            
            # æ­¥éª¤5: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            await self.step5_generate_final_report()
            
            # ğŸ”§ ä¿®å¤: åŒºåˆ†ä¸åŒçš„æˆåŠŸæƒ…å†µ
            if self.extracted_jobs and len(self.extracted_jobs) > 0:
                self.logger.success("ğŸ‰ å¸¦ç­›é€‰çš„æµæ°´çº¿æ‰§è¡Œå®Œæˆ", self._get_final_stats())
                self._show_usage_suggestions()
            else:
                self.logger.success_no_data("âœ… æµæ°´çº¿æ‰§è¡ŒæˆåŠŸ - æ•°æ®åº“å·²æ˜¯æœ€æ–°çŠ¶æ€", self._get_final_stats())
                self.logger.info("â„¹ï¸ è¿™æ˜¯æ­£å¸¸æƒ…å†µï¼Œè¯´æ˜å»é‡å’Œç­›é€‰ç³»ç»Ÿå·¥ä½œæ­£å¸¸")
                self._show_optimization_suggestions()
            
            return True
            
        except Exception as e:
            self.logger.error("æµæ°´çº¿æ‰§è¡Œå¤±è´¥", {"error": str(e)}, e)
            return False
    
    def _get_final_stats(self) -> Dict[str, Any]:
        """è·å–æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        return {
            "æ€»çˆ¬å–å²—ä½": len(self.raw_jobs) if self.raw_jobs else 0,
            "å»é‡åå²—ä½": self.filter_stats.get("input_jobs", 0),
            "åŸºç¡€ç­›é€‰é€šè¿‡": self.filter_stats.get("basic_filtered", 0),
            "ä¿¡æ¯æå–æˆåŠŸ": len(self.extracted_jobs) if self.extracted_jobs else 0,
            "æ¨èå²—ä½æ•°": self.filter_stats.get("recommended_jobs", 0),
            "æœ€ç»ˆå†™å…¥": self.filter_stats.get("final_passed", 0),
            "æ•´ä½“é€šè¿‡ç‡": f"{(self.filter_stats.get('final_passed', 0) / max(len(self.raw_jobs) if self.raw_jobs else 1, 1) * 100):.1f}%"
        }
    
    def _show_usage_suggestions(self):
        """æ˜¾ç¤ºä½¿ç”¨å»ºè®®"""
        self.logger.info("ğŸ“± Notionä½¿ç”¨å»ºè®®:")
        self.logger.info("   1. æŒ‰\"ç»¼åˆè¯„åˆ†\"é™åºæ’åˆ—æŸ¥çœ‹æœ€ä¼˜å²—ä½")
        self.logger.info("   2. ç­›é€‰\"æ¨èç­‰çº§\" = \"ğŸŒŸ å¼ºçƒˆæ¨è\"æŸ¥çœ‹é¡¶çº§å²—ä½")
        self.logger.info("   3. æŸ¥çœ‹\"ç»éªŒåŒ¹é…å»ºè®®\"äº†è§£ç”³è¯·ç­–ç•¥")
        self.logger.info("   4. å…³æ³¨\"æ‹›è˜æˆªæ­¢æ—¥æœŸ_æ ‡å‡†åŒ–\"åˆç†å®‰æ’æ—¶é—´")
    
    def _show_optimization_suggestions(self):
        """æ˜¾ç¤ºä¼˜åŒ–å»ºè®®"""
        self.logger.info("ğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        self.logger.info("   1. å°è¯•ä¸åŒçš„æœç´¢å…³é”®è¯")
        self.logger.info("   2. è°ƒæ•´ç­›é€‰æ¡ä»¶ï¼ˆUSER_GRADUATION, TARGET_SALARYç­‰ï¼‰")
        self.logger.info("   3. æ‰©å¤§æœç´¢åœ°åŒºèŒƒå›´")
        self.logger.info("   4. æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ç®€å†åŒ¹é…æ¡ä»¶")
    
    async def _extract_job_info(self) -> bool:
        """ä¿¡æ¯æå–å¤„ç†"""
        if not self.deduplicated_jobs:
            self.logger.warning("æ²¡æœ‰å»é‡åçš„å²—ä½æ•°æ®")
            return False
        
        try:
            self.logger.info(f"å¼€å§‹æå– {len(self.deduplicated_jobs)} ä¸ªå²—ä½çš„è¯¦ç»†ä¿¡æ¯")
            
            extractor = EnhancedNotionExtractor()
            extracted_jobs = []
            failed_count = 0
            
            for i, job in enumerate(self.deduplicated_jobs, 1):
                try:
                    self.logger.debug(f"æå–ç¬¬ {i}/{len(self.deduplicated_jobs)} ä¸ªå²—ä½", {
                        "job_title": job.get("å²—ä½åç§°", "N/A"),
                        "company": job.get("å…¬å¸åç§°", "N/A")
                    })

                    # âœ… æå–å¿…éœ€çš„å‚æ•°
                    html = job.get('html', '')
                    url = job.get('å²—ä½é“¾æ¥', '')
                
                    if not html:
                        self.logger.warning(f"å²—ä½ {i} æ²¡æœ‰HTMLå†…å®¹", {
                            "job_title": job.get('å²—ä½åç§°', 'N/A'),
                            "company": job.get('å…¬å¸åç§°', 'N/A')
                        })
                        failed_count += 1
                        continue
                
                    # âœ… ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•è°ƒç”¨
                    extracted_job = await extractor.extract_for_notion_enhanced(html, url, job)
                    
                    if extracted_job:
                        extracted_jobs.append(extracted_job)
                        self.logger.debug(f"âœ… ç¬¬ {i} ä¸ªå²—ä½æå–æˆåŠŸ")
                    else:
                        failed_count += 1
                        self.logger.warning(f"âŒ ç¬¬ {i} ä¸ªå²—ä½æå–å¤±è´¥")
                        
                except Exception as e:
                    self.logger.warning(f"ç¬¬ {i} ä¸ªå²—ä½æå–å¼‚å¸¸", {
                        "job_title": job.get("å²—ä½åç§°", "N/A"),
                        "error": str(e)
                    }, e)
                    failed_count += 1
                
                # APIè°ƒç”¨é—´éš”
                await asyncio.sleep(1.5)
            
            self.extracted_jobs = extracted_jobs
            self.stats["extracted"] = len(extracted_jobs)
            self.stats["failed"] = failed_count
            
            # ä¿å­˜æå–ç»“æœå¿«ç…§
            if extracted_jobs:
                self.snapshot.capture("extraction_output", extracted_jobs, {
                    "stage": "ä¿¡æ¯æå–è¾“å‡º",
                    "success_count": len(extracted_jobs),
                    "failed_count": failed_count
                })
            
            return len(extracted_jobs) > 0
            
        except Exception as e:
            self.logger.error("ä¿¡æ¯æå–å¤±è´¥", {"error": str(e)}, e)
            return False
    
    def _optimize_job_fields(self, jobs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä¼˜åŒ–å²—ä½å­—æ®µç»“æ„ï¼ˆ16ä¸ªæ ¸å¿ƒå­—æ®µï¼‰"""
        self.logger.debug("å¼€å§‹ä¼˜åŒ–å²—ä½å­—æ®µç»“æ„", {
            "input_count": len(jobs),
            "target_fields": 16
        })
        
        optimized_jobs = []
        
        for job in jobs:
            optimized_job = {}
            
            # 1. æ ¸å¿ƒä¿¡æ¯ (6ä¸ªå­—æ®µ)
            core_fields = ["å²—ä½åç§°", "å…¬å¸åç§°", "è–ªèµ„", "å·¥ä½œåœ°ç‚¹", "å²—ä½æè¿°", "å²—ä½é“¾æ¥"]
            for field in core_fields:
                optimized_job[field] = job.get(field, "")
            
            # 2. ç­›é€‰è¯„åˆ† (2ä¸ªå­—æ®µ) - æ¥è‡ªç­›é€‰ç³»ç»Ÿ
            optimized_job["ç»¼åˆè¯„åˆ†"] = job.get("ç»¼åˆè¯„åˆ†", 0)
            optimized_job["æ¨èç­‰çº§"] = job.get("æ¨èç­‰çº§", "âš ï¸ æœªè¯„åˆ†")
            
            # 3. åŒ¹é…åˆ†æ (2ä¸ªå­—æ®µ)
            optimized_job["ç»éªŒè¦æ±‚"] = job.get("ç»éªŒè¦æ±‚", "")
            
            # ç”Ÿæˆç»éªŒåŒ¹é…å»ºè®®
            experience_suggestion = self._generate_experience_suggestion(job)
            optimized_job["ç»éªŒåŒ¹é…å»ºè®®"] = experience_suggestion
            
            # 4. æ—¶é—´ä¿¡æ¯ (2ä¸ªå­—æ®µ) - ä½¿ç”¨æ ‡å‡†åŒ–ç‰ˆæœ¬
            optimized_job["æ¯•ä¸šæ—¶é—´è¦æ±‚_æ ‡å‡†åŒ–"] = (
                job.get("æ¯•ä¸šæ—¶é—´è¦æ±‚_æ ‡å‡†åŒ–") or 
                job.get("æ¯•ä¸šæ—¶é—´è¦æ±‚", "")
            )
            optimized_job["æ‹›è˜æˆªæ­¢æ—¥æœŸ_æ ‡å‡†åŒ–"] = (
                job.get("æ‹›è˜æˆªæ­¢æ—¥æœŸ_æ ‡å‡†åŒ–") or 
                job.get("æ‹›è˜æˆªæ­¢æ—¥æœŸ", "")
            )
            
            # 5. è¡¥å……ä¿¡æ¯ (2ä¸ªå­—æ®µ)  
            optimized_job["å‘å¸ƒå¹³å°"] = job.get("å‘å¸ƒå¹³å°", "")
            optimized_job["æ‹›å‹Ÿæ–¹å‘"] = job.get("æ‹›å‹Ÿæ–¹å‘", "")
            
            # 6. HRå’ŒæŠ“å–ä¿¡æ¯ (2ä¸ªå­—æ®µ)
            optimized_job["HRæ´»è·ƒåº¦"] = job.get("HRæ´»è·ƒåº¦", "")
            optimized_job["é¡µé¢æŠ“å–æ—¶é—´"] = job.get("é¡µé¢æŠ“å–æ—¶é—´", "")
            
            optimized_jobs.append(optimized_job)
        
        self.logger.success("å²—ä½å­—æ®µä¼˜åŒ–å®Œæˆ", {
            "input_count": len(jobs),
            "output_count": len(optimized_jobs),
            "fields_per_job": 16
        })
        
        return optimized_jobs
    
    def _generate_experience_suggestion(self, job: Dict[str, Any]) -> str:
        """ç”Ÿæˆç»éªŒåŒ¹é…å»ºè®®"""
        experience_req = job.get("ç»éªŒè¦æ±‚", "").lower()
        score = job.get("ç»¼åˆè¯„åˆ†", 0)
        
        if "åº”å±Š" in experience_req or "æ— ç»éªŒ" in experience_req:
            if score >= 80:
                return "ğŸ¯ åº”å±Šç”Ÿå²—ä½ï¼Œå¼ºçƒˆå»ºè®®ç”³è¯·"
            elif score >= 60:
                return "âœ… åº”å±Šç”Ÿå²—ä½ï¼Œå»ºè®®ç”³è¯·"
            else:
                return "âš ï¸ åº”å±Šç”Ÿå²—ä½ï¼Œä½†åŒ¹é…åº¦ä¸€èˆ¬"
        elif "1-3å¹´" in experience_req or "1å¹´" in experience_req:
            if score >= 80:
                return "ğŸŒŸ ç»éªŒè¦æ±‚åŒ¹é…ï¼Œä¼˜å…ˆç”³è¯·"
            elif score >= 60:
                return "âœ… ç»éªŒåŸºæœ¬åŒ¹é…ï¼Œå»ºè®®ç”³è¯·"
            else:
                return "âš ï¸ å¯å°è¯•ç”³è¯·ï¼Œå‡†å¤‡å……åˆ†é¢è¯•"
        elif "3-5å¹´" in experience_req:
            return "ğŸ”„ éœ€è¦æ›´å¤šç»éªŒï¼Œå¯è€ƒè™‘è·³æ§½æ—¶ç”³è¯·"
        else:
            if score >= 70:
                return "ğŸ’¡ å²—ä½ä¼˜è´¨ï¼Œå€¼å¾—å°è¯•ç”³è¯·"
            else:
                return "ğŸ“‹ äº†è§£å²—ä½è¦æ±‚ï¼Œè¯„ä¼°ç”³è¯·å¯è¡Œæ€§"
    
    # async def _ensure_optimized_notion_properties(self) -> bool:
    #     """ç¡®ä¿Notionæ•°æ®åº“æœ‰æ­£ç¡®çš„å±æ€§é…ç½®"""
    #     try:
            
    #         # è¿™é‡Œå¯ä»¥æ·»åŠ æ•°æ®åº“å±æ€§æ›´æ–°é€»è¾‘
    #         # ç›®å‰å‡è®¾æ•°æ®åº“å·²ç»æ­£ç¡®é…ç½®
    #         self.logger.debug("Notionæ•°æ®åº“å±æ€§é…ç½®æ£€æŸ¥å®Œæˆ", {
    #             "properties_count": len(optimized_properties),
    #             "core_fields": 14
    #         })
            
    #         return True
            
    #     except Exception as e:
    #         self.logger.error("Notionæ•°æ®åº“å±æ€§é…ç½®å¤±è´¥", {"error": str(e)}, e)
    #         return False

    async def _batch_write_optimized_jobs(self, jobs_data: List[Dict[str, Any]], max_concurrent: int = 3) -> Dict[str, int]:
        """æ‰¹é‡å†™å…¥ä¼˜åŒ–çš„å²—ä½æ•°æ®"""
        self.logger.info(f"å¼€å§‹æ‰¹é‡å†™å…¥ {len(jobs_data)} ä¸ªå²—ä½åˆ°Notionï¼ˆ14å­—æ®µä¼˜åŒ–ç‰ˆï¼‰")
        
        try:
            # ç›´æ¥ä½¿ç”¨ç°æœ‰çš„OptimizedNotionJobWriter
            from src.optimized_notion_writer import OptimizedNotionJobWriter
            writer = OptimizedNotionJobWriter()
            
            # è°ƒç”¨ç°æœ‰çš„æ‰¹é‡å†™å…¥æ–¹æ³•
            stats = await writer.batch_write_jobs_optimized(jobs_data, max_concurrent)
            
            self.logger.success("æ‰¹é‡å†™å…¥å®Œæˆ", stats)
            return stats
            
        except Exception as e:
            self.logger.error("æ‰¹é‡å†™å…¥å¤±è´¥", {"error": str(e)}, e)
            # è¿”å›å¤±è´¥ç»Ÿè®¡ï¼Œæ ¼å¼ä¸OptimizedNotionJobWriterä¸€è‡´
            return {
                "total": len(jobs_data),
                "success": 0,
                "failed": len(jobs_data),
                "strongly_recommended": 0,
                "recommended": 0,
                "considerable": 0,
                "not_recommended": 0
            }
        

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='å¸¦ç­›é€‰ç³»ç»Ÿçš„æ™ºèƒ½å²—ä½å¤„ç†æµæ°´çº¿ï¼ˆä¿®å¤ç‰ˆï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ”§ ä¿®å¤ç‰ˆç‰¹ç‚¹ï¼š
  âœ… æ­£ç¡®å¤„ç†"æ— æ–°æ•°æ®"æƒ…å†µï¼Œé¿å…è¯¯å¯¼æ€§çš„"å¤±è´¥"çŠ¶æ€
  âœ… åŒºåˆ†"ç³»ç»Ÿé”™è¯¯"å’Œ"æ— æ–°æ•°æ®"
  âœ… æä¾›å‹å¥½çš„ç”¨æˆ·æç¤ºå’Œå»ºè®®

åŠŸèƒ½ç‰¹ç‚¹ï¼š
  ğŸ” åˆ†å±‚ç­›é€‰ï¼šåŸºç¡€ç­›é€‰ï¼ˆç¡¬æ€§æ¡ä»¶ï¼‰+ é«˜çº§ç­›é€‰ï¼ˆæ™ºèƒ½è¯„åˆ†ï¼‰
  ğŸ“Š æ™ºèƒ½è¯„åˆ†ï¼š0-100åˆ†ç»¼åˆè¯„åˆ†ï¼Œæ¨èç­‰çº§è‡ªåŠ¨åˆ†ç±»
  ğŸ’ å­—æ®µä¼˜åŒ–ï¼šä»24ä¸ªå­—æ®µç²¾ç®€åˆ°14ä¸ªæ ¸å¿ƒå­—æ®µ
  ğŸš« ç¡¬æ€§ç­›é€‰ï¼šè‡ªåŠ¨è¿‡æ»¤æ¯•ä¸šæ—¶é—´ä¸ç¬¦ã€æ‹›è˜å·²æˆªæ­¢çš„å²—ä½
  â­ æ¨èæ’åºï¼šæŒ‰åŒ¹é…åº¦å’Œå…¬å¸çŸ¥ååº¦æ™ºèƒ½æ’åº

ä½¿ç”¨æ¨¡å¼ï¼š
  1. å®Œæ•´æµæ°´çº¿ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰ï¼š
     python integrated_pipeline_with_filters.py
  
  2. ä½¿ç”¨å·²æœ‰æ•°æ®ï¼ˆæ¨èè°ƒè¯•ï¼‰ï¼š
     python integrated_pipeline_with_filters.py --skip-crawl --log-level debug
  
  3. ç¦ç”¨ç­›é€‰ï¼ˆä»…æµ‹è¯•ï¼‰ï¼š
     python integrated_pipeline_with_filters.py --no-filters
  
  4. æé€Ÿè°ƒè¯•æ¨¡å¼ï¼š
     python integrated_pipeline_with_filters.py --skip-crawl --skip-notion-load --log-level trace

ä¿®å¤è¯´æ˜ï¼š
  ç°åœ¨å½“æ‰€æœ‰å²—ä½éƒ½å·²å­˜åœ¨æ—¶ï¼Œç³»ç»Ÿä¼šæ˜¾ç¤ºï¼š
  âœ… æµæ°´çº¿æ‰§è¡ŒæˆåŠŸ - æ•°æ®åº“å·²æ˜¯æœ€æ–°çŠ¶æ€
  â„¹ï¸  è¿™æ˜¯æ­£å¸¸æƒ…å†µï¼Œè¯´æ˜å»é‡ç³»ç»Ÿå·¥ä½œæ­£å¸¸
  
  è€Œä¸æ˜¯è¯¯å¯¼æ€§çš„"å¤±è´¥"æ¶ˆæ¯ã€‚

ç¤ºä¾‹ï¼š
  # æ ‡å‡†æ¨¡å¼ï¼ˆæ¨èï¼‰
  python integrated_pipeline_with_filters.py --log-level normal
  
  # è°ƒè¯•æ¨¡å¼  
  python integrated_pipeline_with_filters.py --skip-crawl --log-level debug
        """
    )
    
    parser.add_argument('--log-level', 
                      choices=['production', 'normal', 'debug', 'trace'],
                      default='normal',
                      help='æ—¥å¿—è¯¦ç»†ç¨‹åº¦ (é»˜è®¤: normal)')
    
    parser.add_argument('--no-debug-data', 
                      action='store_true',
                      help='ä¸ä¿å­˜è°ƒè¯•æ•°æ®æ–‡ä»¶ï¼ˆèŠ‚çœç£ç›˜ç©ºé—´ï¼‰')
    
    parser.add_argument('--test-mode',
                      action='store_true', 
                      help='æµ‹è¯•æ¨¡å¼ï¼ˆé™åˆ¶å¤„ç†æ•°é‡ï¼Œé™ä½APIæˆæœ¬ï¼‰')
    
    parser.add_argument('--skip-crawl',
                      action='store_true',
                      help='è·³è¿‡çˆ¬è™«ï¼Œä½¿ç”¨å·²æœ‰æ•°æ®æ–‡ä»¶')
    
    parser.add_argument('--data-file',
                      type=str,
                      help='æŒ‡å®šè¦ä½¿ç”¨çš„æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆä¸--skip-crawlä¸€èµ·ä½¿ç”¨ï¼‰')
    
    parser.add_argument('--skip-notion-load',
                      action='store_true',
                      help='è·³è¿‡NotionåŠ è½½ï¼Œä½¿ç”¨ç¼“å­˜æ–‡ä»¶ï¼ˆå¤§å¹…æé€Ÿï¼‰')
    
    parser.add_argument('--notion-cache-file',
                      type=str,
                      help='æŒ‡å®šè¦ä½¿ç”¨çš„Notionç¼“å­˜æ–‡ä»¶è·¯å¾„')
    
    parser.add_argument('--no-filters',
                      action='store_true',
                      help='ç¦ç”¨ç­›é€‰åŠŸèƒ½ï¼ˆä»…ç”¨äºæµ‹è¯•å¯¹æ¯”ï¼‰')
    
    parser.add_argument('--list-notion-cache',
                      action='store_true',
                      help='åˆ—å‡ºå¯ç”¨çš„Notionç¼“å­˜æ–‡ä»¶')
    
    parser.add_argument('--list-data-files',
                      action='store_true',
                      help='åˆ—å‡ºå¯ç”¨çš„æ•°æ®æ–‡ä»¶')
    
    return parser.parse_args()


async def main():
    """ä¸»å‡½æ•°"""
    if not DEPENDENCIES_OK:
        print("âŒ ä¾èµ–æ£€æŸ¥å¤±è´¥ï¼Œè¯·å®‰è£…å¿…éœ€çš„æ¨¡å—")
        return
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    
    # å¦‚æœåªæ˜¯åˆ—å‡ºæ–‡ä»¶ï¼Œç›´æ¥è¿”å›
    if args.list_data_files:
        from src.enhanced_pipeline_fixed import list_available_data_files
        list_available_data_files()
        return
    
    if args.list_notion_cache:
        from src.enhanced_pipeline_fixed import list_available_notion_cache
        list_available_notion_cache()
        return
    
    # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
    log_level_map = {
        'production': LogLevel.PRODUCTION,
        'normal': LogLevel.NORMAL,
        'debug': LogLevel.DEBUG,
        'trace': LogLevel.TRACE
    }
    
    init_logger(
        level=log_level_map[args.log_level],
        save_debug_data=not args.no_debug_data
    )
    
    logger = get_logger()
    
    # æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
    logger.info("ğŸ”§ ä¿®å¤ç‰ˆå¸¦ç­›é€‰æµæ°´çº¿å¯åŠ¨", {
        "version": "ä¿®å¤ç‰ˆ v1.1",
        "filters_enabled": not args.no_filters,
        "log_level": args.log_level,
        "test_mode": args.test_mode,
        "skip_crawl": args.skip_crawl,
        "skip_notion_load": args.skip_notion_load,
        "python_version": f"3.{os.sys.version_info.minor}",
        "working_directory": os.getcwd()
    })
    
    # æ˜¾ç¤ºç­›é€‰é…ç½®
    if not args.no_filters:
        user_config = {
            "æ¯•ä¸šæ—¶é—´": os.getenv("USER_GRADUATION", "2023-12"),
            "å·¥ä½œç»éªŒ": f"{os.getenv('USER_EXPERIENCE_YEARS', '1.0')}å¹´",
            "ç›®æ ‡è–ªèµ„": f"{os.getenv('TARGET_SALARY', '30')}k"
        }
        logger.info("ç­›é€‰é…ç½®", user_config)
        logger.info("ğŸ’¡ å¯é€šè¿‡ç¯å¢ƒå˜é‡ USER_GRADUATION, USER_EXPERIENCE_YEARS, TARGET_SALARY è‡ªå®šä¹‰é…ç½®")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    required_env = {
        "LLMæå–": ["LLM_PROVIDER", "DEEPSEEK_API_KEY"],
        "Notionå†™å…¥": ["NOTION_TOKEN", "NOTION_DATABASE_ID"]
    }
    
    missing_env = []
    for category, vars in required_env.items():
        for var in vars:
            if not os.getenv(var):
                missing_env.append(f"{category}: {var}")
    
    if missing_env:
        logger.error("ç¼ºå°‘å¿…éœ€çš„ç¯å¢ƒå˜é‡", {"missing_vars": missing_env})
        logger.info("è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®è¿™äº›å˜é‡")
        return
    
    logger.success("ç¯å¢ƒå˜é‡æ£€æŸ¥é€šè¿‡")
    
    # è¿è¡Œå¸¦ç­›é€‰çš„æµæ°´çº¿
    try:
        # å¦‚æœæ˜¯æµ‹è¯•æ¨¡å¼ï¼Œä¿®æ”¹é…ç½®
        config = None
        if args.test_mode:
            config = {
                "crawler": {
                    "enabled_sites": ["boss_playwright"],
                    "max_pages": 1,
                    "max_jobs_test": 3  # æµ‹è¯•æ¨¡å¼åªå¤„ç†3ä¸ªå²—ä½
                },
                "search": {
                    "default_keyword": "å¤§æ¨¡å‹ ç®—æ³•",
                    "default_city": "101010100"
                }
            }
            logger.info("å¯ç”¨æµ‹è¯•æ¨¡å¼", {"max_jobs_per_page": 3})
        
        pipeline = FilteredJobPipeline(
            config=config,
            skip_crawl=args.skip_crawl,
            data_file=args.data_file,
            skip_notion_load=args.skip_notion_load,
            notion_cache_file=args.notion_cache_file,
            enable_filters=not args.no_filters  # ç­›é€‰å¼€å…³
        )
        
        success = await pipeline.run_filtered_pipeline()
        
        if success:
            logger.info("ğŸ‰ ä¿®å¤ç‰ˆæµæ°´çº¿æ‰§è¡ŒæˆåŠŸ")
            
            # æä¾›ä½¿ç”¨å»ºè®®
            logger.info("ğŸ”„ ä¸‹æ¬¡ä½¿ç”¨å»ºè®®:")
            if not args.skip_crawl:
                logger.info("   # ä½¿ç”¨æœ¬æ¬¡æ•°æ®è¿›è¡Œè°ƒè¯•ï¼ˆæ¨èï¼‰")
                logger.info("   python integrated_pipeline_with_filters.py --skip-crawl --log-level debug")
            if not args.skip_notion_load and os.getenv("NOTION_TOKEN"):
                logger.info("   # ä½¿ç”¨Notionç¼“å­˜æé€Ÿ")
                logger.info("   python integrated_pipeline_with_filters.py --skip-notion-load")
            if not args.no_filters:
                logger.info("   # å¯¹æ¯”æ— ç­›é€‰æ•ˆæœ")
                logger.info("   python integrated_pipeline_with_filters.py --no-filters --skip-crawl")
            
        else:
            logger.error("å¸¦ç­›é€‰çš„æµæ°´çº¿æ‰§è¡Œå¤±è´¥")
            logger.info("ğŸ’¡ å¦‚æœé‡åˆ°é—®é¢˜ï¼Œå¯ä»¥åˆ†æ­¥æµ‹è¯•:")
            logger.info("   1. python unified_filter_system.py  # æµ‹è¯•ç­›é€‰åŠŸèƒ½")
            logger.info("   2. python enhanced_extractor.py  # æµ‹è¯•æå–åŠŸèƒ½")
            logger.info("   3. python integrated_pipeline_with_filters.py --no-filters  # æµ‹è¯•æ— ç­›é€‰ç‰ˆæœ¬")
        
    except Exception as e:
        logger.error("ä¸»ç¨‹åºæ‰§è¡Œå¼‚å¸¸", {"error": str(e)}, e)
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†æ—¥å¿—ç³»ç»Ÿ
        try:
            cleanup_logger()
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†æ—¥å¿—ç³»ç»Ÿæ—¶å‡ºé”™: {e}")
            # å°è¯•æ‰‹åŠ¨ä¿å­˜é‡è¦ä¿¡æ¯
            try:
                print("ğŸ“ å°è¯•ä¿å­˜åŸºæœ¬è°ƒè¯•ä¿¡æ¯...")
                # åŸºæœ¬çš„è°ƒè¯•ä¿¡æ¯ä¿å­˜é€»è¾‘
            except:
                pass
        
        if not args.no_debug_data:
            logger.info("ğŸ“ è°ƒè¯•æ–‡ä»¶å·²ç”Ÿæˆ:")
            logger.info("   - debug/pipeline_*.log (æ ‡å‡†æ—¥å¿—)")
            logger.info("   - debug/debug_session_*.json (ç»“æ„åŒ–è°ƒè¯•æ•°æ®)")
            logger.info("   - debug/snapshots/ (æ•°æ®å¿«ç…§)")


if __name__ == "__main__":
    asyncio.run(main())
# enhanced_pipeline_skip_crawl.py - æ”¯æŒè·³è¿‡çˆ¬è™«çš„å¢å¼ºç‰ˆæµæ°´çº¿
"""
åœ¨åŸæœ‰åŸºç¡€ä¸Šå¢åŠ ï¼š
1. --skip-crawl è·³è¿‡çˆ¬è™«ï¼Œä½¿ç”¨å·²æœ‰æ•°æ®
2. --data-file æŒ‡å®šæ•°æ®æ–‡ä»¶
3. --skip-notion-load è·³è¿‡NotionåŠ è½½ï¼Œä½¿ç”¨ç¼“å­˜
4. --notion-cache-file æŒ‡å®šNotionç¼“å­˜æ–‡ä»¶
5. è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ•°æ®æ–‡ä»¶
"""
import asyncio
import json
import os
import glob
import argparse
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path

# å¯¼å…¥æ—¥å¿—ç³»ç»Ÿ
from src.logger_config import LogLevel, init_logger, get_logger, cleanup_logger
from src.data_snapshot import create_snapshot_manager

# å¯¼å…¥å…¶ä»–ç»„ä»¶
try:
    from dotenv import load_dotenv
    # åŠ è½½ç¯å¢ƒå˜é‡
    for env_path in [".env", "../.env", "../../.env"]:
        if os.path.exists(env_path):
            load_dotenv(env_path)
            break
    
    from src.crawler_registry import crawler_registry
    from src.enhanced_extractor import EnhancedNotionExtractor
    from src.optimized_notion_writer import OptimizedNotionJobWriter
    from src.enhanced_job_deduplicator import EnhancedJobDeduplicator, NotionJobDeduplicator
    DEPENDENCIES_OK = True
except ImportError as e:
    print(f"[ERROR] Dependency import failed: {e}")
    DEPENDENCIES_OK = False

class EnhancedNotionJobPipelineWithLogging:
    """æ”¯æŒè·³è¿‡çˆ¬è™«å’ŒNotionç¼“å­˜å¤ç”¨çš„å¢å¼ºç‰ˆæµæ°´çº¿"""
    
    def __init__(self, config=None, skip_crawl=False, data_file=None, 
                 skip_notion_load=False, notion_cache_file=None):
        """åˆå§‹åŒ–å¢å¼ºç‰ˆæµæ°´çº¿"""
        # æ—¥å¿—å’Œå¿«ç…§ç³»ç»Ÿ
        self.logger = get_logger()
        self.snapshot = create_snapshot_manager()
        
        self.config = config or self._load_default_config()
        self.skip_crawl = skip_crawl
        self.data_file = data_file
        self.skip_notion_load = skip_notion_load
        self.notion_cache_file = notion_cache_file
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "crawled": 0,
            "deduplicated": 0,
            "extracted": 0,
            "written": 0,
            "failed": 0,
            "recommended": 0,
            "not_suitable": 0,
            "need_check": 0,
            # å»é‡ç»Ÿè®¡
            "url_duplicates": 0,
            "content_duplicates": 0,
            "notion_duplicates": 0
        }
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.extractor = None
        self.writer = None
        self.deduplicator = None
        self.notion_deduplicator = None
        
        # æ•°æ®å­˜å‚¨
        self.raw_jobs = []
        self.deduplicated_jobs = []
        self.extracted_jobs = []
        
        self.logger.debug("æµæ°´çº¿åˆå§‹åŒ–å®Œæˆ", {
            "config_keys": list(self.config.keys()),
            "skip_crawl": skip_crawl,
            "data_file": data_file,
            "skip_notion_load": skip_notion_load,
            "notion_cache_file": notion_cache_file,
            "stats_initialized": list(self.stats.keys())
        })
    
    def _load_default_config(self) -> Dict[str, Any]:
        """åŠ è½½é»˜è®¤é…ç½®"""
        try:
            from src.config import load_config
            config = load_config()
            self.logger.debug("é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ", {"config_source": "config.py"})
            return config
        except ImportError:
            default_config = {
                "crawler": {
                    "enabled_sites": ["boss_playwright"],
                    "max_pages": 1,
                    "max_jobs_test": 5
                },
                "search": {
                    "default_keyword": "å¤§æ¨¡å‹ ç®—æ³•",
                    "default_city": "101010100"
                }
            }
            self.logger.warning("ä½¿ç”¨é»˜è®¤é…ç½®", {"reason": "config.pyä¸å­˜åœ¨"})
            return default_config
    
    def _find_latest_notion_cache(self) -> Optional[str]:
        """æŸ¥æ‰¾æœ€æ–°çš„Notionç¼“å­˜æ–‡ä»¶ï¼ˆä¼˜å…ˆä½¿ç”¨å·²æœ‰å¿«ç…§ï¼‰"""
        patterns = [
            "debug/snapshots/*_notion_cache.json",  # ä¼˜å…ˆä½¿ç”¨å¿«ç…§ç³»ç»Ÿçš„ç¼“å­˜
            "data/notion_cache_*.json",             # å¤‡ç”¨ï¼šæ‰‹åŠ¨ä¿å­˜çš„ç¼“å­˜
        ]
        
        all_files = []
        for pattern in patterns:
            files = glob.glob(pattern)
            all_files.extend(files)
        
        if not all_files:
            return None
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„
        latest_file = max(all_files, key=os.path.getmtime)
        return latest_file
    
    def _load_notion_cache_from_file(self, cache_file: str) -> Optional[Dict[str, Any]]:
        """ä»æ–‡ä»¶åŠ è½½Notionç¼“å­˜"""
        if not os.path.exists(cache_file):
            self.logger.error(f"Notionç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_file}")
            return None
        
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # éªŒè¯ç¼“å­˜æ•°æ®ç»“æ„
            if not isinstance(cache_data, dict):
                raise ValueError("ç¼“å­˜æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼šåº”ä¸ºJSONå¯¹è±¡")
            
            # éªŒè¯å¿…éœ€å­—æ®µ
            required_fields = ["existing_urls", "existing_fingerprints"]
            for field in required_fields:
                if field not in cache_data:
                    raise ValueError(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                if not isinstance(cache_data[field], list):
                    raise ValueError(f"{field}åº”ä¸ºæ•°ç»„æ ¼å¼")
            
            self.logger.success("Notionç¼“å­˜æ–‡ä»¶åŠ è½½æˆåŠŸ", {
                "cache_file": cache_file,
                "urls_count": len(cache_data["existing_urls"]),
                "fingerprints_count": len(cache_data["existing_fingerprints"]),
                "file_size_kb": round(os.path.getsize(cache_file) / 1024, 2),
                "file_age_hours": round((datetime.now() - datetime.fromtimestamp(os.path.getmtime(cache_file))).total_seconds() / 3600, 1)
            })
            
            return cache_data
            
        except Exception as e:
            self.logger.error(f"åŠ è½½Notionç¼“å­˜æ–‡ä»¶å¤±è´¥: {cache_file}", {
                "error": str(e),
                "error_type": type(e).__name__
            }, e)
            return None
    
    def _find_latest_data_file(self) -> Optional[str]:
        """æŸ¥æ‰¾æœ€æ–°çš„æ•°æ®æ–‡ä»¶"""
        patterns = [
            "data/raw_boss_playwright_*.jsonl",
            "data/deduplicated_jobs_*.json",
            "raw_boss_playwright_*.jsonl",
            "deduplicated_jobs_*.json"
        ]
        
        all_files = []
        for pattern in patterns:
            files = glob.glob(pattern)
            all_files.extend(files)
        
        if not all_files:
            return None
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„
        latest_file = max(all_files, key=os.path.getmtime)
        return latest_file
    
    def _load_existing_data(self, file_path: str) -> List[Dict[str, Any]]:
        """åŠ è½½å·²æœ‰çš„æ•°æ®æ–‡ä»¶"""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        jobs = []
        
        try:
            if file_path.endswith('.jsonl'):
                # JSONLæ ¼å¼ï¼ˆåŸå§‹çˆ¬å–æ•°æ®ï¼‰
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line_num, line in enumerate(f, 1):
                        if line.strip():
                            try:
                                job_data = json.loads(line.strip())
                                jobs.append(job_data)
                            except json.JSONDecodeError as e:
                                self.logger.warning(f"è·³è¿‡æ— æ•ˆè¡Œ {line_num}", {
                                    "error": str(e),
                                    "line_preview": line[:100]
                                })
            
            elif file_path.endswith('.json'):
                # JSONæ ¼å¼ï¼ˆå¤„ç†è¿‡çš„æ•°æ®ï¼‰
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        jobs = data
                    else:
                        raise ValueError("JSONæ–‡ä»¶åº”åŒ…å«å²—ä½æ•°ç»„")
            
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
            
            self.logger.success(f"æ•°æ®æ–‡ä»¶åŠ è½½æˆåŠŸ", {
                "file_path": file_path,
                "job_count": len(jobs),
                "file_type": "JSONL" if file_path.endswith('.jsonl') else "JSON",
                "file_size_mb": round(os.path.getsize(file_path) / 1024 / 1024, 2)
            })
            
            # æ£€æŸ¥æ•°æ®ç»“æ„
            if jobs:
                sample_job = jobs[0]
                self.logger.debug("æ•°æ®ç»“æ„é¢„è§ˆ", {
                    "sample_keys": list(sample_job.keys()),
                    "has_html": 'html' in sample_job,
                    "has_api_data": 'api_data' in sample_job,
                    "has_url": 'url' in sample_job or 'å²—ä½é“¾æ¥' in sample_job
                })
            
            return jobs
            
        except Exception as e:
            self.logger.error(f"åŠ è½½æ•°æ®æ–‡ä»¶å¤±è´¥: {file_path}", {"error": str(e)}, e)
            raise
    
    async def step1_load_or_crawl_jobs(self) -> bool:
        """æ­¥éª¤1: åŠ è½½å·²æœ‰æ•°æ®æˆ–çˆ¬å–æ–°æ•°æ®"""
        if self.skip_crawl:
            self.logger.step_start("åŠ è½½å·²æœ‰æ•°æ®", 1, 4)
            return await self._load_existing_jobs()
        else:
            self.logger.step_start("çˆ¬å–å²—ä½æ•°æ®", 1, 4)
            return await self._crawl_new_jobs()
    
    async def _load_existing_jobs(self) -> bool:
        """åŠ è½½å·²æœ‰æ•°æ®"""
        try:
            # ç¡®å®šæ•°æ®æ–‡ä»¶
            data_file = self.data_file
            if not data_file:
                data_file = self._find_latest_data_file()
                if not data_file:
                    self.logger.error("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ•°æ®æ–‡ä»¶", {
                        "searched_patterns": [
                            "data/raw_boss_playwright_*.jsonl",
                            "data/deduplicated_jobs_*.json",
                            "raw_boss_playwright_*.jsonl",
                            "deduplicated_jobs_*.json"
                        ]
                    })
                    self.logger.step_end("åŠ è½½å·²æœ‰æ•°æ®", False, {"é”™è¯¯": "æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶"})
                    return False
                
                self.logger.info(f"è‡ªåŠ¨é€‰æ‹©æœ€æ–°æ•°æ®æ–‡ä»¶: {data_file}")
            
            # åŠ è½½æ•°æ®
            jobs = self._load_existing_data(data_file)
            
            if not jobs:
                self.logger.error("æ•°æ®æ–‡ä»¶ä¸ºç©º")
                self.logger.step_end("åŠ è½½å·²æœ‰æ•°æ®", False, {"é”™è¯¯": "æ•°æ®æ–‡ä»¶ä¸ºç©º"})
                return False
            
            # å¤„ç†æ•°æ®æ ¼å¼
            self.raw_jobs = self._normalize_job_data(jobs)
            self.stats["crawled"] = len(self.raw_jobs)
            
            # ä¿å­˜æ•°æ®å¿«ç…§
            self.snapshot.capture("loaded_data", self.raw_jobs, {
                "stage": "åŠ è½½å·²æœ‰æ•°æ®",
                "source_file": data_file,
                "file_type": "JSONL" if data_file.endswith('.jsonl') else "JSON"
            })
            
            load_success = len(self.raw_jobs) > 0
            self.logger.step_end("åŠ è½½å·²æœ‰æ•°æ®", load_success, {
                "æ•°æ®æ–‡ä»¶": os.path.basename(data_file),
                "æ€»å²—ä½æ•°": len(self.raw_jobs),
                "æ•°æ®æ¥æº": "æœ¬åœ°æ–‡ä»¶"
            })
            
            return load_success
            
        except Exception as e:
            self.logger.error("åŠ è½½æ•°æ®å¤±è´¥", {"error": str(e)}, e)
            self.logger.step_end("åŠ è½½å·²æœ‰æ•°æ®", False, {"é”™è¯¯": str(e)})
            return False
    
    def _normalize_job_data(self, jobs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ ‡å‡†åŒ–å²—ä½æ•°æ®æ ¼å¼"""
        normalized_jobs = []
        
        for job in jobs:
            normalized_job = {}
            
            # æ£€æŸ¥æ•°æ®æ ¼å¼å¹¶æ ‡å‡†åŒ–
            if 'api_data' in job:
                # åŸå§‹çˆ¬å–æ•°æ®æ ¼å¼
                api_data = job.get('api_data', {})
                normalized_job = {
                    'å²—ä½åç§°': api_data.get('job_name', ''),
                    'å…¬å¸åç§°': api_data.get('company_name', ''),
                    'å·¥ä½œåœ°ç‚¹': api_data.get('location', ''),
                    'è–ªèµ„': api_data.get('salary_desc', ''),
                    'å²—ä½é“¾æ¥': job.get('url', ''),
                    'å²—ä½æè¿°': '',  # éœ€è¦ä»HTMLæå–
                    'html': job.get('html', ''),
                    'source_platform': job.get('source', 'Unknown'),
                    'timestamp': job.get('timestamp', '')
                }
            
            elif 'å²—ä½åç§°' in job:
                # å·²å¤„ç†æ•°æ®æ ¼å¼
                normalized_job = job.copy()
            
            else:
                # å…¶ä»–æ ¼å¼ï¼Œå°è¯•æ˜ å°„
                normalized_job = {
                    'å²—ä½åç§°': job.get('job_name', job.get('title', '')),
                    'å…¬å¸åç§°': job.get('company_name', job.get('company', '')),
                    'å·¥ä½œåœ°ç‚¹': job.get('location', job.get('city', '')),
                    'è–ªèµ„': job.get('salary_desc', job.get('salary', '')),
                    'å²—ä½é“¾æ¥': job.get('url', job.get('link', '')),
                    'å²—ä½æè¿°': job.get('description', ''),
                    'html': job.get('html', ''),
                    'source_platform': job.get('source_platform', job.get('source', 'Unknown')),
                    'timestamp': job.get('timestamp', '')
                }
            
            if normalized_job.get('å²—ä½åç§°') or normalized_job.get('å…¬å¸åç§°'):
                normalized_jobs.append(normalized_job)
            else:
                self.logger.warning("è·³è¿‡æ— æ•ˆæ•°æ®", {"job_data": job})
        
        self.logger.debug("æ•°æ®æ ‡å‡†åŒ–å®Œæˆ", {
            "input_count": len(jobs),
            "output_count": len(normalized_jobs),
            "sample_normalized": normalized_jobs[0] if normalized_jobs else {}
        })
        
        return normalized_jobs
    
    async def _crawl_new_jobs(self) -> bool:
        """çˆ¬å–æ–°æ•°æ®ï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
        try:
            # è·å–é…ç½®
            enabled_sites = self.config.get("crawler", {}).get("enabled_sites", ["boss_playwright"])
            max_pages = self.config.get("crawler", {}).get("max_pages", 1)
            max_jobs_test = self.config.get("crawler", {}).get("max_jobs_test", None)
            keyword = self.config.get("search", {}).get("default_keyword", "å¤§æ¨¡å‹ ç®—æ³•")
            city = self.config.get("search", {}).get("default_city", "101010100")
            
            crawl_params = {
                "keyword": keyword,
                "city": city,
                "max_pages": max_pages,
                "enabled_sites": enabled_sites,
                "max_jobs_test": max_jobs_test
            }
            
            self.logger.info("å¼€å§‹çˆ¬å–å²—ä½", crawl_params)
            
            # åŠ è½½çˆ¬è™«
            crawlers = crawler_registry.load_enabled_crawlers(enabled_sites)
            
            if not crawlers:
                self.logger.error("æ²¡æœ‰å¯ç”¨çš„çˆ¬è™«", {"enabled_sites": enabled_sites})
                return False
            
            self.logger.success(f"çˆ¬è™«åŠ è½½å®Œæˆ", {
                "crawler_count": len(crawlers),
                "crawler_names": list(crawlers.keys())
            })
            
            # æ‰§è¡Œçˆ¬å–
            all_jobs = []
            for platform_name, fetch_func in crawlers.items():
                self.logger.info(f"å¼€å§‹çˆ¬å–: {platform_name}")
                
                try:
                    # ä¼ é€’å‚æ•°ç»™çˆ¬è™«å‡½æ•°
                    crawl_args = {
                        "keyword": keyword,
                        "city": city,
                        "max_pages": max_pages
                    }
                    if max_jobs_test:
                        crawl_args["max_jobs_test"] = max_jobs_test
                    
                    jobs = await fetch_func(**crawl_args)
                    
                    if jobs:
                        # æ·»åŠ æ¥æºæ ‡è¯†
                        for job in jobs:
                            if isinstance(job, dict):
                                job['source_platform'] = platform_name
                        
                        all_jobs.extend(jobs)
                        
                        self.logger.success(f"{platform_name}çˆ¬å–å®Œæˆ", {
                            "job_count": len(jobs),
                            "platform": platform_name
                        })
                    else:
                        self.logger.warning(f"{platform_name}æ²¡æœ‰è·å–åˆ°å²—ä½")
                
                except Exception as e:
                    self.logger.error(f"{platform_name}çˆ¬å–å¤±è´¥", {
                        "platform": platform_name,
                        "error": str(e)
                    }, e)
                    continue
                
                # å¹³å°é—´å»¶è¿Ÿ
                await asyncio.sleep(2)
            
            # æ ‡å‡†åŒ–æ•°æ®
            self.raw_jobs = self._normalize_job_data(all_jobs)
            self.stats["crawled"] = len(self.raw_jobs)
            
            # ä¿å­˜åŸå§‹æ•°æ®å¿«ç…§
            if self.raw_jobs:
                self.snapshot.capture("raw_crawl", self.raw_jobs, {
                    "stage": "åŸå§‹çˆ¬å–",
                    "total_platforms": len(crawlers),
                    "crawl_params": crawl_params
                })
            
            crawl_success = len(self.raw_jobs) > 0
            self.logger.step_end("çˆ¬å–å²—ä½æ•°æ®", crawl_success, {
                "æ€»å²—ä½æ•°": len(self.raw_jobs),
                "çˆ¬è™«æ•°é‡": len(crawlers),
                "æˆåŠŸå¹³å°": sum(1 for jobs in [jobs for _, jobs in crawlers.items()] if jobs)
            })
            
            return crawl_success
            
        except Exception as e:
            self.logger.error("çˆ¬å–æ­¥éª¤å¤±è´¥", {"error": str(e)}, e)
            self.logger.step_end("çˆ¬å–å²—ä½æ•°æ®", False, {"é”™è¯¯": str(e)})
            return False
    
    async def step2_deduplicate_jobs(self) -> bool:
        """æ­¥éª¤2: å»é‡å¤„ç† - æ¨¡æ¿æ–¹æ³•ï¼ˆç»Ÿä¸€å…¥å£ï¼‰"""
        step_name = "å»é‡å¤„ç†"
        self.logger.step_start(step_name, 2, self.get_total_steps())
        
        if not self.raw_jobs:
            self.logger.error("æ²¡æœ‰åŸå§‹å²—ä½æ•°æ®")
            self.logger.step_end(step_name, False, {"é”™è¯¯": "æ²¡æœ‰è¾“å…¥æ•°æ®"})
            return False
        
        try:
            # ğŸ¯ æ ¸å¿ƒå»é‡é€»è¾‘ï¼ˆç»Ÿä¸€å®ç°ï¼ŒURLå»é‡ä¿®å¤åœ¨è¿™é‡Œï¼‰
            deduplicated_jobs = await self._perform_core_deduplication()
            
            # ğŸ¯ é’©å­æ–¹æ³•ï¼šå­ç±»å¯ä»¥é‡å†™è¿›è¡Œåå¤„ç†ï¼ˆå¦‚ç­›é€‰ï¼‰
            final_jobs = await self._post_deduplication_processing(deduplicated_jobs)
            
            # æ›´æ–°ç»“æœ
            self.deduplicated_jobs = final_jobs
            success = len(final_jobs) > 0
            
            # ç”Ÿæˆç»Ÿè®¡
            step_stats = self._generate_dedup_stats(deduplicated_jobs, final_jobs)
            self.logger.step_end(step_name, success, step_stats)
            
            return success
            
        except Exception as e:
            self.logger.error("å»é‡æ­¥éª¤å¤±è´¥", {"error": str(e)}, e)
            self.logger.step_end(step_name, False, {"é”™è¯¯": str(e)})
            return False
    
    async def _perform_core_deduplication(self) -> List[Dict[str, Any]]:
        """æ ¸å¿ƒå»é‡é€»è¾‘ - è¿™é‡Œä¿®å¤URLå»é‡ï¼Œåªéœ€æ”¹ä¸€æ¬¡ï¼"""
        
        # ğŸ¯ ç¬¬ä¸€é˜¶æ®µï¼šæœ¬åœ°å»é‡ï¼ˆä¿®å¤URLå»é‡åŠŸèƒ½ï¼‰
        self.logger.info("å¼€å§‹æœ¬åœ°å»é‡å¤„ç†", {
            "input_count": len(self.raw_jobs),
            "strategy": "URL + å†…å®¹æŒ‡çº¹ + LLMè¯­ä¹‰å»é‡"
        })
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        llm_client = None
        try:
            llm_client = EnhancedNotionExtractor(config=self.config)
            self.logger.success("LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            self.logger.warning("LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€å»é‡", {"error": str(e)})
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ¢å¤æœ¬åœ°å»é‡å™¨ï¼ˆåŒ…å«URLå»é‡ï¼‰
        from src.enhanced_job_deduplicator import EnhancedJobDeduplicator
        local_deduplicator = EnhancedJobDeduplicator(
            llm_client=llm_client, 
            use_llm=bool(llm_client)
        )
        
        # æ‰§è¡Œæœ¬åœ°å»é‡ï¼ˆåŒ…å«URLå»é‡ï¼‰
        locally_deduplicated = await local_deduplicator.deduplicate_jobs(self.raw_jobs)
        local_stats = local_deduplicator.get_stats()
        
        # ä¿å­˜æœ¬åœ°å»é‡ç»Ÿè®¡
        self.stats.update({
            "url_duplicates": local_stats.get("url_duplicates", 0),
            "content_duplicates": local_stats.get("content_duplicates", 0),
            "semantic_duplicates": local_stats.get("semantic_duplicates", 0)
        })
        
        self.logger.success("æœ¬åœ°å»é‡å®Œæˆ", {
            "input_count": len(self.raw_jobs),
            "output_count": len(locally_deduplicated),
            "url_duplicates": self.stats["url_duplicates"],
            "content_duplicates": self.stats["content_duplicates"],
            "semantic_duplicates": self.stats["semantic_duplicates"]
        })
        
        # ğŸ¯ ç¬¬äºŒé˜¶æ®µï¼šNotionå¢é‡å»é‡
        notion_token = os.getenv("NOTION_TOKEN")
        database_id = os.getenv("NOTION_DATABASE_ID")
        
        if notion_token and database_id and locally_deduplicated:
            self.logger.info("å¼€å§‹Notionå¢é‡å»é‡", {
                "input_count": len(locally_deduplicated),
                "strategy": "ä¸æ•°æ®åº“å¯¹æ¯”å»é‡"
            })
            
            try:
                from src.enhanced_job_deduplicator import NotionJobDeduplicator
                notion_deduplicator = NotionJobDeduplicator(
                    notion_token=notion_token,
                    database_id=database_id,
                    skip_notion_load=getattr(self, 'skip_notion_load', False),
                    notion_cache_file=getattr(self, 'notion_cache_file', None)
                )
                
                # åŠ è½½Notionä¸­å·²å­˜åœ¨çš„å²—ä½
                await notion_deduplicator.load_existing_jobs()
                
                # æ‰§è¡ŒNotionå»é‡
                new_jobs, duplicate_jobs = await notion_deduplicator.deduplicate_against_notion(locally_deduplicated)
                
                # æ›´æ–°ç»Ÿè®¡
                self.stats["notion_duplicates"] = len(duplicate_jobs)
                
                self.logger.success("Notionå¢é‡å»é‡å®Œæˆ", {
                    "input_count": len(locally_deduplicated),
                    "new_jobs": len(new_jobs),
                    "duplicate_jobs": len(duplicate_jobs)
                })
                
                return new_jobs
                
            except Exception as e:
                self.logger.error("Notionå»é‡å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°å»é‡ç»“æœ", {"error": str(e)}, e)
                return locally_deduplicated
        else:
            self.logger.info("è·³è¿‡Notionå»é‡ï¼Œä½¿ç”¨æœ¬åœ°å»é‡ç»“æœ")
            return locally_deduplicated

    async def _post_deduplication_processing(self, deduplicated_jobs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡åå¤„ç†é’©å­ - åŸºç±»é»˜è®¤ä¸åšé¢å¤–å¤„ç†ï¼Œå­ç±»å¯é‡å†™æ·»åŠ ç­›é€‰"""
        return deduplicated_jobs
    
    def _generate_dedup_stats(self, deduplicated_jobs: List[Dict], final_jobs: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆå»é‡ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "åŸå§‹å²—ä½": len(self.raw_jobs) if self.raw_jobs else 0,
            "æœ¬åœ°å»é‡å": len(deduplicated_jobs),
            "URLé‡å¤": self.stats.get('url_duplicates', 0),
            "å†…å®¹é‡å¤": self.stats.get('content_duplicates', 0),
            "è¯­ä¹‰é‡å¤": self.stats.get('semantic_duplicates', 0),
            "Notioné‡å¤": self.stats.get('notion_duplicates', 0),
            "æœ€ç»ˆå²—ä½": len(final_jobs)
        }
    
    def get_total_steps(self) -> int:
        """è·å–æ€»æ­¥éª¤æ•° - å­ç±»å¯é‡å†™"""
        return 4  # åŸºç±»é»˜è®¤4æ­¥
    
    async def step3_extract_info(self) -> bool:
        """æ­¥éª¤3: å¢å¼ºç‰ˆä¿¡æ¯æå– - å¢å¼ºæ—¥å¿—ç‰ˆæœ¬"""
        self.logger.step_start("å¢å¼ºç‰ˆä¿¡æ¯æå–", 3, 4)
        
        if not self.deduplicated_jobs:
            self.logger.error("æ²¡æœ‰å»é‡åçš„å²—ä½æ•°æ®")
            self.logger.step_end("å¢å¼ºç‰ˆä¿¡æ¯æå–", False, {"é”™è¯¯": "æ²¡æœ‰è¾“å…¥æ•°æ®"})
            return False
        
        try:
            # åˆå§‹åŒ–å¢å¼ºç‰ˆæå–å™¨
            self.extractor = EnhancedNotionExtractor(config=self.config)
            
            extraction_info = {
                "å²—ä½æ•°é‡": len(self.deduplicated_jobs),
                "ç”¨æˆ·æ¯•ä¸šæ—¶é—´": "2023å¹´12æœˆ",
                "æå–å™¨ç±»å‹": "å¢å¼ºç‰ˆLLMæå–å™¨"
            }
            
            self.logger.info("å¼€å§‹ä¿¡æ¯æå–", extraction_info)
            
            extracted_jobs = []
            failed_count = 0
            
            for i, job in enumerate(self.deduplicated_jobs, 1):
                job_title = job.get('å²—ä½åç§°', 'N/A')
                company = job.get('å…¬å¸åç§°', 'N/A')
                
                self.logger.trace(f"æå–ç¬¬ {i}/{len(self.deduplicated_jobs)} ä¸ªå²—ä½", {
                    "job_title": job_title,
                    "company": company,
                    "url": job.get('å²—ä½é“¾æ¥', 'N/A')
                })
                
                try:
                    html = job.get('html', '')
                    url = job.get('å²—ä½é“¾æ¥', '')
                    
                    if not html:
                        self.logger.warning(f"å²—ä½ {i} æ²¡æœ‰HTMLå†…å®¹", {
                            "job_title": job_title,
                            "company": company
                        })
                        failed_count += 1
                        continue
                    
                    # ä½¿ç”¨å¢å¼ºç‰ˆæå–å™¨
                    result = await self.extractor.extract_for_notion_enhanced(html, url, job)
                    
                    if result:
                        # æ·»åŠ åŸå§‹æ¥æºä¿¡æ¯
                        result['source_platform'] = job.get('source_platform', 'Unknown')
                        result['åŸå§‹æ—¶é—´æˆ³'] = job.get('timestamp', '')
                        extracted_jobs.append(result)
                        
                        match_status = result.get('æ¯•ä¸šæ—¶é—´_åŒ¹é…çŠ¶æ€', 'N/A')
                        
                        # æ ¹æ®åŒ¹é…çŠ¶æ€æ˜¾ç¤ºä¸åŒä¿¡æ¯å¹¶ç»Ÿè®¡
                        if 'ç¬¦åˆ' in match_status:
                            self.logger.success(f"æå–æˆåŠŸã€æ¨èã€‘: {job_title} - {company}")
                            self.stats["recommended"] += 1
                        elif 'ä¸ç¬¦åˆ' in match_status:
                            self.logger.info(f"æå–æˆåŠŸã€ä¸åŒ¹é…ã€‘: {job_title} - {company}")
                            self.stats["not_suitable"] += 1
                        else:
                            self.logger.info(f"æå–æˆåŠŸã€éœ€ç¡®è®¤ã€‘: {job_title} - {company}")
                            self.stats["need_check"] += 1
                    else:
                        self.logger.warning(f"å²—ä½ {i} æå–å¤±è´¥", {
                            "job_title": job_title,
                            "company": company
                        })
                        failed_count += 1
                
                except Exception as e:
                    self.logger.error(f"å²—ä½ {i} å¤„ç†å¼‚å¸¸", {
                        "job_title": job_title,
                        "company": company,
                        "error": str(e)
                    }, e)
                    failed_count += 1
                
                # APIè°ƒç”¨é—´éš”
                await asyncio.sleep(1.5)
            
            self.extracted_jobs = extracted_jobs
            self.stats["extracted"] = len(extracted_jobs)
            self.stats["failed"] = failed_count
            
            # ä¿å­˜æå–ç»“æœå¿«ç…§
            if extracted_jobs:
                self.snapshot.capture("extraction_output", extracted_jobs, {
                    "stage": "ä¿¡æ¯æå–è¾“å‡º",
                    "success_count": len(extracted_jobs),
                    "failed_count": failed_count
                })
                
                # ä¿å­˜åˆ°æ–‡ä»¶
                await self._save_extracted_data(extracted_jobs)
            
            extraction_success = len(extracted_jobs) > 0
            
            step_stats = {
                "æˆåŠŸæå–": len(extracted_jobs),
                "æå–å¤±è´¥": failed_count,
                "æ¨èå²—ä½": self.stats['recommended'],
                "ä¸åˆé€‚å²—ä½": self.stats['not_suitable'],
                "éœ€è¦ç¡®è®¤": self.stats['need_check']
            }
            
            if len(extracted_jobs) + failed_count > 0:
                step_stats["æˆåŠŸç‡"] = f"{len(extracted_jobs)/(len(extracted_jobs)+failed_count)*100:.1f}%"
            
            self.logger.step_end("å¢å¼ºç‰ˆä¿¡æ¯æå–", extraction_success, step_stats)
            
            return extraction_success
            
        except Exception as e:
            self.logger.error("ä¿¡æ¯æå–æ­¥éª¤å¤±è´¥", {"error": str(e)}, e)
            self.logger.step_end("å¢å¼ºç‰ˆä¿¡æ¯æå–", False, {"é”™è¯¯": str(e)})
            return False
    
    async def step4_write_to_notion(self) -> bool:
        """æ­¥éª¤4: å†™å…¥Notionï¼ˆå¢å¼ºç‰ˆï¼‰ - å¢å¼ºæ—¥å¿—ç‰ˆæœ¬"""
        self.logger.step_start("å†™å…¥Notionæ•°æ®åº“ï¼ˆå¢å¼ºç‰ˆï¼‰", 4, 4)
        
        if not self.extracted_jobs:
            self.logger.error("æ²¡æœ‰æå–çš„å²—ä½æ•°æ®")
            self.logger.step_end("å†™å…¥Notionæ•°æ®åº“", False, {"é”™è¯¯": "æ²¡æœ‰è¾“å…¥æ•°æ®"})
            return False
        
        try:
            # æ£€æŸ¥Notioné…ç½®
            if not os.getenv("NOTION_TOKEN") or not os.getenv("NOTION_DATABASE_ID"):
                self.logger.error("Notioné…ç½®ä¸å®Œæ•´", {
                    "notion_token_exists": bool(os.getenv("NOTION_TOKEN")),
                    "database_id_exists": bool(os.getenv("NOTION_DATABASE_ID"))
                })
                self.logger.step_end("å†™å…¥Notionæ•°æ®åº“", False, {"é”™è¯¯": "Notioné…ç½®ä¸å®Œæ•´"})
                return False
            
            # åˆå§‹åŒ–å¢å¼ºç‰ˆNotionå†™å…¥å™¨
            self.writer = OptimizedNotionJobWriter()
            
            # æ£€æŸ¥æ•°æ®åº“ç»“æ„
            self.logger.info("æ£€æŸ¥Notionæ•°æ®åº“ç»“æ„")
            if not self.writer.check_database_schema():
                self.logger.error("æ•°æ®åº“ç»“æ„ä¸å®Œæ•´")
                self.logger.step_end("å†™å…¥Notionæ•°æ®åº“", False, {"é”™è¯¯": "æ•°æ®åº“ç»“æ„ä¸å®Œæ•´"})
                return False
            
            # æ•°æ®é¢„è§ˆå’Œåˆ†ç±»
            recommended_jobs = [job for job in self.extracted_jobs if 'ç¬¦åˆ' in job.get('æ¯•ä¸šæ—¶é—´_åŒ¹é…çŠ¶æ€', '')]
            not_suitable_jobs = [job for job in self.extracted_jobs if 'ä¸ç¬¦åˆ' in job.get('æ¯•ä¸šæ—¶é—´_åŒ¹é…çŠ¶æ€', '')]
            need_check_jobs = [job for job in self.extracted_jobs 
                             if job not in recommended_jobs and job not in not_suitable_jobs]
            
            write_preview = {
                "æ€»å²—ä½æ•°": len(self.extracted_jobs),
                "æ¨èå²—ä½": len(recommended_jobs),
                "ä¸åˆé€‚å²—ä½": len(not_suitable_jobs),
                "éœ€è¦ç¡®è®¤": len(need_check_jobs)
            }
            
            self.logger.info("å‡†å¤‡å†™å…¥Notion", write_preview)
            
            # æ˜¾ç¤ºæ¨èå²—ä½é¢„è§ˆ
            if recommended_jobs:
                self.logger.info(f"å‘ç° {len(recommended_jobs)} ä¸ªæ¨èå²—ä½ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨ï¼")
                for i, job in enumerate(recommended_jobs[:3], 1):
                    self.logger.debug(f"æ¨èå²—ä½ {i}", {
                        "job_title": job.get('å²—ä½åç§°', 'N/A'),
                        "company": job.get('å…¬å¸åç§°', 'N/A'),
                        "salary": job.get('è–ªèµ„', 'N/A'),
                        "deadline_status": job.get('æ‹›è˜æˆªæ­¢æ—¥æœŸ_çŠ¶æ€', 'N/A')
                    })
            
            # æ‰§è¡Œæ‰¹é‡å†™å…¥
            self.logger.info("å¼€å§‹æ‰¹é‡å†™å…¥åˆ°Notion")
            stats = await self.writer.batch_write_jobs(
                self.extracted_jobs, 
                max_concurrent=2  # æ§åˆ¶å¹¶å‘é¿å…APIé™æµ
            )
            
            self.stats["written"] = stats["success"]
            
            # ä¿å­˜æœ€ç»ˆè¾“å‡ºå¿«ç…§
            if stats["success"] > 0:
                final_output = {
                    "write_stats": stats,
                    "successful_jobs": [job for job in self.extracted_jobs[:stats["success"]]],
                    "write_time": datetime.now().isoformat()
                }
                self.snapshot.capture("final_output", final_output, {
                    "stage": "æœ€ç»ˆNotionå†™å…¥ç»“æœ",
                    "success_count": stats["success"]
                })
            
            write_success = stats["success"] > 0
            
            step_stats = {
                "æˆåŠŸå†™å…¥": stats["success"],
                "å†™å…¥å¤±è´¥": stats["failed"],
                "æ¨èå²—ä½": stats.get("recommended", 0),
                "ä¸åˆé€‚å²—ä½": stats.get("not_suitable", 0),
                "éœ€è¦ç¡®è®¤": stats.get("need_check", 0)
            }
            
            if stats["total"] > 0:
                step_stats["æˆåŠŸç‡"] = f"{stats['success']/stats['total']*100:.1f}%"
            
            self.logger.step_end("å†™å…¥Notionæ•°æ®åº“", write_success, step_stats)
            
            return write_success
            
        except Exception as e:
            self.logger.error("Notionå†™å…¥æ­¥éª¤å¤±è´¥", {"error": str(e)}, e)
            self.logger.step_end("å†™å…¥Notionæ•°æ®åº“", False, {"é”™è¯¯": str(e)})
            return False
    
    async def _save_deduplicated_data(self, deduplicated_jobs: List[Dict[str, Any]]):
        """ä¿å­˜å»é‡åçš„æ•°æ®åˆ°æœ¬åœ°æ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        os.makedirs("data", exist_ok=True)
        output_file = f"data/deduplicated_jobs_{timestamp}.json"
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(deduplicated_jobs, f, ensure_ascii=False, indent=2)
            
            self.logger.debug("å»é‡åæ•°æ®å·²ä¿å­˜", {
                "file_path": output_file,
                "job_count": len(deduplicated_jobs)
            })
            
        except Exception as e:
            self.logger.error("ä¿å­˜å»é‡æ•°æ®å¤±è´¥", {
                "file_path": output_file,
                "error": str(e)
            }, e)
    
    async def _save_extracted_data(self, extracted_jobs: List[Dict[str, Any]]):
        """ä¿å­˜æå–çš„æ•°æ®åˆ°æœ¬åœ°æ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        os.makedirs("data", exist_ok=True)
        output_file = f"data/enhanced_pipeline_extracted_{timestamp}.json"
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(extracted_jobs, f, ensure_ascii=False, indent=2)
            
            self.logger.debug("æå–æ•°æ®å·²ä¿å­˜", {
                "file_path": output_file,
                "job_count": len(extracted_jobs)
            })
            
        except Exception as e:
            self.logger.error("ä¿å­˜æå–æ•°æ®å¤±è´¥", {
                "file_path": output_file,
                "error": str(e)
            }, e)
    
    async def run_full_enhanced_pipeline_with_logging(self) -> bool:
        """è¿è¡Œå®Œæ•´çš„å¢å¼ºç‰ˆæµæ°´çº¿ï¼ˆå«æ—¥å¿—ç³»ç»Ÿï¼‰"""
        start_time = datetime.now()
        
        pipeline_mode = "ä½¿ç”¨å·²æœ‰æ•°æ®" if self.skip_crawl else "çˆ¬å–æ–°æ•°æ®"
        self.logger.info("å¯åŠ¨å¢å¼ºç‰ˆNotionå²—ä½å¤„ç†æµæ°´çº¿", {
            "pipeline_version": "å¢å¼ºç‰ˆå«æ™ºèƒ½å»é‡",
            "pipeline_mode": pipeline_mode,
            "data_file": self.data_file if self.skip_crawl else None,
            "features": ["æ™ºèƒ½å»é‡", "å¢é‡æ›´æ–°", "æ¯•ä¸šæ—¶é—´åŒ¹é…", "æ‹›è˜æˆªæ­¢æ—¥æœŸ", "æ‹›å‹Ÿæ–¹å‘æå–"],
            "start_time": start_time.isoformat()
        })
        
        pipeline_success = True
        
        try:
            # æ­¥éª¤1: åŠ è½½æ•°æ®æˆ–çˆ¬å–
            step1_success = await self.step1_load_or_crawl_jobs()
            if not step1_success:
                self.logger.error("æµæ°´çº¿åœ¨æ•°æ®è·å–æ­¥éª¤å¤±è´¥")
                return False
            
            # æ­¥éª¤2: æ™ºèƒ½å»é‡
            step2_success = await self.step2_deduplicate_jobs()
            if not step2_success:
                self.logger.error("æµæ°´çº¿åœ¨å»é‡æ­¥éª¤å¤±è´¥")
                return False
            
            # æ­¥éª¤3: å¢å¼ºç‰ˆæå–
            step3_success = await self.step3_extract_info()
            if not step3_success:
                self.logger.error("æµæ°´çº¿åœ¨ä¿¡æ¯æå–æ­¥éª¤å¤±è´¥")
                return False
            
            # æ­¥éª¤4: å†™å…¥Notion
            step4_success = await self.step4_write_to_notion()
            if not step4_success:
                self.logger.warning("æµæ°´çº¿åœ¨Notionå†™å…¥æ­¥éª¤å¤±è´¥ï¼Œä½†æ•°æ®å·²ä¿å­˜åˆ°æœ¬åœ°")
                pipeline_success = False
            
        except KeyboardInterrupt:
            self.logger.warning("ç”¨æˆ·ä¸­æ–­æµæ°´çº¿")
            pipeline_success = False
        except Exception as e:
            self.logger.error("æµæ°´çº¿æ‰§è¡Œå¤±è´¥", {"error": str(e)}, e)
            pipeline_success = False
        
        finally:
            # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
            end_time = datetime.now()
            duration = end_time - start_time
            
            final_stats = {
                "æ‰§è¡Œæ¨¡å¼": pipeline_mode,
                "æ‰§è¡Œæ—¶é—´": f"{duration.total_seconds():.1f}ç§’",
                "çˆ¬å–/åŠ è½½å²—ä½": self.stats['crawled'],
                "å»é‡åå‰©ä½™": self.stats['deduplicated'],
                "æˆåŠŸæå–": self.stats['extracted'],
                "å†™å…¥Notion": self.stats['written'],
                "å¤„ç†å¤±è´¥": self.stats['failed'],
                "æ¨èå²—ä½": self.stats['recommended'],
                "ä¸åˆé€‚å²—ä½": self.stats['not_suitable'],
                "éœ€è¦ç¡®è®¤": self.stats['need_check']
            }
            
            # å»é‡æ•ˆæœç»Ÿè®¡
            total_removed = (self.stats['url_duplicates'] + 
                           self.stats['content_duplicates'] + 
                           self.stats['notion_duplicates'])
            
            if total_removed > 0:
                dedup_rate = (total_removed / self.stats['crawled']) * 100 if self.stats['crawled'] > 0 else 0
                final_stats["å»é‡ç‡"] = f"{dedup_rate:.1f}%"
                final_stats["URLé‡å¤"] = self.stats['url_duplicates']
                final_stats["å†…å®¹é‡å¤"] = self.stats['content_duplicates']
                if self.stats['notion_duplicates'] > 0:
                    final_stats["Notioné‡å¤"] = self.stats['notion_duplicates']
            
            # ä¿å­˜æœ€ç»ˆç»Ÿè®¡å¿«ç…§
            self.snapshot.capture("pipeline_final_stats", final_stats, {
                "stage": "æµæ°´çº¿æœ€ç»ˆç»Ÿè®¡",
                "success": pipeline_success,
                "duration_seconds": duration.total_seconds()
            })
            
            if pipeline_success:
                self.logger.success("å¢å¼ºç‰ˆæµæ°´çº¿æ‰§è¡Œå®Œæˆ", final_stats)
                
                if self.stats['written'] > 0:
                    if self.stats['recommended'] > 0:
                        self.logger.info(f"ğŸ‰ æˆåŠŸï¼å‘ç° {self.stats['recommended']} ä¸ªæ¨èå²—ä½å·²æ·»åŠ åˆ°Notionæ•°æ®åº“")
                        self.logger.info("ğŸ’¡ å»ºè®®åœ¨Notionä¸­ç­›é€‰\"æ¯•ä¸šæ—¶é—´åŒ¹é…çŠ¶æ€\" = \"âœ… ç¬¦åˆ\"æŸ¥çœ‹æ¨èå²—ä½")
                    else:
                        self.logger.info(f"âœ… æˆåŠŸï¼{self.stats['written']} ä¸ªå²—ä½å·²æ·»åŠ åˆ°Notionæ•°æ®åº“")
                        self.logger.info("âš ï¸  æ²¡æœ‰å‘ç°å®Œå…¨ç¬¦åˆæ¡ä»¶çš„å²—ä½ï¼Œå»ºè®®æŸ¥çœ‹éœ€è¦ç¡®è®¤çš„å²—ä½")
                    
                    if total_removed > 0:
                        self.logger.info(f"ğŸ”„ æ™ºèƒ½å»é‡èŠ‚çœäº† {total_removed} ä¸ªé‡å¤å²—ä½çš„å¤„ç†æ—¶é—´")
                else:
                    self.logger.warning("æ²¡æœ‰å²—ä½å†™å…¥Notionï¼Œè¯·æ£€æŸ¥æ•°æ®æˆ–é…ç½®")
            else:
                self.logger.error("å¢å¼ºç‰ˆæµæ°´çº¿æ‰§è¡Œå¤±è´¥", final_stats)
            
            # ä¿å­˜å¿«ç…§æ‘˜è¦
            self.snapshot.save_summary()
        
        return pipeline_success


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='å¢å¼ºç‰ˆæ™ºèƒ½å²—ä½å¤„ç†æµæ°´çº¿ - æ”¯æŒä½¿ç”¨å·²æœ‰æ•°æ®',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨æ¨¡å¼ï¼š
  1. å®Œæ•´æµæ°´çº¿ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰ï¼š
     python enhanced_pipeline_skip_crawl.py
  
  2. ä½¿ç”¨å·²æœ‰æ•°æ®ï¼ˆè·³è¿‡çˆ¬è™«ï¼‰ï¼š
     python enhanced_pipeline_skip_crawl.py --skip-crawl
  
  3. ä½¿ç”¨ç¼“å­˜ï¼ˆè·³è¿‡NotionåŠ è½½ï¼‰ï¼š
     python enhanced_pipeline_skip_crawl.py --skip-notion-load
  
  4. æé€Ÿè°ƒè¯•æ¨¡å¼ï¼ˆè·³è¿‡çˆ¬è™«+NotionåŠ è½½ï¼‰ï¼š
     python enhanced_pipeline_skip_crawl.py --skip-crawl --skip-notion-load --log-level trace
  
  5. æŒ‡å®šæ–‡ä»¶ï¼š
     python enhanced_pipeline_skip_crawl.py --skip-crawl --data-file data/my_jobs.jsonl --skip-notion-load --notion-cache-file data/my_cache.json

æ€§èƒ½å¯¹æ¯”ï¼š
  å®Œæ•´æµæ°´çº¿:     çˆ¬è™«(2-5åˆ†é’Ÿ) + NotionåŠ è½½(30ç§’-2åˆ†é’Ÿ) + å¤„ç†
  è·³è¿‡çˆ¬è™«:      NotionåŠ è½½(30ç§’-2åˆ†é’Ÿ) + å¤„ç†  
  è·³è¿‡Notion:    çˆ¬è™«(2-5åˆ†é’Ÿ) + å¤„ç†
  æé€Ÿæ¨¡å¼:      ä»…å¤„ç† (å‡ ç§’é’Ÿå¯åŠ¨)  â† æ¨èè°ƒè¯•æ—¶ä½¿ç”¨

æ—¥å¿—çº§åˆ«è¯´æ˜ï¼š
  production  - æœ€ç®€æ´è¾“å‡ºï¼Œä»…æ˜¾ç¤ºè­¦å‘Šå’Œé”™è¯¯
  normal      - æ ‡å‡†è¾“å‡ºï¼Œæ˜¾ç¤ºä¸»è¦æ­¥éª¤ä¿¡æ¯ï¼ˆé»˜è®¤ï¼‰
  debug       - è¯¦ç»†è°ƒè¯•ï¼Œæ˜¾ç¤ºå¤„ç†ç»†èŠ‚å’Œæ•°æ®ç»Ÿè®¡
  trace       - æœ€è¯¦ç»†è¿½è¸ªï¼Œæ˜¾ç¤ºæ¯ä¸ªå²—ä½çš„å¤„ç†è¿‡ç¨‹

ç¤ºä¾‹ï¼š
  # æ ‡å‡†çˆ¬å–æ¨¡å¼
  python enhanced_pipeline_skip_crawl.py
  
  # ä½¿ç”¨å·²æœ‰æ•°æ® + è¯¦ç»†è°ƒè¯•ï¼ˆæ¨èç”¨äºé—®é¢˜æ’æŸ¥ï¼‰
  python enhanced_pipeline_skip_crawl.py --skip-crawl --log-level trace
  
  # æŒ‡å®šç‰¹å®šæ•°æ®æ–‡ä»¶
  python enhanced_pipeline_skip_crawl.py --skip-crawl --data-file data/my_jobs.jsonl --log-level debug
  
  # æµ‹è¯•æ¨¡å¼ï¼ˆå°æ•°æ®é‡ + è¯¦ç»†æ—¥å¿—ï¼‰
  python enhanced_pipeline_skip_crawl.py --test-mode --log-level debug
        """
    )
    
    parser.add_argument('--log-level', 
                      choices=['production', 'normal', 'debug', 'trace'],
                      default='normal',
                      help='æ—¥å¿—è¯¦ç»†ç¨‹åº¦ (é»˜è®¤: normal)')
    
    parser.add_argument('--no-debug-data', 
                      action='store_true',
                      help='ä¸ä¿å­˜è°ƒè¯•æ•°æ®æ–‡ä»¶ï¼ˆèŠ‚çœç£ç›˜ç©ºé—´ï¼‰')
    
    parser.add_argument('--test-mode',
                      action='store_true', 
                      help='æµ‹è¯•æ¨¡å¼ï¼ˆé™åˆ¶å¤„ç†æ•°é‡ï¼Œé™ä½APIæˆæœ¬ï¼‰')
    
    parser.add_argument('--skip-crawl',
                      action='store_true',
                      help='è·³è¿‡çˆ¬è™«ï¼Œä½¿ç”¨å·²æœ‰æ•°æ®æ–‡ä»¶')
    
    parser.add_argument('--data-file',
                      type=str,
                      help='æŒ‡å®šè¦ä½¿ç”¨çš„æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆä¸--skip-crawlä¸€èµ·ä½¿ç”¨ï¼‰')
    
    parser.add_argument('--skip-notion-load',
                      action='store_true',
                      help='è·³è¿‡NotionåŠ è½½ï¼Œä½¿ç”¨ç¼“å­˜æ–‡ä»¶ï¼ˆå¤§å¹…æé€Ÿï¼‰')
    
    parser.add_argument('--notion-cache-file',
                      type=str,
                      help='æŒ‡å®šè¦ä½¿ç”¨çš„Notionç¼“å­˜æ–‡ä»¶è·¯å¾„')
    
    parser.add_argument('--list-notion-cache',
                      action='store_true',
                      help='åˆ—å‡ºå¯ç”¨çš„Notionç¼“å­˜æ–‡ä»¶')
    
    parser.add_argument('--list-data-files',
                      action='store_true',
                      help='åˆ—å‡ºå¯ç”¨çš„æ•°æ®æ–‡ä»¶')
    
    return parser.parse_args()

def list_available_notion_cache():
    """åˆ—å‡ºå¯ç”¨çš„Notionç¼“å­˜æ–‡ä»¶"""
    patterns = [
        "debug/snapshots/*_notion_cache.json",
        "data/notion_cache_*.json",
        "notion_cache_*.json"
    ]
    
    print("ğŸ“ å¯ç”¨çš„Notionç¼“å­˜æ–‡ä»¶:")
    found_files = []
    
    for pattern in patterns:
        files = glob.glob(pattern)
        for file in files:
            stat = os.stat(file)
            size_kb = stat.st_size / 1024
            mod_time = datetime.fromtimestamp(stat.st_mtime)
            
            file_info = {
                "path": file,
                "size_kb": size_kb,
                "modified": mod_time,
                "type": "å¿«ç…§ç¼“å­˜" if "snapshots" in file else "ä¿å­˜ç¼“å­˜"
            }
            found_files.append(file_info)
    
    if not found_files:
        print("   âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„Notionç¼“å­˜æ–‡ä»¶")
        print("   ğŸ’¡ è¯·å…ˆè¿è¡Œä¸€æ¬¡å®Œæ•´æµæ°´çº¿ç”Ÿæˆç¼“å­˜æ–‡ä»¶")
        return
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
    found_files.sort(key=lambda x: x["modified"], reverse=True)
    
    for i, file_info in enumerate(found_files):
        marker = "ğŸ“" if i == 0 else "  "
        print(f"{marker} {file_info['path']}")
        print(f"     ç±»å‹: {file_info['type']}")
        print(f"     å¤§å°: {file_info['size_kb']:.2f} KB")
        print(f"     ä¿®æ”¹æ—¶é—´: {file_info['modified'].strftime('%Y-%m-%d %H:%M:%S')}")
        if i == 0:
            print(f"     â­ æœ€æ–°æ–‡ä»¶ï¼ˆé»˜è®¤ä½¿ç”¨ï¼‰")
        print()

def list_available_data_files():
    """åˆ—å‡ºå¯ç”¨çš„æ•°æ®æ–‡ä»¶"""
    patterns = [
        "data/raw_boss_playwright_*.jsonl",
        "data/deduplicated_jobs_*.json",
        "data/enhanced_pipeline_extracted_*.json",
        "raw_boss_playwright_*.jsonl",
        "deduplicated_jobs_*.json"
    ]
    
    print("ğŸ“ å¯ç”¨çš„æ•°æ®æ–‡ä»¶:")
    found_files = []
    
    for pattern in patterns:
        files = glob.glob(pattern)
        for file in files:
            stat = os.stat(file)
            size_mb = stat.st_size / 1024 / 1024
            mod_time = datetime.fromtimestamp(stat.st_mtime)
            
            file_info = {
                "path": file,
                "size_mb": size_mb,
                "modified": mod_time,
                "type": "åŸå§‹çˆ¬å–æ•°æ®" if "raw_boss" in file else 
                       "å»é‡åæ•°æ®" if "deduplicated" in file else 
                       "æå–åæ•°æ®" if "extracted" in file else "å…¶ä»–"
            }
            found_files.append(file_info)
    
    if not found_files:
        print("   âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ•°æ®æ–‡ä»¶")
        print("   ğŸ’¡ è¯·å…ˆè¿è¡Œçˆ¬è™«ç”Ÿæˆæ•°æ®æ–‡ä»¶")
        return
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
    found_files.sort(key=lambda x: x["modified"], reverse=True)
    
    for i, file_info in enumerate(found_files):
        marker = "ğŸ“" if i == 0 else "  "
        print(f"{marker} {file_info['path']}")
        print(f"     ç±»å‹: {file_info['type']}")
        print(f"     å¤§å°: {file_info['size_mb']:.2f} MB")
        print(f"     ä¿®æ”¹æ—¶é—´: {file_info['modified'].strftime('%Y-%m-%d %H:%M:%S')}")
        if i == 0:
            print(f"     â­ æœ€æ–°æ–‡ä»¶ï¼ˆé»˜è®¤ä½¿ç”¨ï¼‰")
        print()

async def main():
    """ä¸»å‡½æ•°"""
    if not DEPENDENCIES_OK:
        print("âŒ ä¾èµ–æ£€æŸ¥å¤±è´¥ï¼Œè¯·å®‰è£…å¿…éœ€çš„æ¨¡å—")
        return
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    
    # å¦‚æœåªæ˜¯åˆ—å‡ºæ–‡ä»¶ï¼Œç›´æ¥è¿”å›
    if args.list_data_files:
        list_available_data_files()
        return
    
    if args.list_notion_cache:
        list_available_notion_cache()
        return
    
    # éªŒè¯å‚æ•°ç»„åˆ
    if args.data_file and not args.skip_crawl:
        print("âŒ --data-file å¿…é¡»ä¸ --skip-crawl ä¸€èµ·ä½¿ç”¨")
        return
    
    if args.notion_cache_file and not args.skip_notion_load:
        print("âŒ --notion-cache-file å¿…é¡»ä¸ --skip-notion-load ä¸€èµ·ä½¿ç”¨")
        return
    
    # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
    log_level = LogLevel(args.log_level)
    logger = init_logger(log_level, not args.no_debug_data)
    
    logger.info("ç³»ç»Ÿå¯åŠ¨", {
        "log_level": args.log_level,
        "debug_data_enabled": not args.no_debug_data,
        "test_mode": args.test_mode,
        "skip_crawl": args.skip_crawl,
        "data_file": args.data_file,
        "skip_notion_load": args.skip_notion_load,
        "notion_cache_file": args.notion_cache_file,
        "python_version": f"{os.sys.version_info.major}.{os.sys.version_info.minor}",
        "working_directory": os.getcwd()
    })
    
    # å¦‚æœè·³è¿‡çˆ¬è™«ï¼Œæ˜¾ç¤ºå°†è¦ä½¿ç”¨çš„æ•°æ®æ–‡ä»¶
    if args.skip_crawl:
        if args.data_file:
            if not os.path.exists(args.data_file):
                logger.error(f"æŒ‡å®šçš„æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.data_file}")
                return
            logger.info(f"å°†ä½¿ç”¨æŒ‡å®šæ•°æ®æ–‡ä»¶: {args.data_file}")
        else:
            # æŸ¥æ‰¾æœ€æ–°æ–‡ä»¶
            patterns = [
                "data/raw_boss_playwright_*.jsonl",
                "data/deduplicated_jobs_*.json", 
                "raw_boss_playwright_*.jsonl",
                "deduplicated_jobs_*.json"
            ]
            
            all_files = []
            for pattern in patterns:
                all_files.extend(glob.glob(pattern))
            
            if not all_files:
                logger.error("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ•°æ®æ–‡ä»¶")
                logger.info("ğŸ’¡ è¯·å…ˆè¿è¡Œçˆ¬è™«æˆ–ä½¿ç”¨ --list-data-files æŸ¥çœ‹å¯ç”¨æ–‡ä»¶")
                return
            
            latest_file = max(all_files, key=os.path.getmtime)
            logger.info(f"è‡ªåŠ¨é€‰æ‹©æœ€æ–°æ•°æ®æ–‡ä»¶: {latest_file}")
            logger.info("ğŸ’¡ ä½¿ç”¨ --list-data-files æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ–‡ä»¶")
    
    # å¦‚æœè·³è¿‡NotionåŠ è½½ï¼Œæ˜¾ç¤ºå°†è¦ä½¿ç”¨çš„ç¼“å­˜æ–‡ä»¶
    if args.skip_notion_load:
        if args.notion_cache_file:
            if not os.path.exists(args.notion_cache_file):
                logger.error(f"æŒ‡å®šçš„Notionç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {args.notion_cache_file}")
                return
            logger.info(f"å°†ä½¿ç”¨æŒ‡å®šNotionç¼“å­˜æ–‡ä»¶: {args.notion_cache_file}")
        else:
            # æŸ¥æ‰¾æœ€æ–°ç¼“å­˜æ–‡ä»¶
            patterns = [
                "debug/snapshots/*_notion_cache.json",
                "data/notion_cache_*.json",
                "notion_cache_*.json"
            ]
            
            all_cache_files = []
            for pattern in patterns:
                all_cache_files.extend(glob.glob(pattern))
            
            if not all_cache_files:
                logger.error("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„Notionç¼“å­˜æ–‡ä»¶")
                logger.info("ğŸ’¡ è¯·å…ˆè¿è¡Œå®Œæ•´æµæ°´çº¿æˆ–ä½¿ç”¨ --list-notion-cache æŸ¥çœ‹å¯ç”¨æ–‡ä»¶")
                return
            
            latest_cache = max(all_cache_files, key=os.path.getmtime)
            logger.info(f"è‡ªåŠ¨é€‰æ‹©æœ€æ–°Notionç¼“å­˜: {latest_cache}")
            logger.info("ğŸ’¡ ä½¿ç”¨ --list-notion-cache æŸ¥çœ‹æ‰€æœ‰å¯ç”¨ç¼“å­˜")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    required_env = {
        "LLMæå–": ["LLM_PROVIDER", "DEEPSEEK_API_KEY"],
        "Notionå†™å…¥": ["NOTION_TOKEN", "NOTION_DATABASE_ID"]
    }
    
    missing_env = []
    for category, vars in required_env.items():
        for var in vars:
            if not os.getenv(var):
                missing_env.append(f"{category}: {var}")
    
    if missing_env:
        logger.error("ç¼ºå°‘å¿…éœ€çš„ç¯å¢ƒå˜é‡", {"missing_vars": missing_env})
        logger.info("è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®è¿™äº›å˜é‡")
        return
    
    logger.success("ç¯å¢ƒå˜é‡æ£€æŸ¥é€šè¿‡")
    
    # è¿è¡Œå¢å¼ºç‰ˆæµæ°´çº¿
    try:
        # å¦‚æœæ˜¯æµ‹è¯•æ¨¡å¼ï¼Œä¿®æ”¹é…ç½®
        config = None
        if args.test_mode:
            config = {
                "crawler": {
                    "enabled_sites": ["boss_playwright"],
                    "max_pages": 1,
                    "max_jobs_test": 3  # æµ‹è¯•æ¨¡å¼åªå¤„ç†3ä¸ªå²—ä½
                },
                "search": {
                    "default_keyword": "å¤§æ¨¡å‹ ç®—æ³•",
                    "default_city": "101010100"
                }
            }
            logger.info("å¯ç”¨æµ‹è¯•æ¨¡å¼", {"max_jobs_per_page": 3})
        
        pipeline = EnhancedNotionJobPipelineWithLogging(
            config=config,
            skip_crawl=args.skip_crawl,
            data_file=args.data_file,
            skip_notion_load=args.skip_notion_load,
            notion_cache_file=args.notion_cache_file
        )
        success = await pipeline.run_full_enhanced_pipeline_with_logging()
        
        if success:
            logger.info("æµæ°´çº¿æ‰§è¡ŒæˆåŠŸ")
            
            # æä¾›ä½¿ç”¨å»ºè®®
            if args.skip_crawl or args.skip_notion_load:
                logger.info("ğŸ”„ ä¸‹æ¬¡ä½¿ç”¨å»ºè®®:")
                if not args.skip_crawl:
                    logger.info("   # ä½¿ç”¨æœ¬æ¬¡æ•°æ®è¿›è¡Œè°ƒè¯•ï¼ˆæ¨èï¼‰")
                    logger.info("   python enhanced_pipeline_skip_crawl.py --skip-crawl --log-level trace")
                if not args.skip_notion_load and os.getenv("NOTION_TOKEN"):
                    logger.info("   # ä½¿ç”¨Notionç¼“å­˜æé€Ÿ")
                    logger.info("   python enhanced_pipeline_skip_crawl.py --skip-notion-load")
                logger.info("   # æé€Ÿè°ƒè¯•æ¨¡å¼")
                logger.info("   python enhanced_pipeline_skip_crawl.py --skip-crawl --skip-notion-load --log-level trace")
            else:
                logger.info("ğŸ”„ ä¸‹æ¬¡è¿è¡Œå»ºè®®:")
                logger.info("   # ä½¿ç”¨æœ¬æ¬¡æ•°æ®è¿›è¡Œè°ƒè¯•ï¼ˆæ¨èï¼‰")
                logger.info("   python enhanced_pipeline_skip_crawl.py --skip-crawl --log-level trace")
                logger.info("   # ä½¿ç”¨ç¼“å­˜æé€Ÿ")
                logger.info("   python enhanced_pipeline_skip_crawl.py --skip-notion-load")
                logger.info("   # çˆ¬å–æ–°æ•°æ®")
                logger.info("   python enhanced_pipeline_skip_crawl.py")
            
            logger.info("ğŸ“± Notionä½¿ç”¨å»ºè®®:")
            logger.info("   1. ç­›é€‰\"æ¯•ä¸šæ—¶é—´åŒ¹é…çŠ¶æ€\" = \"âœ… ç¬¦åˆ\"æŸ¥çœ‹æ¨èå²—ä½")
            logger.info("   2. ç­›é€‰\"æ‹›è˜æˆªæ­¢æ—¥æœŸçŠ¶æ€\" = \"âœ… æœªè¿‡æœŸ\"æŸ¥çœ‹æœ‰æ•ˆå²—ä½")
            logger.info("   3. æ ¹æ®\"æ‹›å‹Ÿæ–¹å‘\"äº†è§£æŠ€æœ¯è¦æ±‚")
            
        else:
            logger.error("æµæ°´çº¿æ‰§è¡Œå¤±è´¥")
            logger.info("ğŸ’¡ å¦‚æœé‡åˆ°é—®é¢˜ï¼Œå¯ä»¥åˆ†æ­¥æ‰§è¡Œ:")
            logger.info("   1. python enhanced_job_deduplicator.py  # æµ‹è¯•å»é‡åŠŸèƒ½")
            logger.info("   2. python enhanced_extractor.py  # æµ‹è¯•æå–åŠŸèƒ½")
            logger.info("   3. python enhanced_notion_writer.py  # æµ‹è¯•å†™å…¥åŠŸèƒ½")
        
    except Exception as e:
        logger.error("ä¸»ç¨‹åºæ‰§è¡Œå¼‚å¸¸", {"error": str(e)}, e)
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†æ—¥å¿—ç³»ç»Ÿ
        cleanup_logger()
        
        if not args.no_debug_data:
            logger.info("ğŸ“ è°ƒè¯•æ–‡ä»¶å·²ç”Ÿæˆ:")
            logger.info("   - debug/pipeline_*.log (æ ‡å‡†æ—¥å¿—)")
            logger.info("   - debug/debug_session_*.json (ç»“æ„åŒ–è°ƒè¯•æ•°æ®)")
            logger.info("   - debug/snapshots/ (æ•°æ®å¿«ç…§)")
            logger.info("   - debug/debug_session_latest.json (æœ€æ–°è°ƒè¯•æ•°æ®)")

if __name__ == "__main__":
    asyncio.run(main())
# enhanced_job_deduplicator.py - é›†æˆæ—¥å¿—ç³»ç»Ÿçš„å¢å¼ºç‰ˆå»é‡å™¨ï¼ˆè¯­æ³•ä¿®å¤ç‰ˆï¼‰
"""
ğŸ”§ å¢å¼ºç‰ˆå²—ä½å»é‡å™¨ - è¯­æ³•ä¿®å¤ç‰ˆ

åŠŸèƒ½ç‰¹ç‚¹ï¼š
1. ğŸ§  æ™ºèƒ½å»é‡ç­–ç•¥ï¼š
   - LLMæ™ºèƒ½å»é‡ï¼šä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œè¯­ä¹‰å»é‡
   - ä¼ ç»Ÿè§„åˆ™å»é‡ï¼šåŸºäºURLå’Œå†…å®¹æŒ‡çº¹çš„å¿«é€Ÿå»é‡
2. ğŸ“Š å®Œæ•´æ—¥å¿—ç³»ç»Ÿï¼š
   - è¯¦ç»†çš„å¤„ç†æ—¥å¿—å’Œç»Ÿè®¡ä¿¡æ¯
   - æ•°æ®å¿«ç…§ä¿å­˜å’Œè¿½è¸ª
   - æ€§èƒ½ç›‘æ§å’Œé”™è¯¯å¤„ç†
3. ğŸ”§ ä¿®å¤ç‰ˆæ”¹è¿›ï¼š
   - æ­£ç¡®åŒºåˆ†"ç³»ç»Ÿé”™è¯¯"å’Œ"æ— æ–°æ•°æ®"æƒ…å†µ
   - æ”¹è¿›æ—¥å¿—æ¶ˆæ¯ï¼Œé¿å…è¯¯å¯¼æ€§çŠ¶æ€æç¤º
   - å¢å¼ºé”™è¯¯å¤„ç†å’Œç»Ÿè®¡æŠ¥å‘Š
   - ä¿®å¤æ‰€æœ‰è¯­æ³•é”™è¯¯

æ ¸å¿ƒç±»ï¼š
- EnhancedJobDeduplicator: ä¸»è¦å»é‡å™¨ç±»
- EnhancedLLMJobDeduplicator: LLMæ™ºèƒ½å»é‡å®ç°  
- NotionJobDeduplicator: Notionæ•°æ®åº“å»é‡å™¨

ä¿®å¤å†…å®¹ï¼š
- âœ… ä¿®å¤æ‰€æœ‰å­—ç¬¦ä¸²å­—é¢é‡è¯­æ³•é”™è¯¯
- âœ… æ”¹è¿›æ—¥å¿—æ¶ˆæ¯ï¼Œæ­£ç¡®åŒºåˆ†"ç³»ç»Ÿé”™è¯¯"å’Œ"æ— æ–°æ•°æ®"
- âœ… ä¿®å¤å»é‡å®Œæˆçš„çŠ¶æ€è¯´æ˜
- âœ… æ·»åŠ è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯å’Œå¤„ç†å»ºè®®
- âœ… ä¼˜åŒ–æ€§èƒ½ç›‘æ§å’Œé”™è¯¯è¿½è¸ª
"""
import os
import glob
import json
from datetime import datetime
from typing import Optional, Dict, Any, List, Set, Tuple
import hashlib
import re
import asyncio
import time

# å¯¼å…¥æ—¥å¿—ç³»ç»Ÿ
from src.logger_config import get_logger, log_function_call
from src.data_snapshot import create_snapshot_manager

try:
    from notion_client import Client
    HAS_NOTION_CLIENT = True
except ImportError:
    HAS_NOTION_CLIENT = False
    print("âš ï¸ Notionå®¢æˆ·ç«¯æœªå®‰è£…ï¼ŒNotionå»é‡åŠŸèƒ½å°†ä¸å¯ç”¨")

# å¯¼å…¥LLMå…³é”®è¯æå–å™¨
try:
    from src.llm_keyword_extractor import LLMKeywordExtractor, LLMJobDeduplicator
    HAS_LLM_EXTRACTOR = True
except ImportError:
    HAS_LLM_EXTRACTOR = False
    print("âš ï¸ LLMæå–å™¨æœªå®‰è£…ï¼Œæ™ºèƒ½å»é‡åŠŸèƒ½å°†ä¸å¯ç”¨")

class EnhancedJobDeduplicator:
    """å¢å¼ºç‰ˆå²—ä½å»é‡å™¨ - é›†æˆè¯¦ç»†æ—¥å¿—ç³»ç»Ÿï¼ˆè¯­æ³•ä¿®å¤ç‰ˆï¼‰"""
    
    def __init__(self, llm_client=None, use_llm=True):
        """åˆå§‹åŒ–å»é‡å™¨"""
        self.llm_client = llm_client
        self.use_llm = use_llm and HAS_LLM_EXTRACTOR and llm_client is not None
        
        # æ—¥å¿—å’Œå¿«ç…§ç³»ç»Ÿ
        self.logger = get_logger()
        self.snapshot = create_snapshot_manager()
        
        # åˆå§‹åŒ–ç»Ÿè®¡
        self.stats = {
            "total_processed": 0,
            "url_duplicates": 0,
            "content_duplicates": 0,
            "semantic_duplicates": 0,
            "unique_jobs": 0,
            "processing_time": 0.0
        }
        
        # åˆå§‹åŒ–å»é‡ç­–ç•¥
        if self.use_llm:
            self.logger.success("Enabled LLM smart deduplication strategy")
            self.llm_deduplicator = EnhancedLLMJobDeduplicator(llm_client)
        else:
            self.logger.info("Using traditional rule-based deduplication strategy")
            self.url_cache: Set[str] = set()
            self.fingerprint_cache: Set[str] = set()
        
        self.logger.debug("Deduplicator initialization completed", {
            "use_llm": self.use_llm,
            "has_llm_extractor": HAS_LLM_EXTRACTOR,
            "llm_client_available": llm_client is not None,
            "strategy": "LLMæ™ºèƒ½å»é‡" if self.use_llm else "ä¼ ç»Ÿè§„åˆ™å»é‡"
        })
    
    @log_function_call("å²—ä½å»é‡å¤„ç†")
    async def deduplicate_jobs(self, jobs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ™ºèƒ½å»é‡ - å¢å¼ºæ—¥å¿—ç‰ˆæœ¬ï¼ˆè¯­æ³•ä¿®å¤ç‰ˆï¼‰"""
        if not jobs:
            self.logger.warning("Input job list is empty")
            return []
        
        start_time = time.time()
        
        self.logger.debug("Starting deduplication processing", {
            "input_count": len(jobs),
            "use_llm": self.use_llm,
            "dedup_strategy": "LLM smart deduplication" if self.use_llm else "Traditional rule-based deduplication"
        })
        
        # æ•è·è¾“å…¥æ•°æ®å¿«ç…§
        self.snapshot.capture("local_dedup_input", jobs, {
            "stage": "Local deduplication input",
            "strategy": "LLMæ™ºèƒ½å»é‡" if self.use_llm else "ä¼ ç»Ÿè§„åˆ™å»é‡",
            "input_count": len(jobs)
        })
        
        try:
            # æ‰§è¡Œå»é‡
            if self.use_llm:
                result = await self.llm_deduplicator.deduplicate_jobs(jobs)
                self.stats = self.llm_deduplicator.get_stats()
            else:
                result = self._deduplicate_jobs_traditional(jobs)
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            self.stats["processing_time"] = time.time() - start_time
            
            # æ•è·è¾“å‡ºæ•°æ®å¿«ç…§
            self.snapshot.capture("local_dedup_output", result, {
                "stage": "Local deduplication output",
                "removed_count": len(jobs) - len(result),
                "processing_time": self.stats["processing_time"]
            })
            
            # ä¿®å¤: æ”¹è¿›å»é‡ç»“æœæ—¥å¿—æ¶ˆæ¯
            self._log_dedup_results(jobs, result)
            
            return result
            
        except Exception as e:
            self.logger.error("Deduplication processing failed", {
                "error": str(e),
                "input_count": len(jobs),
                "processing_time": time.time() - start_time
            }, e)
            # å‘ç”Ÿé”™è¯¯æ—¶è¿”å›åŸå§‹æ•°æ®ï¼Œé¿å…æ•°æ®ä¸¢å¤±
            return jobs
    
    def _log_dedup_results(self, input_jobs: List[Dict], result_jobs: List[Dict]):
        """è®°å½•å»é‡ç»“æœï¼ˆä¿®å¤ç‰ˆï¼‰"""
        input_count = len(input_jobs)
        output_count = len(result_jobs)
        removed_count = input_count - output_count
        dedup_rate = (removed_count / input_count * 100) if input_count > 0 else 0
        
        # ä¿®å¤: æ”¹è¿›æ¶ˆæ¯å†…å®¹ï¼Œæä¾›æ›´å‡†ç¡®çš„çŠ¶æ€æè¿°
        if output_count == 0:
            result_message = "å»é‡å¤„ç†å®Œæˆ - æ‰€æœ‰å²—ä½éƒ½æ˜¯é‡å¤çš„"
            status = "æ— å”¯ä¸€å²—ä½"
            suggestion = "å¯èƒ½åŸå› ï¼š1) æ•°æ®æºé‡å¤åº¦é«˜ 2) å»é‡ç­–ç•¥è¿‡äºä¸¥æ ¼"
        elif output_count == input_count:
            result_message = "å»é‡å¤„ç†å®Œæˆ - æ‰€æœ‰å²—ä½éƒ½æ˜¯å”¯ä¸€çš„"
            status = "å…¨éƒ¨å”¯ä¸€"
            suggestion = "æ•°æ®è´¨é‡è‰¯å¥½ï¼Œæ— é‡å¤å²—ä½"
        else:
            result_message = f"å»é‡å¤„ç†å®Œæˆ - å»é™¤äº†{removed_count}ä¸ªé‡å¤å²—ä½"
            status = "éƒ¨åˆ†å»é‡"
            suggestion = f"å»é‡æ•ˆæœæ­£å¸¸ï¼Œä¿ç•™äº†{output_count}ä¸ªå”¯ä¸€å²—ä½"
        
        log_data = {
            "input_count": input_count,
            "output_count": output_count,
            "removed_count": removed_count,
            "dedup_rate_percent": round(dedup_rate, 1),
            "url_duplicates": self.stats.get("url_duplicates", 0),
            "content_duplicates": self.stats.get("content_duplicates", 0),
            "semantic_duplicates": self.stats.get("semantic_duplicates", 0),
            "processing_time": round(self.stats.get("processing_time", 0), 2),
            "status": status,
            "message": suggestion
        }
        
        # æ ¹æ®ç»“æœé€‰æ‹©åˆé€‚çš„æ—¥å¿—çº§åˆ«
        if output_count == 0:
            self.logger.info_no_data(result_message, log_data)
            self.logger.info("ğŸ’¡ " + suggestion)
        else:
            self.logger.success(result_message, log_data)
    
    def _deduplicate_jobs_traditional(self, jobs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä¼ ç»Ÿå»é‡æ–¹æ³•ï¼ˆè§„åˆ™åŸºç¡€ï¼‰"""
        unique_jobs = []
        
        self.stats["total_processed"] = len(jobs)
        
        for job in jobs:
            # URLå»é‡
            job_url = self._extract_job_id(job.get('å²—ä½é“¾æ¥', ''))
            if job_url and job_url in self.url_cache:
                self.stats["url_duplicates"] += 1
                continue
            
            # å†…å®¹æŒ‡çº¹å»é‡
            fingerprint = self._create_smart_fingerprint(job)
            if fingerprint in self.fingerprint_cache:
                self.stats["content_duplicates"] += 1
                continue
            
            # æ·»åŠ åˆ°ç¼“å­˜
            if job_url:
                self.url_cache.add(job_url)
            self.fingerprint_cache.add(fingerprint)
            
            unique_jobs.append(job)
        
        self.stats["unique_jobs"] = len(unique_jobs)
        
        return unique_jobs
    
    def _extract_job_id(self, url: str) -> str:
        """ä»URLä¸­æå–å²—ä½ID"""
        if not url:
            return ""
        
        # å¤„ç†ä¸åŒæ‹›è˜ç½‘ç«™çš„URLæ ¼å¼
        base_url = url.split('?')[0]
        
        # Bossç›´è˜
        if 'zhipin.com' in url:
            match = re.search(r'/job_detail/([^/.]+)', base_url)
            return match.group(1) if match else base_url.split('/')[-1] if '/' in base_url else base_url
        
        # å…¶ä»–ç½‘ç«™çš„é€šç”¨å¤„ç†
        return base_url.split('/')[-1] if '/' in base_url else base_url
    
    def _create_smart_fingerprint(self, job: Dict[str, Any]) -> str:
        """æ™ºèƒ½æŒ‡çº¹ç”Ÿæˆ"""
        company = self._normalize_company_name(job.get('å…¬å¸åç§°', ''))
        title = self._normalize_job_title(job.get('å²—ä½åç§°', ''))
        location = self._normalize_location(job.get('å·¥ä½œåœ°ç‚¹', ''))
        
        base_fingerprint = f"{company}_{title}_{location}"
        return hashlib.md5(base_fingerprint.encode('utf-8')).hexdigest()
    
    def _normalize_company_name(self, company: str) -> str:
        """å…¬å¸åç§°æ ‡å‡†åŒ–"""
        if not company:
            return ""
        
        company = company.strip()
        
        # ç§»é™¤å¸¸è§çš„å…¬å¸åç¼€
        suffixes = [
            r'æœ‰é™å…¬å¸$',
            r'ç§‘æŠ€æœ‰é™å…¬å¸$',
            r'ç½‘ç»œç§‘æŠ€æœ‰é™å…¬å¸$',
            r'ä¿¡æ¯ç§‘æŠ€æœ‰é™å…¬å¸$',
            r'æŠ€æœ¯æœ‰é™å…¬å¸$',
            r'è‚¡ä»½æœ‰é™å…¬å¸$',
            r'é›†å›¢æœ‰é™å…¬å¸$',
            r'\(.*\)$',  # ç§»é™¤æ‹¬å·å†…å®¹
            r'ï¼ˆ.*ï¼‰$'   # ç§»é™¤ä¸­æ–‡æ‹¬å·å†…å®¹
        ]
        
        for suffix in suffixes:
            company = re.sub(suffix, '', company)
        
        return company.strip().lower()
    
    def _normalize_job_title(self, title: str) -> str:
        """å²—ä½åç§°æ ‡å‡†åŒ–"""
        if not title:
            return ""
        
        title = title.strip().lower()
        
        # ç§»é™¤å¸¸è§çš„ä¿®é¥°è¯
        removals = [
            r'ï¼ˆ.*?ï¼‰',  # ä¸­æ–‡æ‹¬å·
            r'\(.*?\)',  # è‹±æ–‡æ‹¬å·
            r'ã€.*?ã€‘',  # ä¸­æ–‡æ–¹æ‹¬å·
            r'\[.*?\]',  # è‹±æ–‡æ–¹æ‹¬å·
            r'æ€¥æ‹›',
            r'é«˜è–ª',
            r'åŒ…ä½',
            r'äº”é™©ä¸€é‡‘'
        ]
        
        for removal in removals:
            title = re.sub(removal, '', title)
        
        return title.strip()
    
    def _normalize_location(self, location: str) -> str:
        """å·¥ä½œåœ°ç‚¹æ ‡å‡†åŒ–"""
        if not location:
            return ""
        
        location = location.strip().lower()
        
        # æå–ä¸»è¦åŸå¸‚å’ŒåŒºåŸŸ
        location = re.sub(r'Â·.*$', '', location)  # ç§»é™¤è¯¦ç»†åœ°å€
        location = re.sub(r'-.*$', '', location)  # ç§»é™¤è¯¦ç»†æè¿°
        
        return location.strip()
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å»é‡ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()


class EnhancedLLMJobDeduplicator:
    """å¢å¼ºç‰ˆLLMæ™ºèƒ½å»é‡å™¨"""
    
    def __init__(self, llm_client):
        """åˆå§‹åŒ–LLMå»é‡å™¨"""
        self.llm_client = llm_client
        self.logger = get_logger()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_processed": 0,
            "url_duplicates": 0,
            "content_duplicates": 0,
            "semantic_duplicates": 0,
            "unique_jobs": 0,
            "llm_calls": 0,
            "processing_time": 0.0
        }
    
    async def deduplicate_jobs(self, jobs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½å»é‡"""
        if not jobs:
            return []
        
        start_time = time.time()
        self.stats["total_processed"] = len(jobs)
        
        self.logger.info("Starting LLM smart deduplication", {
            "input_count": len(jobs),
            "llm_provider": getattr(self.llm_client, 'provider', 'unknown')
        })
        
        try:
            # é¦–å…ˆè¿›è¡Œå¿«é€Ÿçš„URLå’ŒåŸºç¡€å†…å®¹å»é‡
            quick_dedup_jobs = self._quick_deduplicate(jobs)
            
            # ç„¶åè¿›è¡ŒLLMè¯­ä¹‰å»é‡
            if len(quick_dedup_jobs) > 1:
                semantic_unique_jobs = await self._semantic_deduplicate(quick_dedup_jobs)
            else:
                semantic_unique_jobs = quick_dedup_jobs
            
            self.stats["unique_jobs"] = len(semantic_unique_jobs)
            self.stats["processing_time"] = time.time() - start_time
            
            self.logger.success("LLM smart deduplication completed", {
                "input_count": len(jobs),
                "output_count": len(semantic_unique_jobs),
                "url_duplicates": self.stats["url_duplicates"],
                "content_duplicates": self.stats["content_duplicates"],
                "semantic_duplicates": self.stats["semantic_duplicates"],
                "llm_calls": self.stats["llm_calls"],
                "processing_time": round(self.stats["processing_time"], 2)
            })
            
            return semantic_unique_jobs
            
        except Exception as e:
            self.logger.error("LLM deduplication failed", {"error": str(e)}, e)
            # å‘ç”Ÿé”™è¯¯æ—¶å›é€€åˆ°å¿«é€Ÿå»é‡ç»“æœ
            return quick_dedup_jobs if 'quick_dedup_jobs' in locals() else jobs
    
    def _quick_deduplicate(self, jobs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¿«é€Ÿå»é‡ï¼ˆURL + åŸºç¡€å†…å®¹ï¼‰"""
        url_cache = set()
        fingerprint_cache = set()
        unique_jobs = []
        
        for job in jobs:
            # URLå»é‡
            job_url = self._extract_job_id(job.get('å²—ä½é“¾æ¥', ''))
            if job_url and job_url in url_cache:
                self.stats["url_duplicates"] += 1
                continue
            
            # å†…å®¹æŒ‡çº¹å»é‡
            fingerprint = self._create_simple_fingerprint(job)
            if fingerprint in fingerprint_cache:
                self.stats["content_duplicates"] += 1
                continue
            
            # æ·»åŠ åˆ°ç¼“å­˜
            if job_url:
                url_cache.add(job_url)
            fingerprint_cache.add(fingerprint)
            
            unique_jobs.append(job)
        
        return unique_jobs
    
    async def _semantic_deduplicate(self, jobs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¯­ä¹‰å»é‡ï¼ˆä½¿ç”¨LLMï¼‰"""
        if len(jobs) <= 1:
            return jobs
        
        unique_jobs = []
        
        for i, current_job in enumerate(jobs):
            is_duplicate = False
            
            # ä¸å·²ç¡®è®¤å”¯ä¸€çš„å²—ä½è¿›è¡Œè¯­ä¹‰æ¯”è¾ƒ
            for unique_job in unique_jobs:
                if await self._are_semantically_similar(current_job, unique_job):
                    self.stats["semantic_duplicates"] += 1
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_jobs.append(current_job)
        
        return unique_jobs
    
    async def _are_semantically_similar(self, job1: Dict[str, Any], job2: Dict[str, Any]) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªå²—ä½æ˜¯å¦è¯­ä¹‰ç›¸ä¼¼"""
        try:
            self.stats["llm_calls"] += 1
            
            # æ„å»ºæ¯”è¾ƒprompt
            prompt = self._build_comparison_prompt(job1, job2)
            
            # è°ƒç”¨LLM
            messages = [{"role": "user", "content": prompt}]
            response = await self.llm_client._call_llm_api(messages)
            
            # è§£æç»“æœ
            if response:
                return "æ˜¯" in response or "ç›¸ä¼¼" in response or "é‡å¤" in response
            else:
                return False
            
        except Exception as e:
            self.logger.warning("LLM semantic comparison failed", {
                "job1": job1.get("å²—ä½åç§°", "N/A"),
                "job2": job2.get("å²—ä½åç§°", "N/A"),
                "error": str(e)
            })
            # å‘ç”Ÿé”™è¯¯æ—¶ä¿å®ˆå¤„ç†ï¼Œè®¤ä¸ºä¸é‡å¤
            return False
    
    def _build_comparison_prompt(self, job1: Dict[str, Any], job2: Dict[str, Any]) -> str:
        """æ„å»ºå²—ä½æ¯”è¾ƒçš„prompt"""
        return f"""è¯·åˆ¤æ–­ä»¥ä¸‹ä¸¤ä¸ªå²—ä½æ˜¯å¦æ˜¯é‡å¤çš„ï¼ˆåŒä¸€ä¸ªå²—ä½ï¼‰ï¼š

å²—ä½1ï¼š
- å²—ä½åç§°ï¼š{job1.get('å²—ä½åç§°', 'N/A')}
- å…¬å¸åç§°ï¼š{job1.get('å…¬å¸åç§°', 'N/A')}
- å·¥ä½œåœ°ç‚¹ï¼š{job1.get('å·¥ä½œåœ°ç‚¹', 'N/A')}
- è–ªèµ„ï¼š{job1.get('è–ªèµ„', 'N/A')}

å²—ä½2ï¼š
- å²—ä½åç§°ï¼š{job2.get('å²—ä½åç§°', 'N/A')}
- å…¬å¸åç§°ï¼š{job2.get('å…¬å¸åç§°', 'N/A')}
- å·¥ä½œåœ°ç‚¹ï¼š{job2.get('å·¥ä½œåœ°ç‚¹', 'N/A')}
- è–ªèµ„ï¼š{job2.get('è–ªèµ„', 'N/A')}

åˆ¤æ–­æ ‡å‡†ï¼š
1. å…¬å¸åç§°å®Œå…¨ç›¸åŒæˆ–é«˜åº¦ç›¸ä¼¼
2. å²—ä½åç§°è¡¨è¾¾ç›¸åŒèŒä½
3. å·¥ä½œåœ°ç‚¹ç›¸åŒæˆ–ç›¸è¿‘
4. è–ªèµ„èŒƒå›´é‡å 

è¯·å›ç­”ï¼šæ˜¯ æˆ– å¦"""
    
    def _extract_job_id(self, url: str) -> str:
        """ä»URLä¸­æå–å²—ä½ID"""
        if not url:
            return ""
        
        base_url = url.split('?')[0]
        match = re.search(r'/job_detail/([^/.]+)', base_url)
        return match.group(1) if match else base_url.split('/')[-1] if '/' in base_url else base_url
    
    def _create_simple_fingerprint(self, job: Dict[str, Any]) -> str:
        """åˆ›å»ºç®€å•æŒ‡çº¹"""
        company = job.get('å…¬å¸åç§°', '').strip().lower()
        title = job.get('å²—ä½åç§°', '').strip().lower()
        location = job.get('å·¥ä½œåœ°ç‚¹', '').strip().lower()
        
        # ç®€å•æ¸…ç†
        company = re.sub(r'æœ‰é™å…¬å¸$|ç§‘æŠ€æœ‰é™å…¬å¸$|ç½‘ç»œç§‘æŠ€æœ‰é™å…¬å¸$', '', company)
        title = re.sub(r'ï¼ˆ.*?ï¼‰|\(.*?\)', '', title)
        
        fingerprint = f"{company}_{title}_{location}"
        return hashlib.md5(fingerprint.encode('utf-8')).hexdigest()
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()


class NotionJobDeduplicator:
    """Notionæ•°æ®åº“å²—ä½å»é‡å™¨"""
    
    def __init__(self, notion_token: str, database_id: str, skip_notion_load: bool = False, notion_cache_file: str = None):
        """åˆå§‹åŒ–Notionå»é‡å™¨"""
        if not HAS_NOTION_CLIENT:
            raise ImportError("Notionå®¢æˆ·ç«¯æœªå®‰è£…ï¼Œè¯·å®‰è£…notion-clientåŒ…")
        
        self.notion = Client(auth=notion_token)
        self.database_id = database_id
        self.skip_notion_load = skip_notion_load
        self.logger = get_logger()
        self.snapshot = create_snapshot_manager()

        # å¤„ç†ç¼“å­˜æ–‡ä»¶å‚æ•°
        if skip_notion_load:
            if notion_cache_file:
                # ç”¨æˆ·æŒ‡å®šäº†æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨
                self.notion_cache_file = notion_cache_file
                self.logger.info("ä½¿ç”¨æŒ‡å®šçš„ç¼“å­˜æ–‡ä»¶", {"file": notion_cache_file})
            else:
                # ç”¨æˆ·æ²¡æœ‰æŒ‡å®šæ–‡ä»¶ï¼Œè‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„
                self.notion_cache_file = self._find_latest_cache_file()
                if self.notion_cache_file:
                    self.logger.info("è‡ªåŠ¨é€‰æ‹©æœ€æ–°ç¼“å­˜æ–‡ä»¶", {"file": self.notion_cache_file})
                else:
                    self.logger.warning("Cache file not found, falling back to API loading mode")
        else:
            self.notion_cache_file = None
        
        # ç¼“å­˜å·²å­˜åœ¨çš„å²—ä½
        self.existing_jobs_cache = {}
        self.cache_loaded = False
        
        self.logger.debug("Notionå»é‡å™¨åˆå§‹åŒ–å®Œæˆ", {
            "database_id": database_id[:8] + "...",
            "notion_client": "å·²è¿æ¥",
            "skip_notion_load": skip_notion_load,
            "cache_file": self.notion_cache_file or "æ— "
        })
    
    @log_function_call("Notionå»é‡å¤„ç†")
    async def deduplicate_against_notion(self, jobs: List[Dict[str, Any]]) -> Tuple[List[Dict], List[Dict]]:
        """å¯¹æ¯”Notionæ•°æ®åº“è¿›è¡Œå»é‡"""
        if not jobs:
            self.logger.warning("Input job list is empty")
            return [], []
        
        self.logger.info("Starting Notion database deduplication", {
            "input_count": len(jobs),
            "database_id": self.database_id[:8] + "..."
        })
        
        try:
            # åŠ è½½Notionä¸­å·²å­˜åœ¨çš„å²—ä½
            if not self.cache_loaded:
                await self._load_existing_jobs()
            
            new_jobs = []
            duplicate_jobs = []
            
            for job in jobs:
                fingerprint = self._create_notion_fingerprint(job)
                
                if fingerprint in self.existing_jobs_cache:
                    duplicate_jobs.append(job)
                    self.logger.debug("å‘ç°é‡å¤å²—ä½", {
                        "job_title": job.get("å²—ä½åç§°", "N/A"),
                        "company": job.get("å…¬å¸åç§°", "N/A"),
                        "fingerprint": fingerprint[:8] + "..."
                    })
                else:
                    new_jobs.append(job)
                    # æ·»åŠ åˆ°ç¼“å­˜ä¸­ï¼Œé¿å…æ‰¹é‡å¤„ç†æ—¶çš„å†…éƒ¨é‡å¤
                    self.existing_jobs_cache[fingerprint] = {
                        "å²—ä½åç§°": job.get("å²—ä½åç§°", ""),
                        "å…¬å¸åç§°": job.get("å…¬å¸åç§°", ""),
                        "æ·»åŠ æ—¶é—´": datetime.now().isoformat()
                    }
            
            # ä¿å­˜å»é‡ç»“æœå¿«ç…§
            self.snapshot.capture("notion_dedup_result", {
                "new_jobs": new_jobs,
                "duplicate_jobs": duplicate_jobs
            }, {
                "stage": "Notion deduplication result",
                "new_count": len(new_jobs),
                "duplicate_count": len(duplicate_jobs)
            })
            
            result_message = "Notionå»é‡å®Œæˆ"
            if len(new_jobs) == 0:
                result_message += " - æ‰€æœ‰å²—ä½å·²å­˜åœ¨äºæ•°æ®åº“"
                self.logger.info_no_data(result_message, {
                    "input_count": len(jobs),
                    "new_jobs": len(new_jobs),
                    "duplicate_jobs": len(duplicate_jobs),
                    "duplicate_rate": f"{(len(duplicate_jobs)/len(jobs)*100):.1f}%",
                    "message": "æ•°æ®åº“å·²åŒ…å«æ‰€æœ‰å²—ä½ï¼Œæ— éœ€æ·»åŠ æ–°æ•°æ®"
                })
            else:
                result_message += f" - å‘ç°{len(new_jobs)}ä¸ªæ–°å²—ä½"
                self.logger.success(result_message, {
                    "input_count": len(jobs),
                    "new_jobs": len(new_jobs),
                    "duplicate_jobs": len(duplicate_jobs),
                    "duplicate_rate": f"{(len(duplicate_jobs)/len(jobs)*100):.1f}%"
                })
            
            return new_jobs, duplicate_jobs
            
        except Exception as e:
            self.logger.error("Notion deduplication failed", {"error": str(e)}, e)
            # å‘ç”Ÿé”™è¯¯æ—¶è¿”å›æ‰€æœ‰å²—ä½ä½œä¸ºæ–°å²—ä½ï¼Œé¿å…æ•°æ®ä¸¢å¤±
            return jobs, []
    
    def _find_latest_cache_file(self) -> Optional[str]:
        """æŸ¥æ‰¾æœ€æ–°çš„Notionç¼“å­˜æ–‡ä»¶"""
        import time
        
        cache_patterns = [
            "notion_cache_*.json",           # å½“å‰ç›®å½•
            "debug/notion_cache_*.json",     # debugç›®å½•
            "cache/notion_cache_*.json",     # cacheç›®å½•
            "data/notion_cache_*.json"       # dataç›®å½•
        ]
        
        all_cache_files = []
        for pattern in cache_patterns:
            all_cache_files.extend(glob.glob(pattern))
        
        if not all_cache_files:
            return None
        
        # è¿”å›æœ€æ–°çš„æ–‡ä»¶
        latest_file = max(all_cache_files, key=os.path.getmtime)
        
        # æ£€æŸ¥æ–‡ä»¶å¹´é¾„ï¼Œå¦‚æœå¤ªæ—§åˆ™è­¦å‘Š
        file_age = time.time() - os.path.getmtime(latest_file)
        age_hours = file_age / 3600
        
        if age_hours > 24:  # å¦‚æœæ–‡ä»¶è¶…è¿‡24å°æ—¶
            self.logger.warning("Cache file is old", {
                "file": os.path.basename(latest_file),
                "age_hours": round(age_hours, 1),
                "suggestion": "è€ƒè™‘é‡æ–°åŠ è½½ä»¥è·å–æœ€æ–°æ•°æ®"
            })
        
        return latest_file
    
    async def _load_from_cache(self) -> bool:
        """ä»ç¼“å­˜æ–‡ä»¶åŠ è½½æ•°æ®"""
        try:
            if not os.path.exists(self.notion_cache_file):
                self.logger.warning("Cache file does not exist", {"file": self.notion_cache_file})
                return False
            
            self.logger.info("Loading Notion data from cache file", {"file": self.notion_cache_file})
            
            with open(self.notion_cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # é‡å»ºæŒ‡çº¹ç¼“å­˜
            self.existing_jobs_cache = {}
            loaded_count = 0
            
            for job_data in cache_data.get('jobs', []):
                fingerprint = self._create_notion_fingerprint(job_data)
                self.existing_jobs_cache[fingerprint] = job_data
                loaded_count += 1
            
            self.cache_loaded = True
            
            self.logger.success("Cache data loading completed", {
                "file": os.path.basename(self.notion_cache_file),
                "total_jobs": len(cache_data.get('jobs', [])),
                "valid_jobs": loaded_count,
                "cache_timestamp": cache_data.get('timestamp', 'unknown')
            })
            return True
            
        except Exception as e:
            self.logger.error("Cache loading failed", {"error": str(e)}, e)
            return False
        
    async def _save_to_cache(self, jobs_data: List[Dict]):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜æ–‡ä»¶"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            cache_file = f"debug/notion_cache_{timestamp}.json"
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(cache_file), exist_ok=True)
            
            cache_data = {
                "timestamp": datetime.now().isoformat(),
                "database_id": self.database_id,
                "total_jobs": len(jobs_data),
                "jobs": jobs_data
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info("Notionæ•°æ®å·²ä¿å­˜åˆ°ç¼“å­˜", {
                "cache_file": cache_file,
                "jobs_count": len(jobs_data)
            })
            
        except Exception as e:
            self.logger.warning("Cache file save failed", {"error": str(e)})

    async def _load_existing_jobs(self):
        """åŠ è½½Notionä¸­å·²å­˜åœ¨çš„å²—ä½"""
        # å¦‚æœå¯ç”¨è·³è¿‡æ¨¡å¼ä¸”æœ‰ç¼“å­˜æ–‡ä»¶ï¼Œå°è¯•ä»ç¼“å­˜åŠ è½½
        if self.skip_notion_load and self.notion_cache_file:
            if await self._load_from_cache():
                return
            else:
                self.logger.warning("Cache file loading failed, falling back to API loading mode")
        
        # ä»Notion APIåŠ è½½æ•°æ®
        await self._load_from_notion_api()

    async def _load_from_notion_api(self):
        """ä»Notion APIåŠ è½½æ•°æ®"""
        try:
            self.logger.info("Starting to load job data from Notion API")
            
            all_results = []
            has_more = True
            next_cursor = None
            page_count = 0
            
            while has_more:
                query_params = {
                    "database_id": self.database_id,
                    "page_size": 100
                }
                
                if next_cursor:
                    query_params["start_cursor"] = next_cursor
                
                response = self.notion.databases.query(**query_params)
                
                all_results.extend(response["results"])
                has_more = response["has_more"]
                next_cursor = response.get("next_cursor")
                page_count += 1
                
                self.logger.debug(f"åŠ è½½ç¬¬{page_count}é¡µ", {
                    "page_size": len(response["results"]),
                    "total_loaded": len(all_results),
                    "has_more": has_more
                })
                
                # é¿å…APIé™æµ
                if has_more:
                    await asyncio.sleep(0.5)
            
            # æ„å»ºæŒ‡çº¹ç¼“å­˜
            jobs_data = []
            for result in all_results:
                job_data = self._extract_job_data_from_notion(result)
                if job_data:
                    fingerprint = self._create_notion_fingerprint(job_data)
                    self.existing_jobs_cache[fingerprint] = job_data
                    jobs_data.append(job_data)
            
            # ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶ï¼ˆåªåœ¨æ­£å¸¸APIåŠ è½½æ—¶ä¿å­˜ï¼‰
            if not self.skip_notion_load:
                await self._save_to_cache(jobs_data)
            
            self.cache_loaded = True
            
            self.logger.success("Notion API data loading completed", {
                "total_pages": page_count,
                "total_jobs": len(all_results),
                "valid_jobs": len(self.existing_jobs_cache)
            })
            
        except Exception as e:
            self.logger.error("Failed to load data from Notion API", {"error": str(e)}, e)
            raise
    
    def _extract_job_data_from_notion(self, result: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ä»Notionç»“æœä¸­æå–å²—ä½æ•°æ®"""
        try:
            properties = result.get("properties", {})
            job_data = {}
            
            # æå–å²—ä½åç§°
            title_prop = properties.get("å²—ä½åç§°", {})
            if title_prop.get("title"):
                job_data["å²—ä½åç§°"] = title_prop["title"][0]["plain_text"]
            
            # æå–å…¬å¸åç§°
            company_prop = properties.get("å…¬å¸åç§°", {})
            if company_prop.get("rich_text"):
                job_data["å…¬å¸åç§°"] = company_prop["rich_text"][0]["plain_text"]
            
            # æå–å·¥ä½œåœ°ç‚¹
            location_prop = properties.get("å·¥ä½œåœ°ç‚¹", {})
            if location_prop.get("rich_text"):
                job_data["å·¥ä½œåœ°ç‚¹"] = location_prop["rich_text"][0]["plain_text"]
            
            # æå–å²—ä½é“¾æ¥
            url_prop = properties.get("å²—ä½é“¾æ¥", {})
            if url_prop.get("url"):
                job_data["å²—ä½é“¾æ¥"] = url_prop["url"]
            
            return job_data if any(job_data.values()) else None
            
        except Exception as e:
            self.logger.warning("Failed to extract Notion job data", {
                "notion_id": result.get("id", "N/A"),
                "error": str(e)
            })
            return None
    
    def _create_notion_fingerprint(self, job_data: Dict[str, Any]) -> str:
        """åˆ›å»ºNotionæŒ‡çº¹ï¼ˆç”¨äºå»é‡ï¼‰"""
        company = job_data.get('å…¬å¸åç§°', '').strip().lower()
        title = job_data.get('å²—ä½åç§°', '').strip().lower()
        location = job_data.get('å·¥ä½œåœ°ç‚¹', '').strip().lower()
        url = job_data.get('å²—ä½é“¾æ¥', '').strip()
        
        # ç®€å•æ¸…ç†
        company = re.sub(r'æœ‰é™å…¬å¸$|ç§‘æŠ€æœ‰é™å…¬å¸$|ç½‘ç»œç§‘æŠ€æœ‰é™å…¬å¸$', '', company)
        title = re.sub(r'ï¼ˆ.*?ï¼‰|\(.*?\)', '', title)
        location = re.sub(r'Â·.*$|-.*$', '', location)

        # Extract job ID from URL for better deduplication
        job_id = self._extract_job_id(url) if url else ''

        # Prioritize URL-based fingerprint, fall back to content-based
        if job_id:
            fingerprint = f"url_{job_id}"
        else:
            fingerprint = f"content_{company}_{title}_{location}"
        
        return hashlib.md5(fingerprint.encode('utf-8')).hexdigest()
    
    def _extract_job_id(self, url: str) -> str:
        """ä»URLä¸­æå–å²—ä½ID"""
        if not url:
            return ""
        
        # å¤„ç†ä¸åŒæ‹›è˜ç½‘ç«™çš„URLæ ¼å¼
        base_url = url.split('?')[0]
        
        # Bossç›´è˜
        if 'zhipin.com' in url:
            match = re.search(r'/job_detail/([^/.]+)', base_url)
            return match.group(1) if match else base_url.split('/')[-1] if '/' in base_url else base_url
        
        # å…¶ä»–ç½‘ç«™çš„é€šç”¨å¤„ç†
        return base_url.split('/')[-1] if '/' in base_url else base_url
    
    async def load_existing_jobs(self):
        return await self._load_existing_jobs()

    def get_fingerprint_details(self, job_data: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–æŒ‡çº¹ç”Ÿæˆè¯¦æƒ…ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
        original_company = job_data.get('å…¬å¸åç§°', '')
        original_title = job_data.get('å²—ä½åç§°', '')
        original_location = job_data.get('å·¥ä½œåœ°ç‚¹', '')
        
        cleaned_company = re.sub(r'æœ‰é™å…¬å¸$|ç§‘æŠ€æœ‰é™å…¬å¸$|ç½‘ç»œç§‘æŠ€æœ‰é™å…¬å¸$', '', original_company.strip().lower())
        cleaned_title = re.sub(r'ï¼ˆ.*?ï¼‰|\(.*?\)', '', original_title.strip().lower())
        cleaned_location = re.sub(r'Â·.*$|-.*', '', original_location.strip().lower())
        
        fingerprint = f"{cleaned_company}_{cleaned_title}_{cleaned_location}"
        
        return {
            "original": {
                "company": original_company,
                "title": original_title,
                "location": original_location
            },
            "cleaned": {
                "company": cleaned_company,
                "title": cleaned_title,
                "location": cleaned_location
            },
            "fingerprint": hashlib.md5(fingerprint.encode('utf-8')).hexdigest(),
            "fingerprint_string": fingerprint
        }


# æµ‹è¯•å‡½æ•°
async def test_enhanced_deduplicator():
    """æµ‹è¯•å¢å¼ºç‰ˆå»é‡å™¨"""
    print("ğŸ§ª æµ‹è¯•å¢å¼ºç‰ˆå»é‡å™¨")
    
    # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
    from src.logger_config import init_logger, LogLevel
    init_logger(LogLevel.DEBUG, enable_file_logging=True)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_jobs = [
        {
            "å²—ä½åç§°": "Pythonå¼€å‘å·¥ç¨‹å¸ˆ",
            "å…¬å¸åç§°": "é˜¿é‡Œå·´å·´ç§‘æŠ€æœ‰é™å…¬å¸",
            "å·¥ä½œåœ°ç‚¹": "æ­å·Â·è¥¿æ¹–åŒº",
            "è–ªèµ„": "20-35K",
            "å²—ä½é“¾æ¥": "https://www.zhipin.com/job_detail/12345.html"
        },
        {
            "å²—ä½åç§°": "Pythonå¼€å‘å·¥ç¨‹å¸ˆï¼ˆæ€¥æ‹›ï¼‰",
            "å…¬å¸åç§°": "é˜¿é‡Œå·´å·´ç§‘æŠ€æœ‰é™å…¬å¸",
            "å·¥ä½œåœ°ç‚¹": "æ­å·Â·è¥¿æ¹–åŒºÂ·æ–‡ä¸‰è·¯",
            "è–ªèµ„": "20-35K",
            "å²—ä½é“¾æ¥": "https://www.zhipin.com/job_detail/12345.html"  # ç›¸åŒURL
        },
        {
            "å²—ä½åç§°": "Javaå¼€å‘å·¥ç¨‹å¸ˆ",
            "å…¬å¸åç§°": "è…¾è®¯ç§‘æŠ€æœ‰é™å…¬å¸",
            "å·¥ä½œåœ°ç‚¹": "æ·±åœ³Â·å—å±±åŒº",
            "è–ªèµ„": "25-40K",
            "å²—ä½é“¾æ¥": "https://www.zhipin.com/job_detail/67890.html"
        }
    ]
    
    print(f"ğŸ“Š æµ‹è¯•æ•°æ®ï¼š{len(test_jobs)}ä¸ªå²—ä½")
    
    # æµ‹è¯•ä¼ ç»Ÿå»é‡
    print("\n1ï¸âƒ£ æµ‹è¯•ä¼ ç»Ÿå»é‡")
    deduplicator = EnhancedJobDeduplicator(use_llm=False)
    traditional_result = await deduplicator.deduplicate_jobs(test_jobs.copy())
    print(f"   ä¼ ç»Ÿå»é‡ç»“æœï¼š{len(traditional_result)}ä¸ªå”¯ä¸€å²—ä½")
    
    # å¦‚æœæœ‰LLMå®¢æˆ·ç«¯ï¼Œæµ‹è¯•LLMå»é‡
    if HAS_LLM_EXTRACTOR:
        print("\n2ï¸âƒ£ æµ‹è¯•LLMæ™ºèƒ½å»é‡")
        try:
            from llm_client import get_llm_client
            llm_client = get_llm_client()
            llm_deduplicator = EnhancedJobDeduplicator(llm_client=llm_client, use_llm=True)
            llm_result = await llm_deduplicator.deduplicate_jobs(test_jobs.copy())
            print(f"   LLMå»é‡ç»“æœï¼š{len(llm_result)}ä¸ªå”¯ä¸€å²—ä½")
        except Exception as e:
            print(f"   LLMå»é‡æµ‹è¯•å¤±è´¥ï¼š{e}")
    
    # æµ‹è¯•Notionå»é‡ï¼ˆå¦‚æœæœ‰é…ç½®ï¼‰
    if os.getenv("NOTION_TOKEN") and os.getenv("NOTION_DATABASE_ID"):
        print("\n3ï¸âƒ£ æµ‹è¯•Notionå»é‡")
        try:
            notion_deduplicator = NotionJobDeduplicator(
                os.getenv("NOTION_TOKEN"),
                os.getenv("NOTION_DATABASE_ID")
            )
            new_jobs, duplicate_jobs = await notion_deduplicator.deduplicate_against_notion(test_jobs.copy())
            print(f"   Notionå»é‡ç»“æœï¼š{len(new_jobs)}ä¸ªæ–°å²—ä½ï¼Œ{len(duplicate_jobs)}ä¸ªé‡å¤å²—ä½")
        except Exception as e:
            print(f"   Notionå»é‡æµ‹è¯•å¤±è´¥ï¼š{e}")
    
    print("\nâœ… æµ‹è¯•å®Œæˆ")
    print("ğŸ“ æŸ¥çœ‹ç”Ÿæˆçš„è°ƒè¯•æ–‡ä»¶:")
    print("   - debug/pipeline_*.log")
    print("   - debug/debug_session_*.json")
    print("   - debug/snapshots/")


if __name__ == "__main__":
    """ç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶è¿›è¡Œæµ‹è¯•"""
    print("ğŸ”§ å¢å¼ºç‰ˆå²—ä½å»é‡å™¨ - è¯­æ³•ä¿®å¤æµ‹è¯•æ¨¡å¼")
    print("=" * 60)
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_enhanced_deduplicator())
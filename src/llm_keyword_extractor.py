"""
åŸºäºLLMçš„æ™ºèƒ½å…³é”®è¯æå–å™¨
å®Œå…¨ä¾èµ–LLMè¿›è¡Œè¯­ä¹‰ç†è§£å’Œå…³é”®è¯æå–
"""
import httpx
import asyncio
import json
import re
import hashlib
from typing import List, Dict, Any, Optional, Set

class LLMKeywordExtractor:
    """åŸºäºLLMçš„å…³é”®è¯æå–å™¨"""
    
    def __init__(self, llm_client):
        """
        åˆå§‹åŒ–LLMå…³é”®è¯æå–å™¨
        
        Args:
            llm_client: å·²åˆå§‹åŒ–çš„LLMå®¢æˆ·ç«¯ï¼ˆå¦‚EnhancedNotionExtractorï¼‰
        """
        self.llm_client = llm_client
        self.keyword_cache = {}  # ç¼“å­˜LLMç»“æœï¼Œé¿å…é‡å¤è°ƒç”¨
    
    async def extract_discriminative_keywords(self, description: str) -> str:
        """æå–å…·æœ‰åŒºåˆ†åº¦çš„å…³é”®è¯"""
        if not description or not description.strip():
            return ""
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = hashlib.md5(description.encode()).hexdigest()
        if cache_key in self.keyword_cache:
            print(f"ğŸ”„ ä½¿ç”¨ç¼“å­˜çš„å…³é”®è¯æå–ç»“æœ")
            return self.keyword_cache[cache_key]
        
        try:
            print(f"ğŸ§  ä½¿ç”¨LLMæå–å…³é”®è¯...")
            keywords = await self._call_llm_for_keywords(description)
            
            # ç¼“å­˜ç»“æœ
            self.keyword_cache[cache_key] = keywords
            print(f"âœ… LLMæå–å…³é”®è¯: {keywords}")
            
            return keywords
            
        except Exception as e:
            print(f"âš ï¸  LLMå…³é”®è¯æå–å¤±è´¥: {e}")
            return self._fallback_simple_extraction(description)
    
    async def _call_llm_for_keywords(self, description: str) -> str:
        """è°ƒç”¨LLMæå–å…³é”®è¯"""
        
        prompt = f"""è¯·åˆ†æä»¥ä¸‹å²—ä½æè¿°ï¼Œæå–3-5ä¸ªæœ€èƒ½åŒºåˆ†ä¸åŒå²—ä½çš„å…³é”®è¯ã€‚

å²—ä½æè¿°ï¼š
{description}

æå–è¦æ±‚ï¼š
1. **ä¼˜å…ˆçº§æ’åº**ï¼š
   - æœ€é«˜ä¼˜å…ˆçº§ï¼šå…¬å¸éƒ¨é—¨/å›¢é˜Ÿåç§°ï¼ˆå¦‚"åä¸ºäº‘"ã€"ç»ˆç«¯BG"ã€"å¾®ä¿¡å›¢é˜Ÿ"ï¼‰
   - é«˜ä¼˜å…ˆçº§ï¼šäº§å“/å¹³å°åç§°ï¼ˆå¦‚"æŠ–éŸ³"ã€"HarmonyOS"ã€"è…¾è®¯äº‘"ï¼‰
   - ä¸­ç­‰ä¼˜å…ˆçº§ï¼šä¸šåŠ¡æ–¹å‘ï¼ˆå¦‚"æ¨èç³»ç»Ÿ"ã€"æœç´¢å¼•æ“"ã€"å¹¿å‘Šç®—æ³•"ï¼‰
   - è¾ƒä½ä¼˜å…ˆçº§ï¼šç‰¹æ®ŠæŠ€æœ¯è¦æ±‚ï¼ˆå¦‚"å¤§æ¨¡å‹"ã€"è®¡ç®—æœºè§†è§‰"ï¼‰

2. **åŒºåˆ†åº¦è¦æ±‚**ï¼š
   - é€‰æ‹©çš„å…³é”®è¯åº”è¯¥èƒ½æœ‰æ•ˆåŒºåˆ†ä¸åŒçš„å²—ä½
   - é¿å…è¿‡äºé€šç”¨çš„è¯æ±‡ï¼ˆå¦‚"ç®—æ³•"ã€"å¼€å‘"ã€"å·¥ç¨‹å¸ˆ"ï¼‰
   - é¿å…åŸºç¡€æŠ€æœ¯æ ˆï¼ˆå¦‚"Python"ã€"Java"ã€"MySQL"ï¼‰

3. **è¾“å‡ºæ ¼å¼**ï¼š
   - åªè¿”å›å…³é”®è¯ï¼Œç”¨è‹±æ–‡é€—å·åˆ†éš”
   - ä¸è¦è§£é‡Šï¼Œä¸è¦ç¼–å·
   - å…³é”®è¯æŒ‰é‡è¦æ€§æ’åº

ç¤ºä¾‹ï¼š
è¾“å…¥ï¼šåä¸ºäº‘AIå›¢é˜Ÿæ‹›è˜æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆï¼Œè´Ÿè´£æ¨èç³»ç»Ÿç®—æ³•å¼€å‘ï¼Œç†Ÿæ‚‰PyTorch
è¾“å‡ºï¼šåä¸ºäº‘,AIå›¢é˜Ÿ,æ¨èç³»ç»Ÿ

è¾“å…¥ï¼šå­—èŠ‚è·³åŠ¨æŠ–éŸ³æ¨èå›¢é˜Ÿæ‹›è˜ç®—æ³•å·¥ç¨‹å¸ˆï¼Œè´Ÿè´£çŸ­è§†é¢‘æ¨èç®—æ³•ä¼˜åŒ–
è¾“å‡ºï¼šæŠ–éŸ³,æ¨èå›¢é˜Ÿ,çŸ­è§†é¢‘æ¨è

è¾“å…¥ï¼šè…¾è®¯å¾®ä¿¡æ”¯ä»˜å›¢é˜Ÿæ‹›è˜åç«¯å·¥ç¨‹å¸ˆï¼Œè´Ÿè´£æ”¯ä»˜ç³»ç»Ÿæ¶æ„è®¾è®¡
è¾“å‡ºï¼šå¾®ä¿¡,æ”¯ä»˜å›¢é˜Ÿ,æ”¯ä»˜ç³»ç»Ÿ

è¯·æå–å…³é”®è¯ï¼š"""

        messages = [{"role": "user", "content": prompt}]
        
        # ä½¿ç”¨ç°æœ‰çš„LLM APIè°ƒç”¨
        response = await self.llm_client._call_llm_api(messages, max_retries=2)

        print(f"LLMåŸå§‹è¿”å›: {response}")

        if response:
            # æ¸…ç†å’Œæ ‡å‡†åŒ–å…³é”®è¯
            keywords = self._clean_llm_response(response)
            print(f"æ¸…ç†åå…³é”®è¯: {keywords}")
            return keywords
        
        raise Exception("LLM APIè°ƒç”¨å¤±è´¥")
    
    def _clean_llm_response(self, response: str) -> str:
        """æ¸…ç†LLMå“åº”ï¼Œæå–çº¯å…³é”®è¯"""
        # ç§»é™¤å¯èƒ½çš„è§£é‡Šæ–‡å­—
        lines = response.strip().split('\n')
        
        # æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„è¡Œï¼ˆé€šå¸¸æ˜¯åŒ…å«é€—å·çš„è¡Œï¼‰
        keyword_line = ""
        for line in lines:
            line = line.strip()
            # è·³è¿‡æ˜æ˜¾çš„è§£é‡Šè¡Œ
            if any(prefix in line for prefix in ['è¾“å‡ºï¼š', 'å…³é”®è¯ï¼š', 'æå–ï¼š', 'ç»“æœï¼š']):
                keyword_line = re.sub(r'^[^ï¼š]*ï¼š\s*', '', line)
                break
            elif ',' in line and not any(word in line for word in ['è¦æ±‚', 'ç¤ºä¾‹', 'è¯´æ˜']):
                keyword_line = line
                break
        
        if not keyword_line:
            # å¦‚æœæ²¡æ‰¾åˆ°æ˜æ˜¾çš„å…³é”®è¯è¡Œï¼Œä½¿ç”¨ç¬¬ä¸€è¡Œ
            keyword_line = lines[0] if lines else response
        
        # æ¸…ç†å…³é”®è¯
        keywords = [kw.strip() for kw in keyword_line.split(',')]
        
        # è¿‡æ»¤æ— æ•ˆå…³é”®è¯
        valid_keywords = []
        for kw in keywords:
            # ç§»é™¤æ ‡ç‚¹ç¬¦å·
            kw = re.sub(r'[ã€‚ï¼ï¼Ÿï¼Œï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘]', '', kw)
            kw = kw.strip()
            
            # éªŒè¯å…³é”®è¯æœ‰æ•ˆæ€§
            if (len(kw) >= 2 and len(kw) <= 20 and 
                kw not in ['æ— ', 'æ— å…³é”®è¯', 'æš‚æ— ', 'N/A', 'NA'] and
                not kw.isdigit()):
                valid_keywords.append(kw)
        
        # é™åˆ¶æ•°é‡å¹¶è¿”å›
        return "_".join(valid_keywords[:5])
    
    def _fallback_simple_extraction(self, description: str) -> str:
        """LLMå¤±è´¥æ—¶çš„ç®€å•å›é€€ç­–ç•¥"""
        print(f"ğŸ”„ ä½¿ç”¨ç®€å•å›é€€ç­–ç•¥æå–å…³é”®è¯")
        
        # ç®€å•çš„å…³é”®è¯æ¨¡å¼åŒ¹é…
        patterns = [
            # å…¬å¸+éƒ¨é—¨
            r'([\u4e00-\u9fa5a-zA-Z]+(?:äº‘|ç«¯|ç§‘æŠ€|æŠ€æœ¯)[\u4e00-\u9fa5a-zA-Z]*(?:å›¢é˜Ÿ|å®éªŒå®¤|éƒ¨é—¨|äº‹ä¸šéƒ¨|BG))',
            
            # çŸ¥åäº§å“/å¹³å°
            r'(å¾®ä¿¡|QQ|æŠ–éŸ³|å¤´æ¡|æ·˜å®|é’‰é’‰|æ”¯ä»˜å®|ç™¾åº¦|æœç´¢)',
            r'(HarmonyOS|TikTok|WeChat|ChatGPT|Claude)',
            
            # ä¸šåŠ¡æ–¹å‘
            r'([\u4e00-\u9fa5]*(?:æ¨è|æœç´¢|å¹¿å‘Š|æ”¯ä»˜|é£æ§)[\u4e00-\u9fa5]*(?:ç³»ç»Ÿ|å¹³å°|ç®—æ³•|å›¢é˜Ÿ))',
            
            # æŠ€æœ¯æ–¹å‘
            r'(å¤§æ¨¡å‹|æœºå™¨å­¦ä¹ |æ·±åº¦å­¦ä¹ |è®¡ç®—æœºè§†è§‰|è‡ªç„¶è¯­è¨€å¤„ç†|è¯­éŸ³è¯†åˆ«)',
        ]
        
        found_keywords = []
        for pattern in patterns:
            matches = re.findall(pattern, description, re.IGNORECASE)
            found_keywords.extend(matches)
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_keywords = list(dict.fromkeys(found_keywords))  # ä¿æŒé¡ºåºå»é‡
        return "_".join(unique_keywords[:3]) if unique_keywords else ""

class LLMJobDeduplicator:
    """åŸºäºLLMå…³é”®è¯çš„å²—ä½å»é‡å™¨ - ä¿®å¤ç‰ˆ"""
    
    def __init__(self, llm_client):
        self.keyword_extractor = LLMKeywordExtractor(llm_client)
        self.url_cache = set()
        self.existing_jobs = []  # å­˜å‚¨å·²å¤„ç†çš„å²—ä½ä¿¡æ¯
        self.stats = {
            "total_processed": 0,
            "url_duplicates": 0,
            "semantic_duplicates": 0,  # è¯­ä¹‰é‡å¤
            "unique_jobs": 0
        }
        
        # è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼
        self.similarity_threshold = 0.5  # 50%ç›¸ä¼¼åº¦
    
    async def deduplicate_jobs(self, jobs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨LLMå…³é”®è¯å’Œè¯­ä¹‰ç›¸ä¼¼åº¦è¿›è¡Œå²—ä½å»é‡"""
        if not jobs:
            return []
        
        print(f"ğŸ§  å¼€å§‹LLMæ™ºèƒ½å»é‡å¤„ç† {len(jobs)} ä¸ªå²—ä½...")
        
        unique_jobs = []
        
        for i, job in enumerate(jobs, 1):
            self.stats["total_processed"] += 1
            
            print(f"ğŸ”„ å¤„ç†ç¬¬ {i}/{len(jobs)} ä¸ªå²—ä½: {job.get('å²—ä½åç§°', 'N/A')}")
            
            # ç¬¬ä¸€å±‚ï¼šURLå»é‡ï¼ˆå¿«é€Ÿç²¾ç¡®åŒ¹é…ï¼‰
            if await self._is_duplicate_by_url(job):
                self.stats["url_duplicates"] += 1
                print(f"   âš ï¸  URLé‡å¤ï¼Œè·³è¿‡")
                continue
            
            # ç¬¬äºŒå±‚ï¼šLLMè¯­ä¹‰å»é‡
            if await self._is_duplicate_by_semantic_similarity(job):
                self.stats["semantic_duplicates"] += 1
                print(f"   âš ï¸  è¯­ä¹‰é‡å¤ï¼Œè·³è¿‡")
                continue
            
            # ä¿ç•™å”¯ä¸€å²—ä½
            unique_jobs.append(job)
            self.stats["unique_jobs"] += 1
            print(f"   âœ… ä¿ç•™å”¯ä¸€å²—ä½")
        
        self._print_dedup_stats()
        return unique_jobs
    
    async def _is_duplicate_by_url(self, job: Dict[str, Any]) -> bool:
        """åŸºäºURLåˆ¤æ–­é‡å¤"""
        url = job.get('å²—ä½é“¾æ¥', '')
        if not url:
            return False
        
        # æ¸…ç†URLï¼Œåªä¿ç•™æ ¸å¿ƒéƒ¨åˆ†
        clean_url = self._clean_url(url)
        
        if clean_url in self.url_cache:
            return True
        
        self.url_cache.add(clean_url)
        return False
    
    async def _is_duplicate_by_semantic_similarity(self, job: Dict[str, Any]) -> bool:
        """åŸºäºLLMå…³é”®è¯çš„è¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤æ–­é‡å¤"""
        
        # æå–å½“å‰å²—ä½çš„å…³é”®è¯
        job_text = self._build_job_text(job)
        current_keywords = await self.keyword_extractor.extract_discriminative_keywords(job_text)
        
        if not current_keywords:
            print(f"   âš ï¸  æ— æ³•æå–å…³é”®è¯ï¼Œè·³è¿‡è¯­ä¹‰æ¯”è¾ƒ")
            return False
        
        print(f"âœ… LLMæå–å…³é”®è¯: {current_keywords}")
        
        # ä¸å·²æœ‰å²—ä½è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦æ¯”è¾ƒ
        for existing_job in self.existing_jobs:
            similarity = self._calculate_semantic_similarity(
                current_keywords, 
                existing_job['keywords'],
                job,
                existing_job['job_data']
            )
            
            if similarity >= self.similarity_threshold:
                print(f"   ğŸ¯ å‘ç°è¯­ä¹‰ç›¸ä¼¼å²—ä½ (ç›¸ä¼¼åº¦: {similarity:.1%})")
                print(f"      å½“å‰: {current_keywords}")
                print(f"      å·²æœ‰: {existing_job['keywords']}")
                return True
        
        # æ·»åŠ åˆ°å·²å¤„ç†åˆ—è¡¨
        self.existing_jobs.append({
            'keywords': current_keywords,
            'job_data': job,
            'fingerprint': self._create_semantic_fingerprint(job, current_keywords)
        })
        
        return False
    
    def _calculate_semantic_similarity(self, keywords1: str, keywords2: str, 
                                     job1: Dict, job2: Dict) -> float:
        """è®¡ç®—ä¸¤ä¸ªå²—ä½çš„è¯­ä¹‰ç›¸ä¼¼åº¦"""
        
        # åˆ†è§£å…³é”®è¯
        kw1_set = set(kw.strip().lower() for kw in keywords1.split('_') if kw.strip())
        kw2_set = set(kw.strip().lower() for kw in keywords2.split('_') if kw.strip())
        
        # 1. å…³é”®è¯äº¤é›†ç›¸ä¼¼åº¦
        intersection = len(kw1_set.intersection(kw2_set))
        union = len(kw1_set.union(kw2_set))
        keyword_similarity = intersection / union if union > 0 else 0
        
        # 2. å…¬å¸ç›¸ä¼¼åº¦
        company_similarity = self._calculate_company_similarity(
            job1.get('å…¬å¸åç§°', ''), 
            job2.get('å…¬å¸åç§°', '')
        )
        
        # 3. åœ°ç‚¹ç›¸ä¼¼åº¦
        location_similarity = self._calculate_location_similarity(
            job1.get('å·¥ä½œåœ°ç‚¹', ''),
            job2.get('å·¥ä½œåœ°ç‚¹', '')
        )
        
        # 4. ä¸šåŠ¡é¢†åŸŸç›¸ä¼¼åº¦ï¼ˆåŸºäºå…³é”®è¯ä¸­çš„ä¸šåŠ¡è¯æ±‡ï¼‰
        business_similarity = self._calculate_business_similarity(kw1_set, kw2_set)
        
        # ç»¼åˆç›¸ä¼¼åº¦è®¡ç®—ï¼ˆåŠ æƒå¹³å‡ï¼‰
        total_similarity = (
            keyword_similarity * 0.4 +      # å…³é”®è¯ç›¸ä¼¼åº¦æƒé‡40%
            company_similarity * 0.3 +      # å…¬å¸ç›¸ä¼¼åº¦æƒé‡30%  
            business_similarity * 0.2 +     # ä¸šåŠ¡ç›¸ä¼¼åº¦æƒé‡20%
            location_similarity * 0.1       # åœ°ç‚¹ç›¸ä¼¼åº¦æƒé‡10%
        )
        
        return total_similarity
    
    def _calculate_company_similarity(self, company1: str, company2: str) -> float:
        """è®¡ç®—å…¬å¸åç§°ç›¸ä¼¼åº¦"""
        if not company1 or not company2:
            return 0.0
        
        # æ ‡å‡†åŒ–å…¬å¸åç§°
        c1 = self._normalize_company_name(company1)
        c2 = self._normalize_company_name(company2)
        
        if c1 == c2:
            return 1.0
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³ç³»ï¼ˆå¦‚"åä¸ºæŠ€æœ¯" vs "åä¸º"ï¼‰
        if c1 in c2 or c2 in c1:
            return 0.8
        
        return 0.0
    
    def _calculate_location_similarity(self, loc1: str, loc2: str) -> float:
        """è®¡ç®—åœ°ç‚¹ç›¸ä¼¼åº¦"""
        if not loc1 or not loc2:
            return 0.0
        
        # æ ‡å‡†åŒ–åœ°ç‚¹
        l1 = self._normalize_location(loc1)
        l2 = self._normalize_location(loc2)
        
        if l1 == l2:
            return 1.0
        
        # æ£€æŸ¥æ˜¯å¦åŒåŸï¼ˆå¦‚"åŒ—äº¬æµ·æ·€åŒº" vs "åŒ—äº¬æœé˜³åŒº"ï¼‰
        city1 = re.sub(r'[Â·\s]*[^ï¼Œã€‚\s]*åŒº.*', '', l1)
        city2 = re.sub(r'[Â·\s]*[^ï¼Œã€‚\s]*åŒº.*', '', l2)
        
        if city1 and city2 and city1 == city2:
            return 0.7
        
        return 0.0
    
    def _calculate_business_similarity(self, kw1_set: Set[str], kw2_set: Set[str]) -> float:
        """è®¡ç®—ä¸šåŠ¡é¢†åŸŸç›¸ä¼¼åº¦"""
        
        # å®šä¹‰ä¸šåŠ¡é¢†åŸŸå…³é”®è¯
        business_domains = {
            'recommendation': {'æ¨è', 'æ¨èç³»ç»Ÿ', 'ä¸ªæ€§åŒ–æ¨è', 'æ¨èç®—æ³•', 'æ¨èå¼•æ“'},
            'computer_vision': {'è®¡ç®—æœºè§†è§‰', 'cv', 'å›¾åƒè¯†åˆ«', 'è§†è§‰ai', 'å›¾åƒå¤„ç†'},
            'nlp': {'è‡ªç„¶è¯­è¨€å¤„ç†', 'nlp', 'æ–‡æœ¬åˆ†æ', 'è¯­éŸ³è¯†åˆ«', 'å¯¹è¯ç³»ç»Ÿ'},
            'ai_platform': {'aiå›¢é˜Ÿ', 'äººå·¥æ™ºèƒ½', 'aiå®éªŒå®¤', 'aiéƒ¨é—¨'},
            'cloud': {'äº‘æœåŠ¡', 'äº‘è®¡ç®—', 'äº‘å¹³å°', 'åä¸ºäº‘', 'è…¾è®¯äº‘'},
            'mobile': {'ç§»åŠ¨ç«¯', 'æ‰‹æœº', 'ç»ˆç«¯', 'app', 'ç§»åŠ¨åº”ç”¨'}
        }
        
        # æ‰¾å‡ºæ¯ä¸ªå²—ä½çš„ä¸šåŠ¡é¢†åŸŸ
        domains1 = self._extract_business_domains(kw1_set, business_domains)
        domains2 = self._extract_business_domains(kw2_set, business_domains)
        
        if not domains1 or not domains2:
            return 0.0
        
        # è®¡ç®—ä¸šåŠ¡é¢†åŸŸäº¤é›†
        common_domains = domains1.intersection(domains2)
        total_domains = domains1.union(domains2)
        
        return len(common_domains) / len(total_domains) if total_domains else 0.0
    
    def _extract_business_domains(self, keywords: Set[str], business_domains: Dict) -> Set[str]:
        """ä»å…³é”®è¯ä¸­æå–ä¸šåŠ¡é¢†åŸŸ"""
        found_domains = set()
        
        for domain, domain_keywords in business_domains.items():
            for keyword in keywords:
                if any(domain_kw in keyword.lower() for domain_kw in domain_keywords):
                    found_domains.add(domain)
                    break
        
        return found_domains
    
    def _normalize_company_name(self, company: str) -> str:
        """æ ‡å‡†åŒ–å…¬å¸åç§°"""
        if not company:
            return ""
        
        company = company.strip()
        
        # ç§»é™¤å¸¸è§åç¼€
        suffixes = [
            r'æœ‰é™å…¬å¸$', r'è‚¡ä»½æœ‰é™å…¬å¸$', r'ç§‘æŠ€æœ‰é™å…¬å¸$',
            r'æŠ€æœ¯æœ‰é™å…¬å¸$', r'ä¿¡æ¯æŠ€æœ¯æœ‰é™å…¬å¸$', r'ç½‘ç»œç§‘æŠ€æœ‰é™å…¬å¸$',
            r'\(.*?\)', r'ï¼ˆ.*?ï¼‰'  # ç§»é™¤æ‹¬å·å†…å®¹
        ]
        
        for suffix in suffixes:
            company = re.sub(suffix, '', company)
        
        return company.strip().lower()
    
    def _normalize_location(self, location: str) -> str:
        """æ ‡å‡†åŒ–åœ°ç‚¹"""
        if not location:
            return ""
        
        location = location.strip()
        
        # ç§»é™¤è¯¦ç»†åŒºåŸŸä¿¡æ¯ï¼Œä¿ç•™ä¸»è¦åŸå¸‚å’ŒåŒº
        location = re.sub(r'[Â·\s]*', '', location)  # ç§»é™¤ç‰¹æ®Šåˆ†éš”ç¬¦
        
        return location.strip().lower()
    
    def _create_semantic_fingerprint(self, job: Dict[str, Any], keywords: str) -> str:
        """åˆ›å»ºè¯­ä¹‰æŒ‡çº¹ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
        company = self._normalize_company_name(job.get('å…¬å¸åç§°', ''))
        
        # æå–æ ¸å¿ƒä¸šåŠ¡å…³é”®è¯
        kw_list = [kw.strip() for kw in keywords.split('_') if kw.strip()]
        core_keywords = '_'.join(sorted(kw_list[:3]))  # å–å‰3ä¸ªå…³é”®è¯å¹¶æ’åº
        
        fingerprint = f"{company}_{core_keywords}"
        return hashlib.md5(fingerprint.encode()).hexdigest()
    
    def _build_job_text(self, job: Dict[str, Any]) -> str:
        """æ„å»ºç”¨äºLLMåˆ†æçš„å²—ä½æ–‡æœ¬"""
        components = []
        
        # åŸºç¡€ä¿¡æ¯
        if job.get('å²—ä½åç§°'):
            components.append(f"å²—ä½ï¼š{job['å²—ä½åç§°']}")
        
        if job.get('å…¬å¸åç§°'):
            components.append(f"å…¬å¸ï¼š{job['å…¬å¸åç§°']}")
        
        if job.get('å·¥ä½œåœ°ç‚¹'):
            components.append(f"åœ°ç‚¹ï¼š{job['å·¥ä½œåœ°ç‚¹']}")
        
        # å²—ä½æè¿°ï¼ˆæœ€é‡è¦ï¼‰
        if job.get('å²—ä½æè¿°'):
            components.append(f"æè¿°ï¼š{job['å²—ä½æè¿°']}")
        
        return "\n".join(components)
    
    def _clean_url(self, url: str) -> str:
        """æ¸…ç†URL"""
        if not url:
            return ""
        
        # ç§»é™¤æŸ¥è¯¢å‚æ•°
        base_url = url.split('?')[0]
        
        # æå–å²—ä½ID
        match = re.search(r'/job_detail/([^/.]+)', base_url)
        return match.group(1) if match else base_url
    
    def _print_dedup_stats(self):
        """æ‰“å°å»é‡ç»Ÿè®¡"""
        print(f"\nğŸ“Š LLMæ™ºèƒ½å»é‡ç»Ÿè®¡:")
        print(f"   æ€»å¤„ç†: {self.stats['total_processed']} ä¸ª")
        print(f"   URLé‡å¤: {self.stats['url_duplicates']} ä¸ª")
        print(f"   è¯­ä¹‰é‡å¤: {self.stats['semantic_duplicates']} ä¸ª")
        print(f"   ä¿ç•™å”¯ä¸€: {self.stats['unique_jobs']} ä¸ª")
        
        if self.stats['total_processed'] > 0:
            dedup_rate = ((self.stats['url_duplicates'] + self.stats['semantic_duplicates']) / 
                         self.stats['total_processed']) * 100
            print(f"   å»é‡ç‡: {dedup_rate:.1f}%")
    
    @property
    def stats_dict(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats

# æµ‹è¯•å‡½æ•°
async def test_llm_keyword_extraction():
    """æµ‹è¯•LLMå…³é”®è¯æå–åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•LLMå…³é”®è¯æå–åŠŸèƒ½")
    print("=" * 60)
    
    # æ¨¡æ‹ŸLLMå®¢æˆ·ç«¯ï¼ˆéœ€è¦å®é™…çš„LLMé…ç½®ï¼‰
    try:
        from src.enhanced_extractor import EnhancedNotionExtractor
        llm_client = EnhancedNotionExtractor()
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_descriptions = [
            "åä¸ºäº‘AIå›¢é˜Ÿæ‹›è˜æœºå™¨å­¦ä¹ ç®—æ³•å·¥ç¨‹å¸ˆï¼Œè´Ÿè´£æ¨èç³»ç»Ÿç®—æ³•å¼€å‘å’Œä¼˜åŒ–ï¼Œç†Ÿæ‚‰PyTorchã€TensorFlowç­‰æ·±åº¦å­¦ä¹ æ¡†æ¶",
            "åä¸ºç»ˆç«¯AIå®éªŒå®¤æ‹›è˜æœºå™¨å­¦ä¹ ç®—æ³•å·¥ç¨‹å¸ˆï¼Œè´Ÿè´£æ‰‹æœºç«¯AIåŠŸèƒ½å¼€å‘ï¼Œç†Ÿæ‚‰ARMã€NPUä¼˜åŒ–",
            "å­—èŠ‚è·³åŠ¨æŠ–éŸ³æ¨èå›¢é˜Ÿæ‹›è˜ç®—æ³•å·¥ç¨‹å¸ˆï¼Œè´Ÿè´£çŸ­è§†é¢‘æ¨èç®—æ³•ä¼˜åŒ–",
            "è…¾è®¯å¾®ä¿¡æ”¯ä»˜å›¢é˜Ÿæ‹›è˜åç«¯å·¥ç¨‹å¸ˆï¼Œè´Ÿè´£æ”¯ä»˜ç³»ç»Ÿæ¶æ„è®¾è®¡",
            "åä¸ºäº‘AIå›¢é˜Ÿæ‹›è˜æœºå™¨å­¦ä¹ ç®—æ³•å·¥ç¨‹å¸ˆï¼Œè´Ÿè´£æ¨èç³»ç»Ÿç®—æ³•å¼€å‘"  # ä¸ç¬¬ä¸€ä¸ªé‡å¤
        ]
        
        extractor = LLMKeywordExtractor(llm_client)
        
        print("ğŸ” LLMå…³é”®è¯æå–æµ‹è¯•:")
        for i, desc in enumerate(test_descriptions, 1):
            print(f"\nå²—ä½ {i}:")
            print(f"æè¿°: {desc[:50]}...")
            
            keywords = await extractor.extract_discriminative_keywords(desc)
            print(f"å…³é”®è¯: {keywords}")
        
        print(f"\nâœ… LLMå…³é”®è¯æå–æµ‹è¯•å®Œæˆ")
        
    except ImportError:
        print("âŒ è¯·ç¡®ä¿enhanced_extractor.pyå­˜åœ¨ä¸”é…ç½®æ­£ç¡®")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    asyncio.run(test_llm_keyword_extraction())
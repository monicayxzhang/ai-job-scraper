# test_notion.py - Notionå†™å…¥ä¸“ç”¨æµ‹è¯•å™¨
"""
Notionå†™å…¥åŠŸèƒ½çš„ç‹¬ç«‹æµ‹è¯•å™¨ï¼Œæä¾›çµæ´»çš„æµ‹è¯•é€‰é¡¹ï¼š
1. è¿æ¥æµ‹è¯• - éªŒè¯Tokenå’Œæ•°æ®åº“é…ç½®
2. ç»“æ„æµ‹è¯• - æ£€æŸ¥æ•°æ®åº“å­—æ®µæ˜¯å¦å®Œæ•´
3. æ•°æ®æµ‹è¯• - ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æˆ–æ–‡ä»¶æ•°æ®æµ‹è¯•å†™å…¥
4. è¯Šæ–­æ¨¡å¼ - é€æ­¥æ’æŸ¥é—®é¢˜
5. æ€§èƒ½æµ‹è¯• - æµ‹è¯•æ‰¹é‡å†™å…¥æ€§èƒ½
"""
import asyncio
import json
import os
import glob
import argparse
from datetime import datetime
from typing import List, Dict, Any, Optional

try:
    from optimized_notion_writer import OptimizedNotionJobWriter
    from dotenv import load_dotenv
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    for env_path in [".env", "../.env", "../../.env"]:
        if os.path.exists(env_path):
            load_dotenv(env_path)
            break
    
    HAS_DEPENDENCIES = True
except ImportError as e:
    print(f"âŒ ä¾èµ–å¯¼å…¥å¤±è´¥: {e}")
    HAS_DEPENDENCIES = False

class NotionTester:
    """Notionå†™å…¥åŠŸèƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.writer = None
        self.test_results = []
        
    def create_mock_job_data(self, count: int = 5, scenario: str = "normal") -> List[Dict[str, Any]]:
        """åˆ›å»ºæ¨¡æ‹Ÿå²—ä½æ•°æ®"""
        
        if scenario == "empty":
            return []
        
        if scenario == "invalid":
            return [
                {"invalid_field": "invalid_data"},
                {"å²—ä½åç§°": ""},  # ç©ºæ ‡é¢˜
                {}  # å®Œå…¨ç©ºçš„æ•°æ®
            ]
        
        # æ­£å¸¸æµ‹è¯•æ•°æ®
        base_jobs = [
            {
                "å²—ä½åç§°": "æœºå™¨å­¦ä¹ ç®—æ³•å·¥ç¨‹å¸ˆ",
                "å…¬å¸åç§°": "åä¸ºæŠ€æœ¯æœ‰é™å…¬å¸",
                "è–ªèµ„": "25-35kÂ·13è–ª",
                "å·¥ä½œåœ°ç‚¹": "åŒ—äº¬",
                "å²—ä½æè¿°": "è´Ÿè´£å¤§æ¨¡å‹ç®—æ³•ç ”å‘ï¼Œå‚ä¸LLMé¢„è®­ç»ƒå’Œå¾®è°ƒå·¥ä½œã€‚è¦æ±‚ç†Ÿæ‚‰PyTorchã€TensorFlowç­‰æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæœ‰æ¨èç³»ç»Ÿç›¸å…³ç»éªŒä¼˜å…ˆã€‚",
                "å²—ä½é“¾æ¥": "https://www.zhipin.com/job_detail/test123.html",
                "ç»¼åˆè¯„åˆ†": 85,
                "æ¨èç­‰çº§": "ğŸŒŸ å¼ºçƒˆæ¨è",
                "ç»éªŒè¦æ±‚": "1-3å¹´å·¥ä½œç»éªŒ",
                "ç»éªŒåŒ¹é…å»ºè®®": "ç»éªŒå®Œå…¨ç¬¦åˆè¦æ±‚ï¼Œå¼ºçƒˆæ¨èç”³è¯·",
                "æ¯•ä¸šæ—¶é—´è¦æ±‚_æ ‡å‡†åŒ–": "2024å±Š",
                "æ‹›è˜æˆªæ­¢æ—¥æœŸ_æ ‡å‡†åŒ–": "2024-12-31",
                "å‘å¸ƒå¹³å°": "Bossç›´è˜",
                "æ‹›å‹Ÿæ–¹å‘": "å¤§æ¨¡å‹ç®—æ³•æ–¹å‘"
            },
            {
                "å²—ä½åç§°": "æ·±åº¦å­¦ä¹ å·¥ç¨‹å¸ˆ",
                "å…¬å¸åç§°": "è…¾è®¯ç§‘æŠ€",
                "è–ªèµ„": "30-45k",
                "å·¥ä½œåœ°ç‚¹": "æ·±åœ³",
                "å²—ä½æè¿°": "å‚ä¸AIå¤§æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ä¼˜åŒ–å·¥ä½œï¼Œè´Ÿè´£ChatGPTç±»äº§å“çš„ç®—æ³•ç ”å‘ã€‚",
                "å²—ä½é“¾æ¥": "https://www.zhipin.com/job_detail/test456.html",
                "ç»¼åˆè¯„åˆ†": 78,
                "æ¨èç­‰çº§": "âœ¨ æ¨è",
                "ç»éªŒè¦æ±‚": "åº”å±Šæ¯•ä¸šç”Ÿ",
                "ç»éªŒåŒ¹é…å»ºè®®": "é¢å‘åº”å±Šç”Ÿï¼Œé€‚åˆç”³è¯·",
                "æ¯•ä¸šæ—¶é—´è¦æ±‚_æ ‡å‡†åŒ–": "2024å±Š",
                "æ‹›è˜æˆªæ­¢æ—¥æœŸ_æ ‡å‡†åŒ–": "2024-06-30",
                "å‘å¸ƒå¹³å°": "Bossç›´è˜",
                "æ‹›å‹Ÿæ–¹å‘": "å¤šæ¨¡æ€å¤§æ¨¡å‹æ–¹å‘"
            },
            {
                "å²—ä½åç§°": "ç®—æ³•å·¥ç¨‹å¸ˆ",
                "å…¬å¸åç§°": "å­—èŠ‚è·³åŠ¨",
                "è–ªèµ„": "28-40kÂ·13è–ª",
                "å·¥ä½œåœ°ç‚¹": "åŒ—äº¬",
                "å²—ä½æè¿°": "è´Ÿè´£æŠ–éŸ³æ¨èç®—æ³•ä¼˜åŒ–ï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒå’Œéƒ¨ç½²ã€‚",
                "å²—ä½é“¾æ¥": "https://www.zhipin.com/job_detail/test789.html",
                "ç»¼åˆè¯„åˆ†": 72,
                "æ¨èç­‰çº§": "âœ¨ æ¨è",
                "ç»éªŒè¦æ±‚": "1-2å¹´ç»éªŒ",
                "ç»éªŒåŒ¹é…å»ºè®®": "ç»éªŒåŸºæœ¬ç¬¦åˆï¼Œå¯ä»¥ç”³è¯·",
                "æ¯•ä¸šæ—¶é—´è¦æ±‚_æ ‡å‡†åŒ–": "2024å±Š",
                "æ‹›è˜æˆªæ­¢æ—¥æœŸ_æ ‡å‡†åŒ–": "2024-08-15",
                "å‘å¸ƒå¹³å°": "Bossç›´è˜",
                "æ‹›å‹Ÿæ–¹å‘": "æ¨èç®—æ³•æ–¹å‘"
            },
            {
                "å²—ä½åç§°": "AIå·¥ç¨‹å¸ˆ",
                "å…¬å¸åç§°": "æŸåˆ›ä¸šå…¬å¸",
                "è–ªèµ„": "20-25k",
                "å·¥ä½œåœ°ç‚¹": "æ­å·",
                "å²—ä½æè¿°": "è´Ÿè´£äººå·¥æ™ºèƒ½äº§å“çš„ç®—æ³•å¼€å‘å’Œä¼˜åŒ–å·¥ä½œã€‚",
                "å²—ä½é“¾æ¥": "https://www.zhipin.com/job_detail/test999.html",
                "ç»¼åˆè¯„åˆ†": 58,
                "æ¨èç­‰çº§": "âš ï¸ å¯è€ƒè™‘",
                "ç»éªŒè¦æ±‚": "2-3å¹´ç»éªŒ",
                "ç»éªŒåŒ¹é…å»ºè®®": "ç»éªŒè¦æ±‚ç•¥é«˜ï¼Œå¯é€šè¿‡é¡¹ç›®ç»éªŒè¡¥å……",
                "æ¯•ä¸šæ—¶é—´è¦æ±‚_æ ‡å‡†åŒ–": "2024å±Š",
                "æ‹›è˜æˆªæ­¢æ—¥æœŸ_æ ‡å‡†åŒ–": "2024-07-20",
                "å‘å¸ƒå¹³å°": "Bossç›´è˜",
                "æ‹›å‹Ÿæ–¹å‘": "AIäº§å“åŒ–æ–¹å‘"
            },
            {
                "å²—ä½åç§°": "ç ”å‘å·¥ç¨‹å¸ˆ",
                "å…¬å¸åç§°": "å°å…¬å¸",
                "è–ªèµ„": "12-18k",
                "å·¥ä½œåœ°ç‚¹": "æˆéƒ½",
                "å²—ä½æè¿°": "å‚ä¸è½¯ä»¶äº§å“çš„å¼€å‘å’Œç»´æŠ¤å·¥ä½œã€‚",
                "å²—ä½é“¾æ¥": "https://www.zhipin.com/job_detail/test000.html",
                "ç»¼åˆè¯„åˆ†": 42,
                "æ¨èç­‰çº§": "âŒ ä¸æ¨è",
                "ç»éªŒè¦æ±‚": "5å¹´ä»¥ä¸Šç»éªŒ",
                "ç»éªŒåŒ¹é…å»ºè®®": "ç»éªŒè¦æ±‚è¿‡é«˜ï¼Œè–ªèµ„åä½ï¼Œä¸å»ºè®®ç”³è¯·",
                "æ¯•ä¸šæ—¶é—´è¦æ±‚_æ ‡å‡†åŒ–": "ç»éªŒä¸é™",
                "æ‹›è˜æˆªæ­¢æ—¥æœŸ_æ ‡å‡†åŒ–": "2024-09-30",
                "å‘å¸ƒå¹³å°": "Bossç›´è˜",
                "æ‹›å‹Ÿæ–¹å‘": "è½¯ä»¶å¼€å‘æ–¹å‘"
            }
        ]
        
        if scenario == "large":
            # ç”Ÿæˆå¤§é‡æ•°æ®ç”¨äºæ€§èƒ½æµ‹è¯•
            jobs = []
            for i in range(count):
                job = base_jobs[i % len(base_jobs)].copy()
                job["å²—ä½åç§°"] = f"{job['å²—ä½åç§°']}_{i+1}"
                job["å²—ä½é“¾æ¥"] = f"https://www.zhipin.com/job_detail/test{i+1000}.html"
                jobs.append(job)
            return jobs
        
        # è¿”å›æŒ‡å®šæ•°é‡çš„æ­£å¸¸æ•°æ®
        return base_jobs[:min(count, len(base_jobs))]
    
    async def test_environment_setup(self) -> bool:
        """æµ‹è¯•ç¯å¢ƒé…ç½®"""
        print("ğŸ”§ æ£€æŸ¥ç¯å¢ƒé…ç½®...")
        
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        notion_token = os.getenv("NOTION_TOKEN")
        database_id = os.getenv("NOTION_DATABASE_ID")
        
        if not notion_token:
            print("âŒ NOTION_TOKEN æœªè®¾ç½®")
            print("ğŸ’¡ è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®: NOTION_TOKEN=your_token_here")
            return False
        
        if not database_id:
            print("âŒ NOTION_DATABASE_ID æœªè®¾ç½®")
            print("ğŸ’¡ è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®: NOTION_DATABASE_ID=your_database_id")
            return False
        
        print(f"âœ… NOTION_TOKEN: {notion_token[:10]}...{notion_token[-4:]}")
        print(f"âœ… NOTION_DATABASE_ID: {database_id[:8]}...{database_id[-4:]}")
        return True
    
    async def test_connection_only(self) -> bool:
        """ä»…æµ‹è¯•Notionè¿æ¥"""
        print("ğŸ”— æµ‹è¯•Notion APIè¿æ¥...")
        
        try:
            self.writer = OptimizedNotionJobWriter()
            print("âœ… Notionå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ Notionè¿æ¥å¤±è´¥: {e}")
            return False
    
    async def test_database_schema(self) -> bool:
        """æµ‹è¯•æ•°æ®åº“ç»“æ„"""
        print("ğŸ“‹ æ£€æŸ¥æ•°æ®åº“ç»“æ„...")
        
        if not self.writer:
            if not await self.test_connection_only():
                return False
        
        try:
            schema_ok = self.writer.check_database_schema()
            if schema_ok:
                print("âœ… æ•°æ®åº“ç»“æ„æ£€æŸ¥é€šè¿‡")
            else:
                print("âŒ æ•°æ®åº“ç»“æ„ä¸å®Œæ•´")
                print("ğŸ’¡ è¯·æŒ‰ç…§æç¤ºåœ¨Notionä¸­æ·»åŠ ç¼ºå°‘çš„å­—æ®µ")
            return schema_ok
        except Exception as e:
            print(f"âŒ æ•°æ®åº“ç»“æ„æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    async def test_write_with_mock_data(self, count: int = 3, scenario: str = "normal") -> bool:
        """ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•å†™å…¥"""
        print(f"ğŸ§ª ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•å†™å…¥ (åœºæ™¯: {scenario}, æ•°é‡: {count})...")
        
        if not self.writer:
            if not await self.test_connection_only():
                return False
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_jobs = self.create_mock_job_data(count, scenario)
        
        if not test_jobs:
            print("âš ï¸  æµ‹è¯•æ•°æ®ä¸ºç©º")
            return True
        
        # é¢„è§ˆæµ‹è¯•æ•°æ®
        print(f"\nğŸ“‹ æµ‹è¯•æ•°æ®é¢„è§ˆ:")
        for i, job in enumerate(test_jobs[:3], 1):
            title = job.get('å²—ä½åç§°', 'N/A')
            company = job.get('å…¬å¸åç§°', 'N/A')
            score = job.get('ç»¼åˆè¯„åˆ†', 'N/A')
            level = job.get('æ¨èç­‰çº§', 'N/A')
            print(f"  {i}. {title} - {company}")
            print(f"     è¯„åˆ†: {score} | ç­‰çº§: {level}")
        
        # è¯¢é—®ç¡®è®¤
        if scenario != "dry_run":
            confirm = input(f"\næ˜¯å¦å†™å…¥ {len(test_jobs)} ä¸ªæµ‹è¯•å²—ä½åˆ°Notion? (y/N): ").strip().lower()
            if confirm != 'y':
                print("âŒ ç”¨æˆ·å–æ¶ˆæµ‹è¯•")
                return False
        else:
            print("ğŸ” å¹²è¿è¡Œæ¨¡å¼ï¼Œä¸å®é™…å†™å…¥")
            return True
        
        # æ‰§è¡Œå†™å…¥æµ‹è¯•
        try:
            stats = await self.writer.batch_write_jobs_optimized(test_jobs, max_concurrent=1)
            
            print(f"\nğŸ“Š å†™å…¥æµ‹è¯•ç»“æœ:")
            print(f"   æ€»æ•°: {stats['total']}")
            print(f"   æˆåŠŸ: {stats['success']}")
            print(f"   å¤±è´¥: {stats['failed']}")
            
            if stats['total'] > 0:
                success_rate = stats['success'] / stats['total'] * 100
                print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
            
            # è®°å½•æµ‹è¯•ç»“æœ
            self.test_results.append({
                "test_type": "mock_data_write",
                "scenario": scenario,
                "stats": stats,
                "timestamp": datetime.now().isoformat()
            })
            
            return stats['success'] > 0
            
        except Exception as e:
            print(f"âŒ å†™å…¥æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    async def test_write_with_file_data(self, file_path: str, sample_size: Optional[int] = None) -> bool:
        """ä½¿ç”¨æ–‡ä»¶æ•°æ®æµ‹è¯•å†™å…¥"""
        print(f"ğŸ“ ä½¿ç”¨æ–‡ä»¶æ•°æ®æµ‹è¯•å†™å…¥: {file_path}")
        
        if not os.path.exists(file_path):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return False
        
        try:
            # åŠ è½½æ–‡ä»¶æ•°æ®
            with open(file_path, 'r', encoding='utf-8') as f:
                if file_path.endswith('.jsonl'):
                    jobs_data = [json.loads(line) for line in f if line.strip()]
                else:
                    jobs_data = json.load(f)
            
            if not isinstance(jobs_data, list):
                print("âŒ æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›åˆ—è¡¨æ ¼å¼")
                return False
            
            if not jobs_data:
                print("âš ï¸  æ–‡ä»¶æ•°æ®ä¸ºç©º")
                return True
            
            # é™åˆ¶æµ‹è¯•æ•°é‡
            if sample_size and len(jobs_data) > sample_size:
                jobs_data = jobs_data[:sample_size]
                print(f"ğŸ“ é™åˆ¶æµ‹è¯•æ•°é‡ä¸º: {sample_size}")
            
            print(f"âœ… æˆåŠŸåŠ è½½ {len(jobs_data)} ä¸ªå²—ä½")
            
            # æ•°æ®é¢„è§ˆ
            print(f"\nğŸ“‹ æ–‡ä»¶æ•°æ®é¢„è§ˆ:")
            for i, job in enumerate(jobs_data[:3], 1):
                title = job.get('å²—ä½åç§°', 'N/A')
                company = job.get('å…¬å¸åç§°', 'N/A')
                score = job.get('ç»¼åˆè¯„åˆ†', 'N/A')
                level = job.get('æ¨èç­‰çº§', 'N/A')
                print(f"  {i}. {title} - {company}")
                if score != 'N/A':
                    print(f"     è¯„åˆ†: {score} | ç­‰çº§: {level}")
            
            # æ‰§è¡Œå†™å…¥æµ‹è¯•
            return await self.test_write_with_mock_data(count=len(jobs_data), scenario="file_data")
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶æ•°æ®å¤±è´¥: {e}")
            return False
    
    async def find_and_test_latest_data(self) -> bool:
        """æŸ¥æ‰¾å¹¶æµ‹è¯•æœ€æ–°çš„æ•°æ®æ–‡ä»¶"""
        print("ğŸ” æŸ¥æ‰¾æœ€æ–°çš„å²—ä½æ•°æ®æ–‡ä»¶...")
        
        # æŸ¥æ‰¾æ•°æ®æ–‡ä»¶ - åŒ…æ‹¬å¿«ç…§ç›®å½•
        patterns = [
            # æµæ°´çº¿ç”Ÿæˆçš„æ•°æ®æ–‡ä»¶
            "data/enhanced_pipeline_extracted_*.json",
            "data/filtered_jobs_*.json",
            "enhanced_extraction_*.json",
            "extracted_jobs_*.json",
            "optimized_extraction_*.json",
            
            # å¿«ç…§ç³»ç»Ÿä¸­çš„æ•°æ®
            "debug/snapshots/*_after_advanced_filter.json",
            "debug/snapshots/*_extraction_output.json", 
            "debug/snapshots/*_final_output.json",
            "debug/snapshots/*_after_extraction.json"
        ]
        
        all_files = []
        for pattern in patterns:
            found_files = glob.glob(pattern, recursive=True)
            all_files.extend(found_files)
        
        if not all_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å²—ä½æ•°æ®æ–‡ä»¶")
            print("ğŸ’¡ å°è¯•ä»¥ä¸‹é€‰é¡¹:")
            print("   1. ä½¿ç”¨ --mock-data é€‰é¡¹æµ‹è¯•æ¨¡æ‹Ÿæ•°æ®")
            print("   2. å…ˆè¿è¡Œæµæ°´çº¿ç”Ÿæˆæ•°æ®")
            print("   3. æ£€æŸ¥ debug/snapshots/ ç›®å½•")
            return False
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæ‰¾æœ€æ–°çš„
        latest_file = max(all_files, key=os.path.getmtime)
        file_age = datetime.now() - datetime.fromtimestamp(os.path.getmtime(latest_file))
        
        print(f"ğŸ“ æ‰¾åˆ°æœ€æ–°æ•°æ®æ–‡ä»¶: {latest_file}")
        print(f"â° æ–‡ä»¶æ—¶é—´: {file_age.total_seconds()/60:.1f} åˆ†é’Ÿå‰")
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        if "snapshots" in latest_file:
            print("ğŸ“¸ æ•°æ®æ¥æº: æµæ°´çº¿å¿«ç…§")
        else:
            print("ğŸ’¾ æ•°æ®æ¥æº: ç›´æ¥è¾“å‡ºæ–‡ä»¶")
        
        return await self.test_write_with_file_data(latest_file)
    
    async def find_and_test_snapshot_data(self, snapshot_stage: str = None) -> bool:
        """ä»å¿«ç…§æ•°æ®ä¸­æŸ¥æ‰¾å¹¶æµ‹è¯•"""
        print("ğŸ“¸ æŸ¥æ‰¾å¿«ç…§æ•°æ®...")
        
        snapshot_dir = "debug/snapshots"
        if not os.path.exists(snapshot_dir):
            print(f"âŒ å¿«ç…§ç›®å½•ä¸å­˜åœ¨: {snapshot_dir}")
            print("ğŸ’¡ è¯·å…ˆè¿è¡Œæµæ°´çº¿ç”Ÿæˆå¿«ç…§æ•°æ®")
            return False
        
        # å¦‚æœæŒ‡å®šäº†é˜¶æ®µï¼ŒæŸ¥æ‰¾ç‰¹å®šé˜¶æ®µçš„å¿«ç…§
        if snapshot_stage:
            pattern = f"{snapshot_dir}/*_{snapshot_stage}.json"
            stage_files = glob.glob(pattern)
            
            if not stage_files:
                print(f"âŒ æ²¡æœ‰æ‰¾åˆ°é˜¶æ®µ '{snapshot_stage}' çš„å¿«ç…§")
                print("ğŸ’¡ å¯ç”¨çš„å¿«ç…§é˜¶æ®µ:")
                self._list_available_snapshots()
                return False
            
            latest_file = max(stage_files, key=os.path.getmtime)
        else:
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½åŒ…å«å²—ä½æ•°æ®çš„å¿«ç…§
            candidate_patterns = [
                f"{snapshot_dir}/*_after_advanced_filter.json",
                f"{snapshot_dir}/*_extraction_output.json",
                f"{snapshot_dir}/*_final_output.json",
                f"{snapshot_dir}/*_after_extraction.json"
            ]
            
            all_candidates = []
            for pattern in candidate_patterns:
                all_candidates.extend(glob.glob(pattern))
            
            if not all_candidates:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°åŒ…å«å²—ä½æ•°æ®çš„å¿«ç…§")
                print("ğŸ’¡ å¯ç”¨çš„å¿«ç…§æ–‡ä»¶:")
                self._list_available_snapshots()
                return False
            
            latest_file = max(all_candidates, key=os.path.getmtime)
        
        print(f"ğŸ“ ä½¿ç”¨å¿«ç…§æ–‡ä»¶: {os.path.basename(latest_file)}")
        
        # å°è¯•ä»å¿«ç…§ä¸­æå–æ•°æ®
        return await self._test_snapshot_data(latest_file)
    
    async def _test_snapshot_data(self, snapshot_file: str) -> bool:
        """æµ‹è¯•å¿«ç…§æ•°æ®"""
        try:
            with open(snapshot_file, 'r', encoding='utf-8') as f:
                snapshot_data = json.load(f)
            
            # å¿«ç…§æ–‡ä»¶å¯èƒ½åŒ…å«å…ƒæ•°æ®ï¼Œéœ€è¦æå–å®é™…çš„å²—ä½æ•°æ®
            jobs_data = None
            
            if isinstance(snapshot_data, list):
                # ç›´æ¥æ˜¯å²—ä½åˆ—è¡¨
                jobs_data = snapshot_data
            elif isinstance(snapshot_data, dict):
                # å¯èƒ½æ˜¯åŒ…å«å…ƒæ•°æ®çš„å¿«ç…§
                if 'successful_jobs' in snapshot_data:
                    jobs_data = snapshot_data['successful_jobs']
                elif 'data' in snapshot_data:
                    jobs_data = snapshot_data['data']
                elif 'jobs' in snapshot_data:
                    jobs_data = snapshot_data['jobs']
                else:
                    # å°è¯•æ‰¾åˆ°æœ€å¤§çš„åˆ—è¡¨å­—æ®µ
                    for key, value in snapshot_data.items():
                        if isinstance(value, list) and len(value) > 0:
                            # æ£€æŸ¥æ˜¯å¦åƒå²—ä½æ•°æ®
                            if isinstance(value[0], dict) and 'å²—ä½åç§°' in value[0]:
                                jobs_data = value
                                break
            
            if not jobs_data:
                print("âŒ æ— æ³•ä»å¿«ç…§ä¸­æå–å²—ä½æ•°æ®")
                print("ğŸ” å¿«ç…§å†…å®¹ç»“æ„:")
                if isinstance(snapshot_data, dict):
                    for key in snapshot_data.keys():
                        print(f"   - {key}: {type(snapshot_data[key])}")
                return False
            
            if not isinstance(jobs_data, list):
                print("âŒ å¿«ç…§æ•°æ®æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›åˆ—è¡¨æ ¼å¼")
                return False
            
            print(f"âœ… ä»å¿«ç…§ä¸­æå–åˆ° {len(jobs_data)} ä¸ªå²—ä½")
            
            # æ•°æ®é¢„è§ˆ
            print(f"\nğŸ“‹ å¿«ç…§æ•°æ®é¢„è§ˆ:")
            for i, job in enumerate(jobs_data[:3], 1):
                title = job.get('å²—ä½åç§°', 'N/A')
                company = job.get('å…¬å¸åç§°', 'N/A')
                score = job.get('ç»¼åˆè¯„åˆ†', 'N/A')
                level = job.get('æ¨èç­‰çº§', 'N/A')
                print(f"  {i}. {title} - {company}")
                if score != 'N/A':
                    print(f"     è¯„åˆ†: {score} | ç­‰çº§: {level}")
            
            # è¯¢é—®ç¡®è®¤
            confirm = input(f"\næ˜¯å¦å†™å…¥ {len(jobs_data)} ä¸ªå¿«ç…§å²—ä½åˆ°Notion? (y/N): ").strip().lower()
            if confirm != 'y':
                print("âŒ ç”¨æˆ·å–æ¶ˆæµ‹è¯•")
                return False
            
            # æ‰§è¡Œå†™å…¥æµ‹è¯•
            try:
                if not self.writer:
                    if not await self.test_connection_only():
                        return False
                
                stats = await self.writer.batch_write_jobs_optimized(jobs_data, max_concurrent=2)
                
                print(f"\nğŸ“Š å¿«ç…§æ•°æ®å†™å…¥ç»“æœ:")
                print(f"   æ€»æ•°: {stats['total']}")
                print(f"   æˆåŠŸ: {stats['success']}")
                print(f"   å¤±è´¥: {stats['failed']}")
                
                if stats['total'] > 0:
                    success_rate = stats['success'] / stats['total'] * 100
                    print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
                
                return stats['success'] > 0
                
            except Exception as e:
                print(f"âŒ å¿«ç…§æ•°æ®å†™å…¥å¤±è´¥: {e}")
                return False
            
        except Exception as e:
            print(f"âŒ è¯»å–å¿«ç…§æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def _list_available_snapshots(self) -> None:
        """åˆ—å‡ºå¯ç”¨çš„å¿«ç…§æ–‡ä»¶"""
        snapshot_dir = "debug/snapshots"
        if not os.path.exists(snapshot_dir):
            return
        
        snapshot_files = glob.glob(f"{snapshot_dir}/*.json")
        if not snapshot_files:
            print("   (æ²¡æœ‰æ‰¾åˆ°å¿«ç…§æ–‡ä»¶)")
            return
        
        # æŒ‰æ—¶é—´æ’åº
        snapshot_files.sort(key=os.path.getmtime, reverse=True)
        
        print("   æœ€è¿‘çš„å¿«ç…§æ–‡ä»¶:")
        for file in snapshot_files[:10]:  # åªæ˜¾ç¤ºæœ€è¿‘10ä¸ª
            basename = os.path.basename(file)
            mod_time = datetime.fromtimestamp(os.path.getmtime(file))
            age = datetime.now() - mod_time
            print(f"     â€¢ {basename} ({age.total_seconds()/60:.1f}åˆ†é’Ÿå‰)")
    
    async def list_all_data_sources(self) -> None:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®æº"""
        print("ğŸ“‹ æ‰«ææ‰€æœ‰å¯ç”¨çš„æ•°æ®æº...")
        print("=" * 60)
        
        # 1. å¿«ç…§æ•°æ®
        print("1ï¸âƒ£ å¿«ç…§æ•°æ® (debug/snapshots/):")
        snapshot_dir = "debug/snapshots"
        if os.path.exists(snapshot_dir):
            self._list_available_snapshots()
        else:
            print("   âŒ å¿«ç…§ç›®å½•ä¸å­˜åœ¨")
        
        # 2. ç›´æ¥è¾“å‡ºæ–‡ä»¶
        print("\n2ï¸âƒ£ ç›´æ¥è¾“å‡ºæ–‡ä»¶:")
        patterns = [
            "data/enhanced_pipeline_extracted_*.json",
            "data/filtered_jobs_*.json",
            "enhanced_extraction_*.json", 
            "extracted_jobs_*.json"
        ]
        
        output_files = []
        for pattern in patterns:
            output_files.extend(glob.glob(pattern, recursive=True))
        
        if output_files:
            output_files.sort(key=os.path.getmtime, reverse=True)
            for file in output_files[:5]:
                basename = os.path.basename(file)
                mod_time = datetime.fromtimestamp(os.path.getmtime(file))
                age = datetime.now() - mod_time
                print(f"   â€¢ {basename} ({age.total_seconds()/60:.1f}åˆ†é’Ÿå‰)")
        else:
            print("   âŒ æ²¡æœ‰æ‰¾åˆ°è¾“å‡ºæ–‡ä»¶")
        
        print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        print(f"   --latest-data     : è‡ªåŠ¨é€‰æ‹©æœ€æ–°æ•°æ®")
        print(f"   --snapshot-data   : ä»å¿«ç…§ä¸­é€‰æ‹©æ•°æ®")
        print(f"   --file <path>     : ä½¿ç”¨æŒ‡å®šæ–‡ä»¶")
        print(f"   --mock-data       : ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼ˆæœ€å®‰å…¨ï¼‰")
    
    async def run_diagnostic(self) -> None:
        """è¿è¡Œå®Œæ•´è¯Šæ–­"""
        print("ğŸ” è¿è¡ŒNotioné›†æˆè¯Šæ–­...")
        print("=" * 60)
        
        # æ­¥éª¤1: ç¯å¢ƒæ£€æŸ¥
        print("\n1ï¸âƒ£ ç¯å¢ƒé…ç½®æ£€æŸ¥")
        env_ok = await self.test_environment_setup()
        if not env_ok:
            print("ğŸ›‘ ç¯å¢ƒé…ç½®æœ‰é—®é¢˜ï¼Œè¯·å…ˆè§£å†³åé‡è¯•")
            return
        
        # æ­¥éª¤2: è¿æ¥æµ‹è¯•
        print("\n2ï¸âƒ£ APIè¿æ¥æµ‹è¯•")
        conn_ok = await self.test_connection_only()
        if not conn_ok:
            print("ğŸ›‘ APIè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥Tokenæ˜¯å¦æ­£ç¡®")
            return
        
        # æ­¥éª¤3: æ•°æ®åº“ç»“æ„æ£€æŸ¥
        print("\n3ï¸âƒ£ æ•°æ®åº“ç»“æ„æ£€æŸ¥")
        schema_ok = await self.test_database_schema()
        if not schema_ok:
            print("ğŸ›‘ æ•°æ®åº“ç»“æ„ä¸å®Œæ•´ï¼Œè¯·æ·»åŠ ç¼ºå°‘çš„å­—æ®µ")
            return
        
        # æ­¥éª¤4: å°è§„æ¨¡å†™å…¥æµ‹è¯•
        print("\n4ï¸âƒ£ å°è§„æ¨¡å†™å…¥æµ‹è¯•")
        write_ok = await self.test_write_with_mock_data(count=1, scenario="normal")
        if not write_ok:
            print("ğŸ›‘ å†™å…¥æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æƒé™å’Œæ•°æ®æ ¼å¼")
            return
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰è¯Šæ–­æ£€æŸ¥é€šè¿‡ï¼Notioné›†æˆå·¥ä½œæ­£å¸¸")
        print("ğŸ’¡ ç°åœ¨å¯ä»¥å®‰å…¨åœ°è¿è¡Œå®Œæ•´çš„æµæ°´çº¿")
    
    async def run_performance_test(self, job_count: int = 50) -> None:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        print(f"âš¡ è¿è¡Œæ€§èƒ½æµ‹è¯• (å†™å…¥ {job_count} ä¸ªå²—ä½)...")
        
        if not self.writer:
            if not await self.test_connection_only():
                return
        
        # åˆ›å»ºå¤§é‡æµ‹è¯•æ•°æ®
        test_jobs = self.create_mock_job_data(job_count, "large")
        
        print(f"ğŸ“Š å‡†å¤‡å†™å…¥ {len(test_jobs)} ä¸ªå²—ä½...")
        
        confirm = input(f"âš ï¸  è¿™å°†åœ¨Notionä¸­åˆ›å»º {job_count} ä¸ªæµ‹è¯•è®°å½•ï¼Œç¡®è®¤ç»§ç»­? (y/N): ").strip().lower()
        if confirm != 'y':
            print("âŒ ç”¨æˆ·å–æ¶ˆæ€§èƒ½æµ‹è¯•")
            return
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = datetime.now()
        
        try:
            # æ‰§è¡Œæ‰¹é‡å†™å…¥
            stats = await self.writer.batch_write_jobs_optimized(test_jobs, max_concurrent=3)
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            print(f"\nğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœ:")
            print(f"   å†™å…¥å²—ä½: {stats['success']}/{stats['total']}")
            print(f"   è€—æ—¶: {duration:.2f}ç§’")
            print(f"   å¹³å‡é€Ÿåº¦: {stats['success']/duration:.2f} å²—ä½/ç§’")
            print(f"   æˆåŠŸç‡: {stats['success']/stats['total']*100:.1f}%")
            
            if stats['failed'] > 0:
                print(f"   å¤±è´¥æ•°: {stats['failed']}")
                print("ğŸ’¡ å¦‚æœå¤±è´¥ç‡è¾ƒé«˜ï¼Œå»ºè®®é™ä½å¹¶å‘æ•°é‡")
            
        except Exception as e:
            print(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
    
    def print_test_summary(self) -> None:
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        if not self.test_results:
            print("ğŸ“‹ æ²¡æœ‰æµ‹è¯•è®°å½•")
            return
        
        print("\nğŸ“‹ æµ‹è¯•æ‘˜è¦:")
        print("-" * 40)
        
        for i, result in enumerate(self.test_results, 1):
            test_type = result['test_type']
            scenario = result.get('scenario', 'N/A')
            stats = result.get('stats', {})
            timestamp = result['timestamp']
            
            print(f"{i}. {test_type} ({scenario})")
            print(f"   æ—¶é—´: {timestamp}")
            if stats:
                print(f"   ç»“æœ: {stats.get('success', 0)}/{stats.get('total', 0)} æˆåŠŸ")
            print()

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='Notionå†™å…¥åŠŸèƒ½æµ‹è¯•å™¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å®Œæ•´è¯Šæ–­
  python test_notion.py --diagnose
  
  # åªæµ‹è¯•è¿æ¥
  python test_notion.py --connection-only
  
  # åªæµ‹è¯•æ•°æ®åº“ç»“æ„  
  python test_notion.py --schema-only
  
  # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•å†™å…¥
  python test_notion.py --mock-data --count 3
  
  # ä½¿ç”¨æŒ‡å®šæ–‡ä»¶æµ‹è¯•
  python test_notion.py --file data/my_jobs.json --sample-size 5
  
  # æŸ¥æ‰¾æœ€æ–°æ•°æ®å¹¶æµ‹è¯•
  python test_notion.py --latest-data
  
  # ä½¿ç”¨å¿«ç…§æ•°æ®æµ‹è¯•
  python test_notion.py --snapshot-data
  
  # ä½¿ç”¨ç‰¹å®šé˜¶æ®µçš„å¿«ç…§
  python test_notion.py --snapshot-data after_advanced_filter
  
  # åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ•°æ®æº
  python test_notion.py --list-sources
  
  # æ€§èƒ½æµ‹è¯•
  python test_notion.py --performance --count 50
  
  # å¹²è¿è¡Œï¼ˆä¸å®é™…å†™å…¥ï¼‰
  python test_notion.py --mock-data --dry-run
        """
    )
    
    # æµ‹è¯•æ¨¡å¼é€‰æ‹©
    group = parser.add_mutually_exclusive_group()
    group.add_argument('--diagnose', action='store_true', help='è¿è¡Œå®Œæ•´è¯Šæ–­')
    group.add_argument('--connection-only', action='store_true', help='ä»…æµ‹è¯•è¿æ¥')
    group.add_argument('--schema-only', action='store_true', help='ä»…æµ‹è¯•æ•°æ®åº“ç»“æ„')
    group.add_argument('--mock-data', action='store_true', help='ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•')
    group.add_argument('--file', type=str, help='ä½¿ç”¨æŒ‡å®šæ–‡ä»¶æµ‹è¯•')
    group.add_argument('--latest-data', action='store_true', help='ä½¿ç”¨æœ€æ–°æ•°æ®æ–‡ä»¶æµ‹è¯•')
    group.add_argument('--snapshot-data', type=str, nargs='?', const='auto', 
                       help='ä½¿ç”¨å¿«ç…§æ•°æ®æµ‹è¯•ï¼ˆå¯æŒ‡å®šé˜¶æ®µï¼Œå¦‚ after_advanced_filterï¼‰')
    group.add_argument('--list-sources', action='store_true', help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ•°æ®æº')
    group.add_argument('--performance', action='store_true', help='è¿è¡Œæ€§èƒ½æµ‹è¯•')
    
    # å‚æ•°é€‰é¡¹
    parser.add_argument('--count', type=int, default=5, help='æµ‹è¯•å²—ä½æ•°é‡')
    parser.add_argument('--sample-size', type=int, help='æ–‡ä»¶æ•°æ®é‡‡æ ·æ•°é‡')
    parser.add_argument('--scenario', choices=['normal', 'invalid', 'empty'], 
                       default='normal', help='æµ‹è¯•åœºæ™¯')
    parser.add_argument('--dry-run', action='store_true', help='å¹²è¿è¡Œï¼Œä¸å®é™…å†™å…¥')
    
    return parser.parse_args()

async def main():
    """ä¸»å‡½æ•°"""
    if not HAS_DEPENDENCIES:
        print("âŒ è¯·ç¡®ä¿å·²å®‰è£…å¿…è¦çš„ä¾èµ–")
        return
    
    args = parse_args()
    tester = NotionTester()
    
    print("ğŸ§ª Notionå†™å…¥åŠŸèƒ½æµ‹è¯•å™¨")
    print("=" * 60)
    
    try:
        if args.diagnose:
            await tester.run_diagnostic()
        
        elif args.connection_only:
            success = await tester.test_connection_only()
            print(f"ğŸ”— è¿æ¥æµ‹è¯•: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}")
        
        elif args.schema_only:
            success = await tester.test_database_schema()
            print(f"ğŸ“‹ ç»“æ„æµ‹è¯•: {'âœ… é€šè¿‡' if success else 'âŒ å¤±è´¥'}")
        
        elif args.mock_data:
            scenario = "dry_run" if args.dry_run else args.scenario
            success = await tester.test_write_with_mock_data(args.count, scenario)
            print(f"ğŸ§ª æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}")
        
        elif args.file:
            success = await tester.test_write_with_file_data(args.file, args.sample_size)
            print(f"ğŸ“ æ–‡ä»¶æ•°æ®æµ‹è¯•: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}")
        
        elif args.latest_data:
            success = await tester.find_and_test_latest_data()
            print(f"ğŸ” æœ€æ–°æ•°æ®æµ‹è¯•: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}")
        
        elif args.snapshot_data:
            stage = None if args.snapshot_data == 'auto' else args.snapshot_data
            success = await tester.find_and_test_snapshot_data(stage)
            print(f"ğŸ“¸ å¿«ç…§æ•°æ®æµ‹è¯•: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}")
        
        elif args.list_sources:
            await tester.list_all_data_sources()
        
        elif args.performance:
            await tester.run_performance_test(args.count)
        
        else:
            # é»˜è®¤ï¼šè¿è¡ŒåŸºç¡€æ£€æŸ¥
            print("è¿è¡ŒåŸºç¡€æ£€æŸ¥...")
            env_ok = await tester.test_environment_setup()
            if env_ok:
                conn_ok = await tester.test_connection_only()
                if conn_ok:
                    await tester.test_database_schema()
            
            print("\nğŸ’¡ ä½¿ç”¨ --help æŸ¥çœ‹æ›´å¤šæµ‹è¯•é€‰é¡¹")
        
        # æ˜¾ç¤ºæµ‹è¯•æ‘˜è¦
        tester.print_test_summary()
        
    except KeyboardInterrupt:
        print("\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
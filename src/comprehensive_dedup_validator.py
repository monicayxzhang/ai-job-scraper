#!/usr/bin/env python3
# comprehensive_dedup_validator.py - å»é‡æ¨¡å—å…¨é¢éªŒè¯å™¨

"""
å»é‡æ¨¡å—éªŒè¯çš„å…¨é¢æ€§æ£€æŸ¥è„šæœ¬
è¦†ç›–æ‰€æœ‰å¯èƒ½çš„éªŒè¯ç»´åº¦ï¼Œç¡®ä¿å»é‡é€»è¾‘çš„æ­£ç¡®æ€§
"""

import json
import glob
import os
import re
import hashlib
from typing import List, Dict, Any, Set, Optional, Tuple
from datetime import datetime
from collections import Counter, defaultdict
from dataclasses import dataclass

@dataclass
class ValidationResult:
    """éªŒè¯ç»“æœæ•°æ®ç±»"""
    passed: bool
    details: Dict[str, Any]
    issues: List[str]
    recommendations: List[str]

class ComprehensiveDeduplicationValidator:
    """å»é‡æ¨¡å—å…¨é¢éªŒè¯å™¨"""
    
    def __init__(self, session_id=None):
        """åˆå§‹åŒ–éªŒè¯å™¨"""
        self.snapshots_dir = "debug/snapshots"
        
        # éªŒè¯ç»“æœå­˜å‚¨
        self.validation_results = {}
        self.global_issues = []
        self.global_recommendations = []

        self.session_id = session_id or self.find_latest_session()
        
        print(f"ğŸš€ å»é‡æ¨¡å—å…¨é¢éªŒè¯å™¨å¯åŠ¨")
        print(f"ğŸ“ ä¼šè¯ID: {self.session_id}")
        print(f"ğŸ“‚ å¿«ç…§ç›®å½•: {self.snapshots_dir}")
        print("=" * 80)
    
    def find_latest_session(self):
        """æŸ¥æ‰¾æœ€æ–°çš„ä¼šè¯ID"""
        pattern = f"{self.snapshots_dir}/*_summary.json"
        summary_files = glob.glob(pattern)
        if not summary_files:
            raise FileNotFoundError("âŒ æ²¡æœ‰æ‰¾åˆ°å¿«ç…§æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæµæ°´çº¿")
        
        latest_file = max(summary_files, key=os.path.getmtime)
        basename = os.path.basename(latest_file)
        session_id = basename.split('_summary.json')[0]
        return session_id
    
    def load_snapshot(self, stage: str) -> Optional[Any]:
        """åŠ è½½æŒ‡å®šé˜¶æ®µçš„å¿«ç…§"""
        pattern = f"{self.snapshots_dir}/{self.session_id}_{stage}.json"
        files = glob.glob(pattern)
        
        if not files:
            return None
            
        try:
            with open(files[0], 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸  åŠ è½½å¿«ç…§å¤±è´¥ {stage}: {e}")
            return None
    
    # ========== 1. æ•°æ®å®Œæ•´æ€§éªŒè¯ ==========
    
    def validate_data_integrity(self) -> ValidationResult:
        """éªŒè¯æ•°æ®å®Œæ•´æ€§å’Œæµè½¬æ­£ç¡®æ€§"""
        print("ğŸ” 1. æ•°æ®å®Œæ•´æ€§éªŒè¯")
        print("-" * 50)
        
        issues = []
        details = {}
        
        # åŠ è½½æ‰€æœ‰å¿«ç…§
        snapshots = {
            "raw_crawl": self.load_snapshot("raw_crawl"),
            "local_dedup_input": self.load_snapshot("local_dedup_input"),
            "local_dedup_output": self.load_snapshot("local_dedup_output"),
            "notion_dedup_output": self.load_snapshot("notion_dedup_output"),
            "notion_cache": self.load_snapshot("notion_cache")
        }
        
        # æ£€æŸ¥å…³é”®å¿«ç…§æ˜¯å¦å­˜åœ¨
        critical_snapshots = ["local_dedup_input", "local_dedup_output"]
        missing_snapshots = [name for name in critical_snapshots if snapshots[name] is None]
        
        if missing_snapshots:
            issues.append(f"ç¼ºå°‘å…³é”®å¿«ç…§: {', '.join(missing_snapshots)}")
        
        # ç»Ÿè®¡æ•°æ®é‡
        for name, data in snapshots.items():
            if data is not None:
                if isinstance(data, list):
                    count = len(data)
                elif isinstance(data, dict) and "existing_urls" in data:
                    count = len(data.get("existing_urls", []))
                else:
                    count = "æœªçŸ¥æ ¼å¼"
                details[f"{name}_count"] = count
                print(f"   {name}: {count} æ¡è®°å½•")
        
        # éªŒè¯æ•°æ®æµè½¬é€»è¾‘
        if snapshots["local_dedup_input"] and snapshots["local_dedup_output"]:
            input_count = len(snapshots["local_dedup_input"])
            output_count = len(snapshots["local_dedup_output"])
            
            if output_count > input_count:
                issues.append(f"æœ¬åœ°å»é‡è¾“å‡º({output_count})å¤§äºè¾“å…¥({input_count})ï¼Œé€»è¾‘å¼‚å¸¸")
            
            details["local_dedup_ratio"] = output_count / input_count if input_count > 0 else 0
        
        if snapshots["local_dedup_output"] and snapshots["notion_dedup_output"]:
            local_count = len(snapshots["local_dedup_output"])
            notion_count = len(snapshots["notion_dedup_output"])
            
            if notion_count > local_count:
                issues.append(f"Notionå»é‡è¾“å‡º({notion_count})å¤§äºæœ¬åœ°è¾“å‡º({local_count})ï¼Œé€»è¾‘å¼‚å¸¸")
            
            details["notion_dedup_ratio"] = notion_count / local_count if local_count > 0 else 0
        
        recommendations = []
        if not issues:
            recommendations.append("æ•°æ®å®Œæ•´æ€§è‰¯å¥½")
        else:
            recommendations.append("æ£€æŸ¥æµæ°´çº¿æ‰§è¡Œè¿‡ç¨‹æ˜¯å¦æœ‰å¼‚å¸¸")
        
        return ValidationResult(
            passed=len(issues) == 0,
            details=details,
            issues=issues,
            recommendations=recommendations
        )
    
    # ========== 2. URLå»é‡æ­£ç¡®æ€§éªŒè¯ ==========
    
    def validate_url_deduplication(self) -> ValidationResult:
        """éªŒè¯URLå»é‡çš„æ­£ç¡®æ€§"""
        print("\nğŸ” 2. URLå»é‡æ­£ç¡®æ€§éªŒè¯")
        print("-" * 50)
        
        issues = []
        details = {}
        recommendations = []
        
        input_data = self.load_snapshot("local_dedup_input")
        output_data = self.load_snapshot("local_dedup_output")
        
        if not input_data or not output_data:
            issues.append("ç¼ºå°‘å¿…è¦çš„å»é‡æ•°æ®")
            return ValidationResult(False, details, issues, recommendations)
        
        # æå–å’Œåˆ†æURL
        input_urls = [job.get('å²—ä½é“¾æ¥', '') for job in input_data if job.get('å²—ä½é“¾æ¥')]
        output_urls = [job.get('å²—ä½é“¾æ¥', '') for job in output_data if job.get('å²—ä½é“¾æ¥')]
        
        # æ¸…ç†URLï¼ˆä¸å»é‡é€»è¾‘ä¿æŒä¸€è‡´ï¼‰
        def clean_url(url):
            if not url:
                return ""
            base_url = url.split('?')[0].split('#')[0]
            match = re.search(r'/job_detail/([^/.]+)', base_url)
            return match.group(1) if match else base_url.split('/')[-1] if '/' in base_url else base_url
        
        clean_input_urls = [clean_url(url) for url in input_urls]
        clean_output_urls = [clean_url(url) for url in output_urls]
        
        # ç»Ÿè®¡ä¿¡æ¯
        details.update({
            "input_total_urls": len(input_urls),
            "input_unique_urls": len(set(clean_input_urls)),
            "output_total_urls": len(output_urls),
            "output_unique_urls": len(set(clean_output_urls)),
            "url_duplicates_in_input": len(input_urls) - len(set(clean_input_urls)),
            "url_duplicates_in_output": len(output_urls) - len(set(clean_output_urls))
        })
        
        print(f"   è¾“å…¥URLæ€»æ•°: {details['input_total_urls']}")
        print(f"   è¾“å…¥å”¯ä¸€URL: {details['input_unique_urls']}")
        print(f"   è¾“å‡ºURLæ€»æ•°: {details['output_total_urls']}")
        print(f"   è¾“å‡ºå”¯ä¸€URL: {details['output_unique_urls']}")
        
        # éªŒè¯è¾“å‡ºä¸­æ˜¯å¦è¿˜æœ‰é‡å¤URL
        if details["url_duplicates_in_output"] > 0:
            issues.append(f"è¾“å‡ºä¸­ä»æœ‰ {details['url_duplicates_in_output']} ä¸ªé‡å¤URL")
            
            # æ‰¾å‡ºé‡å¤çš„URL
            url_counts = Counter(clean_output_urls)
            duplicates = {url: count for url, count in url_counts.items() if count > 1}
            details["duplicate_urls"] = list(duplicates.keys())[:5]  # åªè®°å½•å‰5ä¸ª
            
            print(f"   âŒ é‡å¤URLç¤ºä¾‹: {details['duplicate_urls']}")
            recommendations.append("æ£€æŸ¥URLæ¸…ç†å’Œå»é‡é€»è¾‘")
        else:
            print(f"   âœ… è¾“å‡ºä¸­æ— é‡å¤URL")
            recommendations.append("URLå»é‡å·¥ä½œæ­£å¸¸")
        
        # éªŒè¯URLæ¸…ç†æ•ˆæœ
        invalid_urls = [url for url in output_urls if not url or url.startswith('http') == False]
        if invalid_urls:
            issues.append(f"å‘ç° {len(invalid_urls)} ä¸ªæ— æ•ˆURL")
            details["invalid_urls"] = invalid_urls[:3]
        
        return ValidationResult(
            passed=len(issues) == 0,
            details=details,
            issues=issues,
            recommendations=recommendations
        )
    
    # ========== 3. å†…å®¹å»é‡æ­£ç¡®æ€§éªŒè¯ ==========
    
    def validate_content_deduplication(self) -> ValidationResult:
        """éªŒè¯å†…å®¹å»é‡çš„æ­£ç¡®æ€§"""
        print("\nğŸ” 3. å†…å®¹å»é‡æ­£ç¡®æ€§éªŒè¯")
        print("-" * 50)
        
        issues = []
        details = {}
        recommendations = []
        
        input_data = self.load_snapshot("local_dedup_input")
        output_data = self.load_snapshot("local_dedup_output")
        
        if not input_data or not output_data:
            issues.append("ç¼ºå°‘å¿…è¦çš„å»é‡æ•°æ®")
            return ValidationResult(False, details, issues, recommendations)
        
        # åˆ›å»ºå†…å®¹æŒ‡çº¹
        def create_content_fingerprint(job):
            company = job.get('å…¬å¸åç§°', '').strip().lower()
            title = job.get('å²—ä½åç§°', '').strip().lower()
            location = job.get('å·¥ä½œåœ°ç‚¹', '').strip().lower()
            
            # æ¸…ç†å…¬å¸åç§°
            company = re.sub(r'æœ‰é™å…¬å¸$|ç§‘æŠ€æœ‰é™å…¬å¸$|æŠ€æœ¯æœ‰é™å…¬å¸$', '', company)
            # æ¸…ç†åœ°ç‚¹
            location = re.sub(r'[Â·\s]*[^ï¼Œã€‚\s]*åŒº', '', location)
            
            return f"{company}_{title}_{location}"
        
        # åˆ†æè¾“å…¥æ•°æ®çš„å†…å®¹é‡å¤
        input_fingerprints = [create_content_fingerprint(job) for job in input_data]
        output_fingerprints = [create_content_fingerprint(job) for job in output_data]
        
        input_fp_counts = Counter(input_fingerprints)
        output_fp_counts = Counter(output_fingerprints)
        
        # ç»Ÿè®¡ä¿¡æ¯
        details.update({
            "input_total_jobs": len(input_data),
            "input_unique_content": len(set(input_fingerprints)),
            "output_total_jobs": len(output_data),
            "output_unique_content": len(set(output_fingerprints)),
            "content_duplicates_in_input": len(input_data) - len(set(input_fingerprints)),
            "content_duplicates_in_output": len(output_data) - len(set(output_fingerprints))
        })
        
        print(f"   è¾“å…¥å²—ä½æ€»æ•°: {details['input_total_jobs']}")
        print(f"   è¾“å…¥å”¯ä¸€å†…å®¹: {details['input_unique_content']}")
        print(f"   è¾“å‡ºå²—ä½æ€»æ•°: {details['output_total_jobs']}")
        print(f"   è¾“å‡ºå”¯ä¸€å†…å®¹: {details['output_unique_content']}")
        
        # éªŒè¯è¾“å‡ºä¸­æ˜¯å¦è¿˜æœ‰å†…å®¹é‡å¤
        if details["content_duplicates_in_output"] > 0:
            issues.append(f"è¾“å‡ºä¸­ä»æœ‰ {details['content_duplicates_in_output']} ä¸ªå†…å®¹é‡å¤")
            
            # æ‰¾å‡ºé‡å¤çš„å†…å®¹
            duplicate_fps = {fp: count for fp, count in output_fp_counts.items() if count > 1}
            details["duplicate_content_count"] = len(duplicate_fps)
            
            print(f"   âŒ å‘ç° {len(duplicate_fps)} ç»„é‡å¤å†…å®¹")
            recommendations.append("æ£€æŸ¥å†…å®¹æŒ‡çº¹ç®—æ³•å’Œå»é‡é€»è¾‘")
            
            # æ˜¾ç¤ºé‡å¤å†…å®¹è¯¦æƒ…
            if duplicate_fps:
                print(f"   é‡å¤å†…å®¹ç¤ºä¾‹:")
                for fp, count in list(duplicate_fps.items())[:3]:
                    print(f"     - {fp} ({count}ä¸ª)")
                    # æ‰¾å‡ºå…·ä½“çš„å²—ä½
                    matching_jobs = [job for job, job_fp in zip(output_data, output_fingerprints) if job_fp == fp]
                    for job in matching_jobs[:2]:
                        print(f"       * {job.get('å²—ä½åç§°', 'N/A')} - {job.get('å…¬å¸åç§°', 'N/A')}")
        else:
            print(f"   âœ… è¾“å‡ºä¸­æ— å†…å®¹é‡å¤")
            recommendations.append("å†…å®¹å»é‡å·¥ä½œæ­£å¸¸")
        
        # åˆ†æå»é‡æ•ˆæœ
        if details["content_duplicates_in_input"] > 0:
            removal_efficiency = (details["content_duplicates_in_input"] - details["content_duplicates_in_output"]) / details["content_duplicates_in_input"]
            details["content_dedup_efficiency"] = removal_efficiency
            print(f"   ğŸ“Š å†…å®¹å»é‡æ•ˆç‡: {removal_efficiency:.1%}")
            
            if removal_efficiency < 0.8:
                issues.append("å†…å®¹å»é‡æ•ˆç‡åä½")
                recommendations.append("ä¼˜åŒ–å†…å®¹ç›¸ä¼¼åº¦ç®—æ³•")
        
        return ValidationResult(
            passed=len(issues) == 0,
            details=details,
            issues=issues,
            recommendations=recommendations
        )
    
    # ========== 4. Notionå¢é‡å»é‡éªŒè¯ ==========
    
    def validate_notion_incremental_dedup(self) -> ValidationResult:
        """éªŒè¯Notionå¢é‡å»é‡çš„æ­£ç¡®æ€§"""
        print("\nğŸ” 4. Notionå¢é‡å»é‡éªŒè¯")
        print("-" * 50)
        
        issues = []
        details = {}
        recommendations = []
        
        local_output = self.load_snapshot("local_dedup_output")
        notion_output = self.load_snapshot("notion_dedup_output")
        notion_cache = self.load_snapshot("notion_cache")
        
        if not local_output:
            issues.append("ç¼ºå°‘æœ¬åœ°å»é‡è¾“å‡ºæ•°æ®")
        if not notion_output:
            issues.append("ç¼ºå°‘Notionå»é‡è¾“å‡ºæ•°æ®")
        if not notion_cache:
            issues.append("ç¼ºå°‘Notionç¼“å­˜æ•°æ®")
        
        if issues:
            return ValidationResult(False, details, issues, recommendations)
        
        # åˆ†æè¢«Notionå»é‡çš„å²—ä½
        notion_output_urls = {job.get('å²—ä½é“¾æ¥', '') for job in notion_output}
        removed_jobs = [job for job in local_output if job.get('å²—ä½é“¾æ¥', '') not in notion_output_urls]
        
        details.update({
            "local_output_count": len(local_output),
            "notion_output_count": len(notion_output),
            "notion_removed_count": len(removed_jobs),
            "cached_urls_count": len(notion_cache.get("existing_urls", [])),
            "cached_fingerprints_count": len(notion_cache.get("existing_fingerprints", []))
        })
        
        print(f"   æœ¬åœ°å»é‡è¾“å‡º: {details['local_output_count']} ä¸ªå²—ä½")
        print(f"   Notionå»é‡è¾“å‡º: {details['notion_output_count']} ä¸ªå²—ä½")
        print(f"   è¢«Notionå»é‡: {details['notion_removed_count']} ä¸ªå²—ä½")
        print(f"   Notionç¼“å­˜URL: {details['cached_urls_count']} ä¸ª")
        print(f"   Notionç¼“å­˜æŒ‡çº¹: {details['cached_fingerprints_count']} ä¸ª")
        
        if details["notion_removed_count"] == 0:
            print("   âœ… æ²¡æœ‰å²—ä½è¢«Notionå»é‡")
            recommendations.append("å¢é‡å»é‡å·¥ä½œæ­£å¸¸")
            return ValidationResult(True, details, issues, recommendations)
        
        # éªŒè¯è¢«å»é‡å²—ä½æ˜¯å¦çœŸçš„åœ¨ç¼“å­˜ä¸­å­˜åœ¨
        cached_urls = set(notion_cache.get("existing_urls", []))
        cached_fingerprints = set(notion_cache.get("existing_fingerprints", []))
        
        verification_stats = {
            "url_verified": 0,
            "fingerprint_verified": 0,
            "not_found": 0,
            "verification_errors": 0
        }
        
        print(f"\n   ğŸ” éªŒè¯è¢«å»é‡å²—ä½çš„çœŸå®æ€§:")
        
        for i, job in enumerate(removed_jobs, 1):
            try:
                # URLéªŒè¯
                clean_url = self._clean_url_for_cache(job.get('å²—ä½é“¾æ¥', ''))
                url_found = clean_url in cached_urls
                
                # æŒ‡çº¹éªŒè¯
                job_fingerprint = self._create_cache_fingerprint(job)
                fingerprint_found = job_fingerprint in cached_fingerprints
                
                if url_found:
                    verification_stats["url_verified"] += 1
                    print(f"     {i}. âœ… URLéªŒè¯é€šè¿‡: {job.get('å²—ä½åç§°', 'N/A')}")
                elif fingerprint_found:
                    verification_stats["fingerprint_verified"] += 1
                    print(f"     {i}. âœ… æŒ‡çº¹éªŒè¯é€šè¿‡: {job.get('å²—ä½åç§°', 'N/A')}")
                else:
                    verification_stats["not_found"] += 1
                    print(f"     {i}. âŒ éªŒè¯å¤±è´¥: {job.get('å²—ä½åç§°', 'N/A')} - {job.get('å…¬å¸åç§°', 'N/A')}")
                    issues.append(f"å²—ä½æœªåœ¨ç¼“å­˜ä¸­æ‰¾åˆ°: {job.get('å²—ä½åç§°', 'N/A')}")
                    
            except Exception as e:
                verification_stats["verification_errors"] += 1
                print(f"     {i}. âš ï¸  éªŒè¯å‡ºé”™: {e}")
        
        details.update(verification_stats)
        
        # è®¡ç®—éªŒè¯æˆåŠŸç‡
        total_verified = verification_stats["url_verified"] + verification_stats["fingerprint_verified"]
        if details["notion_removed_count"] > 0:
            verification_rate = total_verified / details["notion_removed_count"]
            details["verification_success_rate"] = verification_rate
            print(f"   ğŸ“Š éªŒè¯æˆåŠŸç‡: {verification_rate:.1%}")
            
            if verification_rate < 0.9:
                issues.append(f"éªŒè¯æˆåŠŸç‡åä½: {verification_rate:.1%}")
                recommendations.append("æ£€æŸ¥ç¼“å­˜æ•°æ®å®Œæ•´æ€§å’Œå»é‡é€»è¾‘ä¸€è‡´æ€§")
            else:
                recommendations.append("Notionå¢é‡å»é‡å·¥ä½œæ­£å¸¸")
        
        return ValidationResult(
            passed=len(issues) == 0,
            details=details,
            issues=issues,
            recommendations=recommendations
        )
    
    # ========== 5. å»é‡æ€§èƒ½å’Œæ•ˆç‡éªŒè¯ ==========
    
    def validate_dedup_performance(self) -> ValidationResult:
        """éªŒè¯å»é‡æ€§èƒ½å’Œæ•ˆç‡"""
        print("\nğŸ” 5. å»é‡æ€§èƒ½å’Œæ•ˆç‡éªŒè¯")
        print("-" * 50)
        
        issues = []
        details = {}
        recommendations = []
        
        # åŠ è½½æ•°æ®
        input_data = self.load_snapshot("local_dedup_input")
        output_data = self.load_snapshot("local_dedup_output")
        notion_output = self.load_snapshot("notion_dedup_output")
        
        if not all([input_data, output_data]):
            issues.append("ç¼ºå°‘å¿…è¦çš„æ€§èƒ½åˆ†ææ•°æ®")
            return ValidationResult(False, details, issues, recommendations)
        
        # è®¡ç®—å»é‡ç‡
        input_count = len(input_data)
        local_output_count = len(output_data)
        final_output_count = len(notion_output) if notion_output else local_output_count
        
        local_dedup_rate = (input_count - local_output_count) / input_count if input_count > 0 else 0
        overall_dedup_rate = (input_count - final_output_count) / input_count if input_count > 0 else 0
        
        details.update({
            "input_count": input_count,
            "local_output_count": local_output_count,
            "final_output_count": final_output_count,
            "local_dedup_rate": local_dedup_rate,
            "overall_dedup_rate": overall_dedup_rate,
            "local_removed": input_count - local_output_count,
            "notion_removed": local_output_count - final_output_count if notion_output else 0
        })
        
        print(f"   è¾“å…¥å²—ä½: {input_count} ä¸ª")
        print(f"   æœ¬åœ°å»é‡å: {local_output_count} ä¸ª (å»é‡ç‡: {local_dedup_rate:.1%})")
        print(f"   æœ€ç»ˆè¾“å‡º: {final_output_count} ä¸ª (æ€»å»é‡ç‡: {overall_dedup_rate:.1%})")
        
        # è¯„ä¼°å»é‡æ•ˆç‡
        if overall_dedup_rate == 0:
            print("   âš ï¸  æ²¡æœ‰å‘ç°ä»»ä½•é‡å¤å²—ä½")
            recommendations.append("æ£€æŸ¥æ˜¯å¦è¾“å…¥æ•°æ®æœ¬èº«å°±æ²¡æœ‰é‡å¤")
        elif overall_dedup_rate < 0.1:
            print("   ğŸ“Š å»é‡ç‡è¾ƒä½ï¼Œå¯èƒ½è¾“å…¥æ•°æ®è´¨é‡è¾ƒé«˜")
            recommendations.append("å»é‡æ•ˆæœæ­£å¸¸")
        elif overall_dedup_rate > 0.5:
            print("   ğŸ“Š å»é‡ç‡è¾ƒé«˜ï¼Œè¾“å…¥æ•°æ®é‡å¤åº¦è¾ƒå¤§")
            recommendations.append("è€ƒè™‘ä¼˜åŒ–æ•°æ®æºè´¨é‡")
            if overall_dedup_rate > 0.8:
                issues.append("å»é‡ç‡è¿‡é«˜ï¼Œå¯èƒ½å­˜åœ¨è¿‡åº¦å»é‡")
        else:
            print("   âœ… å»é‡ç‡é€‚ä¸­")
            recommendations.append("å»é‡æ•ˆæœè‰¯å¥½")
        
        # æ£€æŸ¥æ•°æ®ä¿ç•™è´¨é‡
        if final_output_count == 0:
            issues.append("æ‰€æœ‰å²—ä½éƒ½è¢«å»é‡ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜")
        elif final_output_count < input_count * 0.1:
            issues.append("ä¿ç•™çš„å²—ä½è¿‡å°‘ï¼Œå¯èƒ½è¿‡åº¦å»é‡")
            recommendations.append("æ£€æŸ¥å»é‡é˜ˆå€¼è®¾ç½®")
        
        return ValidationResult(
            passed=len(issues) == 0,
            details=details,
            issues=issues,
            recommendations=recommendations
        )
    
    # ========== 6. ä¸šåŠ¡é€»è¾‘éªŒè¯ ==========
    
    def validate_business_logic(self) -> ValidationResult:
        """éªŒè¯ä¸šåŠ¡é€»è¾‘çš„åˆç†æ€§"""
        print("\nğŸ” 6. ä¸šåŠ¡é€»è¾‘éªŒè¯")
        print("-" * 50)
        
        issues = []
        details = {}
        recommendations = []
        
        # åŠ è½½æ•°æ®
        input_data = self.load_snapshot("local_dedup_input")
        output_data = self.load_snapshot("local_dedup_output")
        extraction_output = self.load_snapshot("extraction_output")
        
        if not all([input_data, output_data]):
            issues.append("ç¼ºå°‘å¿…è¦çš„ä¸šåŠ¡éªŒè¯æ•°æ®")
            return ValidationResult(False, details, issues, recommendations)
        
        # éªŒè¯é‡è¦å²—ä½æ˜¯å¦è¢«è¯¯åˆ 
        print("   ğŸ¯ æ£€æŸ¥é‡è¦å²—ä½ä¿ç•™æƒ…å†µ:")
        
        # å®šä¹‰é‡è¦å…¬å¸å…³é”®è¯
        important_companies = ['åä¸º', 'è…¾è®¯', 'é˜¿é‡Œ', 'å­—èŠ‚', 'ç™¾åº¦', 'äº¬ä¸œ', 'ç¾å›¢', 'æ»´æ»´', 'å°ç±³']
        high_salary_keywords = ['30k', '40k', '50k', '60k', '25kä»¥ä¸Š', '30ä¸‡', '40ä¸‡', '50ä¸‡']
        
        # ç»Ÿè®¡é‡è¦å²—ä½
        def is_important_job(job):
            company = job.get('å…¬å¸åç§°', '').lower()
            salary = job.get('è–ªèµ„', '').lower()
            title = job.get('å²—ä½åç§°', '').lower()
            
            # é‡è¦å…¬å¸
            if any(keyword in company for keyword in important_companies):
                return True, "é‡è¦å…¬å¸"
            
            # é«˜è–ªå²—ä½
            if any(keyword in salary for keyword in high_salary_keywords):
                return True, "é«˜è–ªå²—ä½"
            
            # é«˜çº§å²—ä½
            if any(keyword in title for keyword in ['ä¸“å®¶', 'æ€»ç›‘', 'æ¶æ„å¸ˆ', 'tech lead', 'senior']):
                return True, "é«˜çº§å²—ä½"
            
            return False, ""
        
        important_input_jobs = []
        important_output_jobs = []
        
        for job in input_data:
            is_important, reason = is_important_job(job)
            if is_important:
                important_input_jobs.append((job, reason))
        
        for job in output_data:
            is_important, reason = is_important_job(job)
            if is_important:
                important_output_jobs.append((job, reason))
        
        details.update({
            "important_input_count": len(important_input_jobs),
            "important_output_count": len(important_output_jobs),
            "important_removal_rate": (len(important_input_jobs) - len(important_output_jobs)) / len(important_input_jobs) if important_input_jobs else 0
        })
        
        print(f"     é‡è¦å²—ä½è¾“å…¥: {len(important_input_jobs)} ä¸ª")
        print(f"     é‡è¦å²—ä½ä¿ç•™: {len(important_output_jobs)} ä¸ª")
        
        if len(important_input_jobs) > len(important_output_jobs):
            removed_important = len(important_input_jobs) - len(important_output_jobs)
            print(f"     âš ï¸  é‡è¦å²—ä½è¢«å»é‡: {removed_important} ä¸ª")
            
            if removed_important > len(important_input_jobs) * 0.3:
                issues.append(f"è¿‡å¤šé‡è¦å²—ä½è¢«å»é‡: {removed_important}ä¸ª")
                recommendations.append("æ£€æŸ¥å»é‡é€»è¾‘æ˜¯å¦å¯¹é‡è¦å²—ä½è¿‡äºæ¿€è¿›")
        else:
            print(f"     âœ… é‡è¦å²—ä½ä¿ç•™è‰¯å¥½")
        
        # éªŒè¯æ•°æ®è´¨é‡
        print("   ğŸ“Š æ•°æ®è´¨é‡æ£€æŸ¥:")
        
        def check_data_quality(jobs, label):
            empty_fields = defaultdict(int)
            invalid_data = []
            
            for job in jobs:
                # æ£€æŸ¥å¿…è¦å­—æ®µ
                required_fields = ['å²—ä½åç§°', 'å…¬å¸åç§°', 'å²—ä½é“¾æ¥']
                for field in required_fields:
                    if not job.get(field) or job.get(field).strip() == '':
                        empty_fields[field] += 1
                
                # æ£€æŸ¥æ•°æ®æ ¼å¼
                if job.get('å²—ä½é“¾æ¥') and not job['å²—ä½é“¾æ¥'].startswith('http'):
                    invalid_data.append(f"æ— æ•ˆURL: {job['å²—ä½é“¾æ¥']}")
            
            print(f"     {label}:")
            for field, count in empty_fields.items():
                if count > 0:
                    print(f"       {field}ä¸ºç©º: {count} ä¸ª")
                    if count > len(jobs) * 0.1:
                        issues.append(f"{label}ä¸­{field}ä¸ºç©ºçš„æ¯”ä¾‹è¿‡é«˜: {count}/{len(jobs)}")
            
            if invalid_data:
                print(f"       æ•°æ®æ ¼å¼é—®é¢˜: {len(invalid_data)} ä¸ª")
                details[f"{label}_invalid_data"] = invalid_data[:5]
        
        check_data_quality(input_data, "è¾“å…¥æ•°æ®")
        check_data_quality(output_data, "è¾“å‡ºæ•°æ®")
        
        # æ£€æŸ¥æå–ç»“æœçš„åˆç†æ€§
        if extraction_output:
            print("   ğŸ“ æ¯•ä¸šæ—¶é—´åŒ¹é…åˆ†æ:")
            
            match_stats = defaultdict(int)
            for job in extraction_output:
                match_status = job.get('æ¯•ä¸šæ—¶é—´_åŒ¹é…çŠ¶æ€', 'æœªçŸ¥')
                match_stats[match_status] += 1
            
            details["graduation_match_stats"] = dict(match_stats)
            
            for status, count in match_stats.items():
                print(f"     {status}: {count} ä¸ª")
            
            # å¦‚æœæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„å²—ä½ï¼Œç»™å‡ºå»ºè®®
            suitable_count = match_stats.get('âœ… ç¬¦åˆ', 0) + match_stats.get('ç¬¦åˆ', 0)
            if suitable_count == 0 and len(extraction_output) > 0:
                recommendations.append("æ²¡æœ‰å‘ç°ç¬¦åˆæ¯•ä¸šæ—¶é—´è¦æ±‚çš„å²—ä½ï¼Œå»ºè®®è°ƒæ•´æœç´¢æ¡ä»¶æˆ–æ¯•ä¸šæ—¶é—´åŒ¹é…é€»è¾‘")
        
        return ValidationResult(
            passed=len(issues) == 0,
            details=details,
            issues=issues,
            recommendations=recommendations
        )
    
    # ========== 7. è¾¹ç•Œæ¡ä»¶éªŒè¯ ==========
    
    def validate_edge_cases(self) -> ValidationResult:
        """éªŒè¯è¾¹ç•Œæ¡ä»¶å’Œå¼‚å¸¸æƒ…å†µå¤„ç†"""
        print("\nğŸ” 7. è¾¹ç•Œæ¡ä»¶éªŒè¯")
        print("-" * 50)
        
        issues = []
        details = {}
        recommendations = []
        
        input_data = self.load_snapshot("local_dedup_input")
        output_data = self.load_snapshot("local_dedup_output")
        
        if not all([input_data, output_data]):
            issues.append("ç¼ºå°‘è¾¹ç•Œæ¡ä»¶éªŒè¯æ•°æ®")
            return ValidationResult(False, details, issues, recommendations)
        
        # æ£€æŸ¥ç©ºæ•°æ®å¤„ç†
        print("   ğŸ” ç©ºæ•°æ®å¤„ç†æ£€æŸ¥:")
        
        empty_field_jobs = []
        for job in output_data:
            empty_fields = []
            critical_fields = ['å²—ä½åç§°', 'å…¬å¸åç§°', 'å²—ä½é“¾æ¥']
            
            for field in critical_fields:
                if not job.get(field) or str(job.get(field)).strip() == '':
                    empty_fields.append(field)
            
            if empty_fields:
                empty_field_jobs.append({
                    'job': job,
                    'empty_fields': empty_fields
                })
        
        if empty_field_jobs:
            print(f"     âš ï¸  å‘ç° {len(empty_field_jobs)} ä¸ªå²—ä½æœ‰ç©ºå­—æ®µ")
            details["empty_field_jobs_count"] = len(empty_field_jobs)
            
            if len(empty_field_jobs) > len(output_data) * 0.1:
                issues.append("ç©ºå­—æ®µå²—ä½æ¯”ä¾‹è¿‡é«˜")
            
            # æ˜¾ç¤ºç¤ºä¾‹
            for i, item in enumerate(empty_field_jobs[:3], 1):
                job = item['job']
                print(f"       {i}. {job.get('å²—ä½åç§°', 'N/A')} - ç¼ºå°‘: {', '.join(item['empty_fields'])}")
        else:
            print("     âœ… æ— ç©ºå­—æ®µé—®é¢˜")
        
        # æ£€æŸ¥å¼‚å¸¸å­—ç¬¦å¤„ç†
        print("   ğŸ” å¼‚å¸¸å­—ç¬¦å¤„ç†æ£€æŸ¥:")
        
        problematic_jobs = []
        for job in output_data:
            issues_found = []
            
            # æ£€æŸ¥ç‰¹æ®Šå­—ç¬¦
            for field in ['å²—ä½åç§°', 'å…¬å¸åç§°']:
                value = job.get(field, '')
                if value:
                    if len(value) > 100:
                        issues_found.append(f"{field}è¿‡é•¿({len(value)}å­—ç¬¦)")
                    if re.search(r'[^\u4e00-\u9fa5a-zA-Z0-9\s\-_()ï¼ˆï¼‰]', value):
                        issues_found.append(f"{field}åŒ…å«ç‰¹æ®Šå­—ç¬¦")
            
            # æ£€æŸ¥URLæ ¼å¼
            url = job.get('å²—ä½é“¾æ¥', '')
            if url and not url.startswith(('http://', 'https://')):
                issues_found.append("URLæ ¼å¼å¼‚å¸¸")
            
            if issues_found:
                problematic_jobs.append({
                    'job': job,
                    'issues': issues_found
                })
        
        if problematic_jobs:
            print(f"     âš ï¸  å‘ç° {len(problematic_jobs)} ä¸ªå²—ä½æœ‰æ ¼å¼é—®é¢˜")
            details["problematic_jobs_count"] = len(problematic_jobs)
            
            for i, item in enumerate(problematic_jobs[:3], 1):
                job = item['job']
                print(f"       {i}. {job.get('å²—ä½åç§°', 'N/A')[:20]}... - é—®é¢˜: {', '.join(item['issues'])}")
        else:
            print("     âœ… æ— æ ¼å¼é—®é¢˜")
        
        # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
        print("   ğŸ” æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥:")
        
        # æ£€æŸ¥åŒä¸€å…¬å¸å²—ä½çš„ä¸€è‡´æ€§
        company_jobs = defaultdict(list)
        for job in output_data:
            company = job.get('å…¬å¸åç§°', '').strip()
            if company:
                company_jobs[company].append(job)
        
        inconsistent_companies = []
        for company, jobs in company_jobs.items():
            if len(jobs) > 1:
                # æ£€æŸ¥åŒä¸€å…¬å¸ä¸åŒå²—ä½çš„åœ°ç‚¹æ˜¯å¦åˆç†
                locations = [job.get('å·¥ä½œåœ°ç‚¹', '') for job in jobs]
                unique_locations = set(loc for loc in locations if loc)
                
                if len(unique_locations) > 3:  # åŒä¸€å…¬å¸è¶…è¿‡3ä¸ªä¸åŒåœ°ç‚¹å¯èƒ½æœ‰é—®é¢˜
                    inconsistent_companies.append({
                        'company': company,
                        'job_count': len(jobs),
                        'locations': list(unique_locations)
                    })
        
        if inconsistent_companies:
            print(f"     âš ï¸  å‘ç° {len(inconsistent_companies)} ä¸ªå…¬å¸åœ°ç‚¹åˆ†å¸ƒå¼‚å¸¸")
            details["inconsistent_companies"] = len(inconsistent_companies)
            
            for item in inconsistent_companies[:3]:
                print(f"       - {item['company']}: {item['job_count']}ä¸ªå²—ä½, {len(item['locations'])}ä¸ªåœ°ç‚¹")
        else:
            print("     âœ… å…¬å¸æ•°æ®ä¸€è‡´æ€§è‰¯å¥½")
        
        # ç”Ÿæˆå»ºè®®
        if empty_field_jobs:
            recommendations.append("å¢å¼ºæ•°æ®æ¸…æ´—é€»è¾‘ï¼Œå¤„ç†ç©ºå­—æ®µé—®é¢˜")
        if problematic_jobs:
            recommendations.append("æ·»åŠ æ•°æ®æ ¼å¼éªŒè¯å’Œæ¸…ç†")
        if inconsistent_companies:
            recommendations.append("æ£€æŸ¥çˆ¬è™«æ•°æ®è´¨é‡ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®æ±¡æŸ“")
        
        if not any([empty_field_jobs, problematic_jobs, inconsistent_companies]):
            recommendations.append("è¾¹ç•Œæ¡ä»¶å¤„ç†è‰¯å¥½")
        
        return ValidationResult(
            passed=len(issues) == 0,
            details=details,
            issues=issues,
            recommendations=recommendations
        )
    
    # ========== 8. ä¸€è‡´æ€§éªŒè¯ ==========
    
    def validate_consistency(self) -> ValidationResult:
        """éªŒè¯ç³»ç»Ÿå†…éƒ¨ä¸€è‡´æ€§"""
        print("\nğŸ” 8. ç³»ç»Ÿä¸€è‡´æ€§éªŒè¯")
        print("-" * 50)
        
        issues = []
        details = {}
        recommendations = []
        
        # æ£€æŸ¥å¿«ç…§ä¹‹é—´çš„æ•°æ®ä¸€è‡´æ€§
        print("   ğŸ”„ å¿«ç…§æ•°æ®ä¸€è‡´æ€§:")
        
        local_input = self.load_snapshot("local_dedup_input")
        local_output = self.load_snapshot("local_dedup_output")
        notion_output = self.load_snapshot("notion_dedup_output")
        
        if not all([local_input, local_output]):
            issues.append("ç¼ºå°‘ä¸€è‡´æ€§éªŒè¯æ•°æ®")
            return ValidationResult(False, details, issues, recommendations)
        
        # éªŒè¯æ•°æ®æµè½¬çš„ä¸€è‡´æ€§
        input_urls = {job.get('å²—ä½é“¾æ¥', '') for job in local_input}
        output_urls = {job.get('å²—ä½é“¾æ¥', '') for job in local_output}
        
        # æ£€æŸ¥è¾“å‡ºä¸­æ˜¯å¦æœ‰ä¸åœ¨è¾“å…¥ä¸­çš„URL
        extra_urls = output_urls - input_urls
        if extra_urls:
            issues.append(f"è¾“å‡ºä¸­å‘ç° {len(extra_urls)} ä¸ªä¸åœ¨è¾“å…¥ä¸­çš„URL")
            details["extra_urls"] = list(extra_urls)[:5]
            print(f"     âŒ å‘ç°é¢å¤–URL: {len(extra_urls)} ä¸ª")
        else:
            print(f"     âœ… URLæ•°æ®æµè½¬ä¸€è‡´")
        
        # æ£€æŸ¥å»é‡é€»è¾‘çš„ç¡®å®šæ€§
        print("   ğŸ¯ å»é‡é€»è¾‘ç¡®å®šæ€§:")
        
        # å¦‚æœæœ‰ç›¸åŒçš„è¾“å…¥ï¼Œå»é‡ç»“æœåº”è¯¥æ˜¯ç¡®å®šçš„
        # è¿™é‡Œæˆ‘ä»¬æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒçš„å²—ä½åœ¨ä¸åŒå¤„ç†ä¸­å¾—åˆ°äº†ä¸åŒçš„ç»“æœ
        
        def create_job_signature(job):
            return f"{job.get('å²—ä½åç§°', '')}_{job.get('å…¬å¸åç§°', '')}_{job.get('å²—ä½é“¾æ¥', '')}"
        
        input_signatures = {create_job_signature(job): job for job in local_input}
        output_signatures = {create_job_signature(job): job for job in local_output}
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç­¾åä¸ä¸€è‡´ä½†è¢«è®¤ä¸ºæ˜¯åŒä¸€å²—ä½çš„æƒ…å†µ
        signature_inconsistencies = []
        for sig, input_job in input_signatures.items():
            if sig in output_signatures:
                output_job = output_signatures[sig]
                # æ£€æŸ¥é™¤äº†æ ¸å¿ƒå­—æ®µå¤–çš„å…¶ä»–å­—æ®µæ˜¯å¦ä¸€è‡´
                for field in ['å·¥ä½œåœ°ç‚¹', 'è–ªèµ„']:
                    if input_job.get(field) != output_job.get(field):
                        signature_inconsistencies.append({
                            'job_signature': sig,
                            'field': field,
                            'input_value': input_job.get(field),
                            'output_value': output_job.get(field)
                        })
        
        if signature_inconsistencies:
            print(f"     âš ï¸  å‘ç° {len(signature_inconsistencies)} ä¸ªå­—æ®µä¸ä¸€è‡´")
            details["signature_inconsistencies"] = len(signature_inconsistencies)
            recommendations.append("æ£€æŸ¥æ•°æ®ä¼ é€’è¿‡ç¨‹æ˜¯å¦æœ‰å­—æ®µä¿®æ”¹")
        else:
            print(f"     âœ… å»é‡è¿‡ç¨‹æ•°æ®ä¸€è‡´")
        
        # æ£€æŸ¥ç¼“å­˜ä¸€è‡´æ€§
        if notion_output:
            print("   ğŸ’¾ ç¼“å­˜ä¸€è‡´æ€§:")
            
            notion_cache = self.load_snapshot("notion_cache")
            if notion_cache:
                cached_urls = set(notion_cache.get("existing_urls", []))
                
                # éªŒè¯è¢«å»é‡çš„å²—ä½ç¡®å®åœ¨ç¼“å­˜ä¸­
                local_urls = {job.get('å²—ä½é“¾æ¥', '') for job in local_output}
                notion_urls = {job.get('å²—ä½é“¾æ¥', '') for job in notion_output}
                removed_urls = local_urls - notion_urls
                
                cache_verified = 0
                cache_not_found = 0
                
                for url in removed_urls:
                    clean_url = self._clean_url_for_cache(url)
                    if clean_url in cached_urls:
                        cache_verified += 1
                    else:
                        cache_not_found += 1
                
                details.update({
                    "cache_verified_count": cache_verified,
                    "cache_not_found_count": cache_not_found
                })
                
                if cache_not_found > 0:
                    cache_consistency_rate = cache_verified / (cache_verified + cache_not_found)
                    print(f"     ğŸ“Š ç¼“å­˜ä¸€è‡´æ€§: {cache_consistency_rate:.1%}")
                    
                    if cache_consistency_rate < 0.9:
                        issues.append(f"ç¼“å­˜ä¸€è‡´æ€§åä½: {cache_consistency_rate:.1%}")
                        recommendations.append("æ£€æŸ¥ç¼“å­˜æ›´æ–°é€»è¾‘")
                    else:
                        print(f"     âœ… ç¼“å­˜ä¸€è‡´æ€§è‰¯å¥½")
                else:
                    print(f"     âœ… ç¼“å­˜å®Œå…¨ä¸€è‡´")
        
        return ValidationResult(
            passed=len(issues) == 0,
            details=details,
            issues=issues,
            recommendations=recommendations
        )
    
    # ========== å·¥å…·æ–¹æ³• ==========
    
    def _clean_url_for_cache(self, url: str) -> str:
        """æ¸…ç†URLç”¨äºç¼“å­˜æ¯”è¾ƒ"""
        if not url:
            return ""
        base_url = url.split('?')[0].split('#')[0]
        match = re.search(r'/job_detail/([^/.]+)', base_url)
        return match.group(1) if match else base_url.split('/')[-1] if '/' in base_url else base_url
    
    def _create_cache_fingerprint(self, job: dict) -> str:
        """åˆ›å»ºç¼“å­˜æŒ‡çº¹"""
        company = job.get('å…¬å¸åç§°', '').strip().lower()
        title = job.get('å²—ä½åç§°', '').strip().lower()
        location = job.get('å·¥ä½œåœ°ç‚¹', '').strip().lower()
        
        company = re.sub(r'æœ‰é™å…¬å¸$|ç§‘æŠ€æœ‰é™å…¬å¸$', '', company)
        location = re.sub(r'[Â·\s]*[^ï¼Œã€‚\s]*åŒº', '', location)
        
        fingerprint_text = f"{company}_{title}_{location}"
        return hashlib.md5(fingerprint_text.encode()).hexdigest()
    
    # ========== ä¸»éªŒè¯æ–¹æ³• ==========
    
    def run_comprehensive_validation(self) -> Dict[str, ValidationResult]:
        """è¿è¡Œå…¨é¢éªŒè¯"""
        print("ğŸš€ å¯åŠ¨å»é‡æ¨¡å—å…¨é¢éªŒè¯")
        print(f"éªŒè¯æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)
        
        # æ‰§è¡Œæ‰€æœ‰éªŒè¯
        validation_methods = [
            ("æ•°æ®å®Œæ•´æ€§", self.validate_data_integrity),
            ("URLå»é‡", self.validate_url_deduplication),
            ("å†…å®¹å»é‡", self.validate_content_deduplication),
            ("Notionå¢é‡å»é‡", self.validate_notion_incremental_dedup),
            ("æ€§èƒ½æ•ˆç‡", self.validate_dedup_performance),
            ("ä¸šåŠ¡é€»è¾‘", self.validate_business_logic),
            ("è¾¹ç•Œæ¡ä»¶", self.validate_edge_cases),
            ("ç³»ç»Ÿä¸€è‡´æ€§", self.validate_consistency)
        ]
        
        results = {}
        
        for test_name, test_method in validation_methods:
            try:
                result = test_method()
                results[test_name] = result
                self.validation_results[test_name] = result
                
                # æ”¶é›†å…¨å±€é—®é¢˜å’Œå»ºè®®
                self.global_issues.extend(result.issues)
                self.global_recommendations.extend(result.recommendations)
                
            except Exception as e:
                error_result = ValidationResult(
                    passed=False,
                    details={"error": str(e)},
                    issues=[f"éªŒè¯è¿‡ç¨‹å¼‚å¸¸: {e}"],
                    recommendations=["æ£€æŸ¥éªŒè¯ç¯å¢ƒå’Œæ•°æ®å®Œæ•´æ€§"]
                )
                results[test_name] = error_result
                self.validation_results[test_name] = error_result
                print(f"   âŒ éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
        
        return results
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆå…¨é¢éªŒè¯æŠ¥å‘Š"""
        results = self.run_comprehensive_validation()
        
        print("\n" + "=" * 80)
        print("ğŸ“Š å…¨é¢éªŒè¯ç»“æœæ±‡æ€»")
        print("=" * 80)
        
        # ç»Ÿè®¡éªŒè¯ç»“æœ
        passed_tests = sum(1 for result in results.values() if result.passed)
        total_tests = len(results)
        success_rate = passed_tests / total_tests if total_tests > 0 else 0
        
        print(f"éªŒè¯é¡¹ç›®: {total_tests} é¡¹")
        print(f"é€šè¿‡éªŒè¯: {passed_tests} é¡¹")
        print(f"éªŒè¯å¤±è´¥: {total_tests - passed_tests} é¡¹")
        print(f"æˆåŠŸç‡: {success_rate:.1%}")
        print()
        
        # è¯¦ç»†ç»“æœ
        for test_name, result in results.items():
            status = "âœ… é€šè¿‡" if result.passed else "âŒ å¤±è´¥"
            print(f"{test_name}: {status}")
            
            if result.issues:
                for issue in result.issues[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªé—®é¢˜
                    print(f"  âš ï¸  {issue}")
            
            if not result.passed and result.recommendations:
                for rec in result.recommendations[:2]:  # åªæ˜¾ç¤ºå‰2ä¸ªå»ºè®®
                    print(f"  ğŸ’¡ {rec}")
        
        # å…¨å±€è¯„ä¼°
        print("\n" + "=" * 80)
        print("ğŸ¯ å…¨å±€è¯„ä¼°")
        print("=" * 80)
        
        if success_rate >= 0.9:
            overall_status = "âœ… ä¼˜ç§€"
            assessment = "å»é‡æ¨¡å—å·¥ä½œæ­£å¸¸ï¼Œå¯ä»¥æ”¾å¿ƒä½¿ç”¨"
        elif success_rate >= 0.7:
            overall_status = "âš ï¸ è‰¯å¥½"
            assessment = "å»é‡æ¨¡å—åŸºæœ¬æ­£å¸¸ï¼Œå»ºè®®ä¼˜åŒ–éƒ¨åˆ†é—®é¢˜"
        elif success_rate >= 0.5:
            overall_status = "âš ï¸ ä¸€èˆ¬"
            assessment = "å»é‡æ¨¡å—å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œéœ€è¦é‡ç‚¹å…³æ³¨å’Œæ”¹è¿›"
        else:
            overall_status = "âŒ éœ€è¦æ”¹è¿›"
            assessment = "å»é‡æ¨¡å—å­˜åœ¨ä¸¥é‡é—®é¢˜ï¼Œå»ºè®®æ·±å…¥æ’æŸ¥å’Œä¿®å¤"
        
        print(f"æ•´ä½“çŠ¶æ€: {overall_status}")
        print(f"è¯„ä¼°ç»“è®º: {assessment}")
        
        # ä¼˜å…ˆçº§å»ºè®®
        if self.global_issues:
            print(f"\nğŸ”§ éœ€è¦ä¿®å¤çš„é—®é¢˜ (å‰5ä¸ª):")
            for i, issue in enumerate(self.global_issues[:5], 1):
                print(f"  {i}. {issue}")
        
        if self.global_recommendations:
            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®® (å‰5ä¸ª):")
            unique_recommendations = list(dict.fromkeys(self.global_recommendations))  # å»é‡
            for i, rec in enumerate(unique_recommendations[:5], 1):
                print(f"  {i}. {rec}")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        self._save_validation_report(results, success_rate, overall_status, assessment)
        
        return {
            "validation_results": results,
            "success_rate": success_rate,
            "overall_status": overall_status,
            "assessment": assessment,
            "global_issues": self.global_issues,
            "global_recommendations": unique_recommendations
        }
    
    def _save_validation_report(self, results: Dict[str, ValidationResult], 
                               success_rate: float, overall_status: str, assessment: str):
        """ä¿å­˜éªŒè¯æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"debug/dedup_validation_report_{timestamp}.json"
        
        # æ„å»ºæŠ¥å‘Šæ•°æ®
        report_data = {
            "validation_info": {
                "session_id": self.session_id,
                "validation_time": datetime.now().isoformat(),
                "validator_version": "1.0.0"
            },
            "summary": {
                "total_tests": len(results),
                "passed_tests": sum(1 for r in results.values() if r.passed),
                "success_rate": success_rate,
                "overall_status": overall_status,
                "assessment": assessment
            },
            "detailed_results": {},
            "global_issues": self.global_issues,
            "global_recommendations": list(dict.fromkeys(self.global_recommendations))
        }
        
        # è½¬æ¢ValidationResultä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        for test_name, result in results.items():
            report_data["detailed_results"][test_name] = {
                "passed": result.passed,
                "details": result.details,
                "issues": result.issues,
                "recommendations": result.recommendations
            }
        
        try:
            os.makedirs("debug", exist_ok=True)
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ“‹ è¯¦ç»†éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜éªŒè¯æŠ¥å‘Šå¤±è´¥: {e}")

# ========== ä¸»ç¨‹åºå…¥å£ ==========

def main():
    """ä¸»ç¨‹åº"""
    try:
        # åˆ›å»ºéªŒè¯å™¨
        validator = ComprehensiveDeduplicationValidator()
        
        # è¿è¡Œå…¨é¢éªŒè¯
        report = validator.generate_comprehensive_report()
        
        # è¿”å›é€‚å½“çš„é€€å‡ºç 
        success_rate = report["success_rate"]
        if success_rate >= 0.9:
            exit_code = 0  # æˆåŠŸ
        elif success_rate >= 0.7:
            exit_code = 1  # è­¦å‘Š
        else:
            exit_code = 2  # å¤±è´¥
        
        print(f"\nç¨‹åºé€€å‡ºç : {exit_code}")
        return exit_code
        
    except FileNotFoundError as e:
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {e}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œæµæ°´çº¿ç”Ÿæˆå¿…è¦çš„å¿«ç…§æ•°æ®")
        return 3
        
    except Exception as e:
        print(f"âŒ éªŒè¯è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return 4

if __name__ == "__main__":
    exit(main())
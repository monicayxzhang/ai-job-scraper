"""
å¢å¼ºç‰ˆNotionæå–å™¨
æ–°å¢åŠŸèƒ½ï¼š
1. æ¯•ä¸šæ—¶é—´è¦æ±‚æå–
2. æ‹›è˜æˆªæ­¢æ—¥æœŸæå–
3. æ‹›å‹Ÿæ–¹å‘æå–
4. æ—¥æœŸæ ‡å‡†åŒ–å¤„ç†
5. æ¯•ä¸šæ—¶é—´åŒ¹é…åˆ¤æ–­
"""
import httpx
import json
import os
import re
import asyncio
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, Tuple
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# å°è¯•åŠ è½½ä¸åŒè·¯å¾„çš„.envæ–‡ä»¶
for env_path in [".env", "../.env", "../../.env"]:
    if os.path.exists(env_path):
        load_dotenv(env_path)
        print(f"[OK] Loading environment variables: {env_path}")
        break

class EnhancedNotionExtractor:
    def __init__(self, provider=None, config=None):
        """å¢å¼ºç‰ˆNotionæå–å™¨"""
        self.provider = provider or os.getenv("LLM_PROVIDER", "deepseek")
        self.config = config or {}
        self.api_key = None
        self.base_url = None
        self.model = None
        
        # ç”¨æˆ·æ¯•ä¸šä¿¡æ¯ï¼ˆå¯é…ç½®ï¼‰
        self.user_graduation = "2023-12"  # 2023å¹´12æœˆæ¯•ä¸š
        
        self.temperature = self._get_config_value("temperature", 0)
        self.max_tokens = self._get_config_value("max_tokens", 1000)
        
        self._setup_provider()
    
    def _get_config_value(self, key: str, default_value):
        """è·å–é…ç½®å€¼"""
        env_key = f"LLM_{key.upper()}"
        env_value = os.getenv(env_key)
        if env_value is not None:
            try:
                if isinstance(default_value, int):
                    return int(env_value)
                elif isinstance(default_value, float):
                    return float(env_value)
                else:
                    return env_value
            except ValueError:
                pass
        
        llm_config = self.config.get("llm", {})
        if key in llm_config:
            return llm_config[key]
        
        return default_value
    
    def _setup_provider(self):
        """è®¾ç½®APIé…ç½®"""
        if self.provider == "deepseek":
            self.api_key = os.getenv("DEEPSEEK_API_KEY")
            self.base_url = "https://api.deepseek.com/v1"
            self.model = os.getenv("DEEPSEEK_MODEL", "deepseek-chat")
        elif self.provider == "zhipu":
            self.api_key = os.getenv("ZHIPU_API_KEY")
            self.base_url = "https://open.bigmodel.cn/api/paas/v4"
            self.model = os.getenv("ZHIPU_MODEL", "glm-4-flash")
        elif self.provider == "siliconflow":
            self.api_key = os.getenv("SILICONFLOW_API_KEY")
            self.base_url = "https://api.siliconflow.cn/v1"
            self.model = os.getenv("SILICONFLOW_MODEL", "Qwen/Qwen2.5-7B-Instruct")
        elif self.provider == "01ai":
            self.api_key = os.getenv("LINGYIWANWU_API_KEY")
            self.base_url = "https://api.lingyiwanwu.com/v1"
            self.model = os.getenv("LINGYIWANWU_MODEL", "yi-large")
        else:  # openai
            self.api_key = os.getenv("OPENAI_API_KEY")
            self.base_url = "https://api.openai.com/v1"
            self.model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        
        generic_model = os.getenv("LLM_MODEL")
        if generic_model:
            self.model = generic_model
        
        if not self.api_key:
            print(f"âš ï¸  {self.provider.upper()}_API_KEY æœªé…ç½®")
    
    def standardize_date_format(self, raw_date: str) -> str:
        """å°†å„ç§æ—¥æœŸæ ¼å¼æ ‡å‡†åŒ–ä¸º YYYY-MM-DD"""
        if not raw_date or not raw_date.strip():
            return ""
        
        # æ¸…ç†æ–‡æœ¬
        text = raw_date.strip()
        
        # å¤„ç†å±Šåˆ«æ ¼å¼
        if 'å±Š' in text:
            return text  # ä¿æŒå±Šåˆ«æ ¼å¼ "2024å±Š"
        
        # å¤„ç†èŒƒå›´æ ¼å¼
        if any(sep in text for sep in ['-', 'åˆ°', 'è‡³', '~']):
            # å¦‚æœæ˜¯æ—¶é—´èŒƒå›´ï¼Œå°è¯•è§£æä¸¤ä¸ªæ—¥æœŸ
            for sep in ['-', 'åˆ°', 'è‡³', '~']:
                if sep in text and 'å¹´' in text:
                    parts = text.split(sep)
                    if len(parts) == 2:
                        start_date = self._parse_single_date(parts[0].strip())
                        end_date = self._parse_single_date(parts[1].strip())
                        if start_date and end_date:
                            return f"{start_date}åˆ°{end_date}"
        
        # å¤„ç†å•ä¸ªæ—¥æœŸ
        return self._parse_single_date(text)
    
    def _parse_single_date(self, date_str: str) -> str:
        """è§£æå•ä¸ªæ—¥æœŸå­—ç¬¦ä¸²"""
        if not date_str:
            return ""
        
        # ç§»é™¤å¸¸è§å‰ç¼€
        text = re.sub(r'^(æˆªæ­¢æ—¥æœŸ|æŠ¥åæˆªæ­¢|ç”³è¯·æˆªæ­¢|æ‹›è˜æˆªæ­¢|æ¯•ä¸šæ—¶é—´)[ï¼š:]\s*', '', date_str)
        
        # æ ‡å‡†åŒ–æ¨¡å¼
        patterns = [
            # å®Œæ•´æ—¥æœŸæ ¼å¼
            (r'(\d{4})[./å¹´](\d{1,2})[./æœˆ](\d{1,2})[æ—¥]?', lambda m: f"{m.group(1)}-{int(m.group(2)):02d}-{int(m.group(3)):02d}"),
            # å¹´æœˆæ ¼å¼
            (r'(\d{4})[./å¹´](\d{1,2})[æœˆ]?', lambda m: f"{m.group(1)}-{int(m.group(2)):02d}-01"),
            # åªæœ‰å¹´ä»½
            (r'(\d{4})å¹´?', lambda m: f"{m.group(1)}-01-01"),
            # ç®€åŒ–å¹´ä»½æ ¼å¼ (å¦‚24å¹´)
            (r'(\d{2})[./å¹´](\d{1,2})[./æœˆ](\d{1,2})[æ—¥]?', lambda m: f"20{m.group(1)}-{int(m.group(2)):02d}-{int(m.group(3)):02d}"),
        ]
        
        for pattern, formatter in patterns:
            match = re.search(pattern, text)
            if match:
                try:
                    return formatter(match)
                except:
                    continue
        
        return date_str  # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›åŸæ–‡
    
    def check_graduation_eligibility(self, graduation_requirement: str) -> str:
        """æ£€æŸ¥æ˜¯å¦ç¬¦åˆæ¯•ä¸šè¦æ±‚"""
        if not graduation_requirement:
            return "æœªçŸ¥"
        
        req = graduation_requirement.lower()
        
        # 2024å±Šæ¯•ä¸šç”Ÿï¼ˆé€šå¸¸åŒ…å«2023å¹´11æœˆ-2024å¹´8æœˆï¼‰
        if "2024å±Š" in graduation_requirement:
            return "âœ… ç¬¦åˆ (2024å±ŠåŒ…å«2023å¹´12æœˆæ¯•ä¸š)"
        
        # 2025å±Šæ¯•ä¸šç”Ÿ
        if "2025å±Š" in graduation_requirement:
            return "âŒ ä¸ç¬¦åˆ (2025å±Šä¸º2024å¹´æ¯•ä¸š)"
        
        # 2023å±Šæ¯•ä¸šç”Ÿ
        if "2023å±Š" in graduation_requirement:
            return "âŒ ä¸ç¬¦åˆ (2023å±Šä¸º2022å¹´æ¯•ä¸š)"
        
        # å…·ä½“æ—¶é—´èŒƒå›´åˆ¤æ–­
        if "2023" in req and "2024" in req:
            if any(month in req for month in ["11æœˆ", "12æœˆ", "1æœˆ", "2æœˆ", "3æœˆ", "4æœˆ", "5æœˆ", "6æœˆ", "7æœˆ", "8æœˆ"]):
                # æ£€æŸ¥æ˜¯å¦åŒ…å«12æœˆ
                if "12æœˆ" in req or ("11æœˆ" in req and "8æœˆ" in req):
                    return "âœ… ç¬¦åˆ (æ—¶é—´èŒƒå›´åŒ…å«2023å¹´12æœˆ)"
        
        # å¦‚æœåŒ…å«åº”å±Šç”Ÿç­‰å…³é”®è¯
        if any(keyword in req for keyword in ["åº”å±Š", "æ ¡æ‹›", "æ¯•ä¸šç”Ÿ"]):
            return "âš ï¸ éœ€è¦ç¡®è®¤ (åº”å±Šç”Ÿæ‹›è˜ï¼Œå»ºè®®æŸ¥çœ‹è¯¦ç»†è¦æ±‚)"
        
        return "âŒ ä¸ç¬¦åˆæˆ–éœ€è¦äººå·¥ç¡®è®¤"
    
    def check_deadline_status(self, deadline_date: str) -> str:
        """æ£€æŸ¥æˆªæ­¢æ—¥æœŸçŠ¶æ€"""
        if not deadline_date:
            return "æœªçŸ¥"
        
        try:
            # è§£ææ ‡å‡†åŒ–æ—¥æœŸ
            if re.match(r'\d{4}-\d{2}-\d{2}$', deadline_date):
                deadline = datetime.strptime(deadline_date, "%Y-%m-%d")
                now = datetime.now()
                
                if deadline < now:
                    return "âŒ å·²è¿‡æœŸ"
                elif deadline < now + timedelta(days=7):
                    return "âš ï¸ å³å°†æˆªæ­¢"
                else:
                    return "âœ… æœªè¿‡æœŸ"
            else:
                return "âš ï¸ æ—¥æœŸæ ¼å¼å¼‚å¸¸"
        except:
            return "âŒ æ—¥æœŸè§£æå¤±è´¥"
    
    def _extract_structured_info(self, html: str, url: str, job_data: Optional[Dict] = None) -> Dict[str, str]:
        """ç»“æ„åŒ–æå–ï¼šå¢å¼ºç‰ˆæœ¬"""
        info = {
            "å²—ä½åç§°": "",
            "è–ªèµ„": "", 
            "å·¥ä½œåœ°ç‚¹": "",
            "ç»éªŒè¦æ±‚": "",
            "å‘å¸ƒå¹³å°": "",
            "HRæ´»è·ƒåº¦": "",
            "é¡µé¢æŠ“å–æ—¶é—´": "",
            # æ–°å¢å­—æ®µ
            "æ¯•ä¸šæ—¶é—´è¦æ±‚": "",
            "æ‹›è˜æˆªæ­¢æ—¥æœŸ": "",
            "æ‹›å‹Ÿæ–¹å‘": ""
        }
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            text = soup.get_text()
            
            # 1. é¡µé¢æŠ“å–æ—¶é—´ - ä»åŸå§‹æ•°æ®è·å–
            if job_data:
                timestamp_fields = ['timestamp', 'åŸå§‹æ—¶é—´æˆ³', 'crawl_time', 'created_at']
                for field in timestamp_fields:
                    if field in job_data and job_data[field]:
                        try:
                            if isinstance(job_data[field], str):
                                crawl_time = datetime.strptime(job_data[field], "%Y-%m-%d %H:%M:%S")
                                info["é¡µé¢æŠ“å–æ—¶é—´"] = crawl_time.strftime("%Y-%m-%d")
                                print(f"ğŸ“… è·å–æŠ“å–æ—¶é—´: {info['é¡µé¢æŠ“å–æ—¶é—´']} (æ¥æº: {field})")
                                break
                        except (ValueError, TypeError) as e:
                            print(f"âš ï¸  æ—¶é—´è§£æå¤±è´¥ {field}: {job_data[field]}, é”™è¯¯: {e}")
                            continue
            
            # 2. ä»URLå’Œæ ‡é¢˜æå–å²—ä½åç§°
            title = soup.find('title')
            if title:
                title_text = title.get_text()
                title_match = re.search(r'ã€Œ([^ã€]+?)(?:æ‹›è˜|å²—ä½)?ã€', title_text)
                if title_match:
                    info["å²—ä½åç§°"] = title_match.group(1)
                    print(f"ğŸ“‹ ä»æ ‡é¢˜æå–å²—ä½åç§°: {info['å²—ä½åç§°']}")
            
            # 3. åˆ¤æ–­å‘å¸ƒå¹³å°
            if 'zhipin.com' in url:
                info["å‘å¸ƒå¹³å°"] = "Bossç›´è˜"
                
                # HRæ´»è·ƒåº¦
                activity_patterns = [
                    r'(\d+æ—¥?å†…æ´»è·ƒ)', r'(åˆšåˆšæ´»è·ƒ)', r'(ä»Šæ—¥æ´»è·ƒ)', 
                    r'(æœ¬å‘¨æ´»è·ƒ)', r'(\d+åˆ†é’Ÿå‰æ´»è·ƒ)', r'(\d+å°æ—¶å‰æ´»è·ƒ)'
                ]
                for pattern in activity_patterns:
                    match = re.search(pattern, text)
                    if match:
                        info["HRæ´»è·ƒåº¦"] = match.group(1)
                        break
            
            # 4. è–ªèµ„æå–
            salary_patterns = [
                r'(\d+[-~åˆ°]\d+[kKä¸‡](?:Â·\d+è–ª)?)',
                r'(\d+[kK][-~åˆ°]\d+[kK](?:Â·\d+è–ª)?)',
                r'(\d+ä¸‡[-~åˆ°]\d+ä¸‡(?:Â·\d+è–ª)?)',
                r'(\d+[kK]\+(?:Â·\d+è–ª)?)',
                r'(\d+ä¸‡\+(?:Â·\d+è–ª)?)',
                r'(\d+[-~åˆ°]\d+ä¸‡/å¹´)',
                r'(\d+[-~åˆ°]\d+å…ƒ/å¤©)',
                r'(\d+[kKä¸‡]Â·\d+è–ª)',
                r'(é¢è®®)',
                r'(\d+ä¸‡(?:Â·\d+è–ª)?)',
                r'(\d+[kK](?:Â·\d+è–ª)?)'
            ]
            
            for i, pattern in enumerate(salary_patterns):
                matches = re.findall(pattern, text, re.IGNORECASE)
                if matches:
                    salary = max(matches, key=len)
                    info["è–ªèµ„"] = salary
                    print(f"ğŸ’° æå–è–ªèµ„: {salary} (æ¨¡å¼{i+1})")
                    if len(matches) > 1:
                        print(f"   ğŸ” æ‰€æœ‰åŒ¹é…: {matches}")
                    break
            
            # 5. å·¥ä½œåœ°ç‚¹æå–ï¼ˆåªä¿ç•™åŸå¸‚ï¼‰
            location_patterns = [
                r'(åŒ—äº¬)(?:å¸‚|[Â·\s]*[^ï¼Œã€‚\s\d]*åŒº)?(?![Â·\s]*\d+[-~]\d*å¹´)',
                r'(ä¸Šæµ·)(?:å¸‚|[Â·\s]*[^ï¼Œã€‚\s\d]*åŒº)?(?![Â·\s]*\d+[-~]\d*å¹´)',
                r'(æ·±åœ³)(?:å¸‚|[Â·\s]*[^ï¼Œã€‚\s\d]*åŒº)?(?![Â·\s]*\d+[-~]\d*å¹´)',
                r'(æ­å·)(?:å¸‚|[Â·\s]*[^ï¼Œã€‚\s\d]*åŒº)?(?![Â·\s]*\d+[-~]\d*å¹´)',
                r'(å¹¿å·)(?:å¸‚|[Â·\s]*[^ï¼Œã€‚\s\d]*åŒº)?(?![Â·\s]*\d+[-~]\d*å¹´)',
                r'(æˆéƒ½)(?:å¸‚|[Â·\s]*[^ï¼Œã€‚\s\d]*åŒº)?(?![Â·\s]*\d+[-~]\d*å¹´)',
                r'(æ­¦æ±‰)(?:å¸‚|[Â·\s]*[^ï¼Œã€‚\s\d]*åŒº)?(?![Â·\s]*\d+[-~]\d*å¹´)',
                r'(è¥¿å®‰)(?:å¸‚|[Â·\s]*[^ï¼Œã€‚\s\d]*åŒº)?(?![Â·\s]*\d+[-~]\d*å¹´)',
                r'(å—äº¬)(?:å¸‚|[Â·\s]*[^ï¼Œã€‚\s\d]*åŒº)?(?![Â·\s]*\d+[-~]\d*å¹´)',
                r'(è‹å·)(?:å¸‚|[Â·\s]*[^ï¼Œã€‚\s\d]*åŒº)?(?![Â·\s]*\d+[-~]\d*å¹´)',
                r'(å¤©æ´¥)(?:å¸‚|[Â·\s]*[^ï¼Œã€‚\s\d]*åŒº)?(?![Â·\s]*\d+[-~]\d*å¹´)',
                r'(é‡åº†)(?:å¸‚|[Â·\s]*[^ï¼Œã€‚\s\d]*åŒº)?(?![Â·\s]*\d+[-~]\d*å¹´)',
                r'(è¿œç¨‹åŠå…¬)', r'(åœ¨å®¶åŠå…¬)', r'(å…¨è¿œç¨‹)', r'(Remote)'
            ]
            
            for i, pattern in enumerate(location_patterns):
                matches = re.findall(pattern, text, re.IGNORECASE)
                if matches:
                    location = matches[0]
                    location = re.sub(r'\s*\d+[-~]\d*å¹´.*$', '', location).strip()
                    info["å·¥ä½œåœ°ç‚¹"] = location
                    print(f"ğŸŒ æå–åœ°ç‚¹: {location} (æ¨¡å¼{i+1})")
                    break
            
            # 6. ç»éªŒè¦æ±‚æå–
            exp_patterns = [
                r'(\d+[-~]\d+å¹´å·¥ä½œç»éªŒ)', r'(\d+[-~]\d+å¹´ç»éªŒ)', r'(\d+å¹´ä»¥ä¸Šå·¥ä½œç»éªŒ)',
                r'(\d+å¹´ä»¥ä¸Šç»éªŒ)', r'(\d+\+å¹´ç»éªŒ)', r'(\d+å¹´å·¥ä½œç»éªŒ)', r'(\d+å¹´ç»éªŒ)',
                r'(åº”å±Šæ¯•ä¸šç”Ÿ)', r'(åº”å±Šç”Ÿ)', r'(å®ä¹ ç”Ÿ)', r'(ç»éªŒä¸é™)',
                r'(åœ¨æ ¡/åº”å±Š)', r'(æ ¡æ‹›)', r'(æ— ç»éªŒè¦æ±‚)', r'(ä¸é™ç»éªŒ)',
                r'(é¢å‘\d+å±Š)', r'(ä¸é™)'
            ]
            
            for pattern in exp_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                if matches:
                    exp = matches[0]
                    if exp in ["ä¸é™", "ç»éªŒä¸é™", "æ— ç»éªŒè¦æ±‚", "ä¸é™ç»éªŒ"]:
                        info["ç»éªŒè¦æ±‚"] = "ç»éªŒä¸é™"
                    else:
                        info["ç»éªŒè¦æ±‚"] = exp
                    print(f"ğŸ“… æå–ç»éªŒ: {info['ç»éªŒè¦æ±‚']}")
                    break
            
            # 7. æ–°å¢ï¼šæ¯•ä¸šæ—¶é—´è¦æ±‚æå–
            graduation_patterns = [
                r'é¢å‘(\d{4})å±Š',
                r'(\d{4})å±Šæ¯•ä¸šç”Ÿ',
                r'æ¯•ä¸šæ—¶é—´[ï¼š:]\s*(\d{4}å¹´?\s*[-~åˆ°è‡³]\s*\d{4}å¹´?)',
                r'(\d{4}å¹´\d{1,2}æœˆ?\s*[-~åˆ°è‡³]\s*\d{4}å¹´\d{1,2}æœˆ?)',
                r'(\d{4}[./å¹´]\d{1,2}[./æœˆ]?\s*[-~åˆ°è‡³]\s*\d{4}[./å¹´]\d{1,2}[./æœˆ]?)',
                r'é¢å‘.*?(\d{4}å¹´\d{1,2}æœˆ[-~åˆ°è‡³]\d{4}å¹´\d{1,2}æœˆ).*?æ¯•ä¸š',
            ]
            
            for i, pattern in enumerate(graduation_patterns):
                matches = re.findall(pattern, text, re.IGNORECASE)
                if matches:
                    graduation_req = matches[0]
                    if pattern == graduation_patterns[0] or pattern == graduation_patterns[1]:
                        graduation_req = f"{graduation_req}å±Š"
                    info["æ¯•ä¸šæ—¶é—´è¦æ±‚"] = graduation_req
                    print(f"ğŸ“ æå–æ¯•ä¸šæ—¶é—´è¦æ±‚: {graduation_req} (æ¨¡å¼{i+1})")
                    break
            
            # 8. æ–°å¢ï¼šæ‹›è˜æˆªæ­¢æ—¥æœŸæå–
            deadline_patterns = [
                r'æˆªæ­¢æ—¥æœŸ[ï¼š:]\s*(\d{4}[./å¹´]\d{1,2}[./æœˆ]\d{1,2}[æ—¥]?)',
                r'æŠ¥åæˆªæ­¢[ï¼š:]\s*(\d{4}[./å¹´]\d{1,2}[./æœˆ]\d{1,2}[æ—¥]?)',
                r'ç”³è¯·æˆªæ­¢[ï¼š:]\s*(\d{4}[./å¹´]\d{1,2}[./æœˆ]\d{1,2}[æ—¥]?)',
                r'æ‹›è˜æˆªæ­¢[ï¼š:]\s*(\d{4}[./å¹´]\d{1,2}[./æœˆ]\d{1,2}[æ—¥]?)',
                r'æˆªæ­¢æ—¶é—´[ï¼š:]\s*(\d{4}[./å¹´]\d{1,2}[./æœˆ]\d{1,2}[æ—¥]?)',
            ]
            
            for i, pattern in enumerate(deadline_patterns):
                matches = re.findall(pattern, text, re.IGNORECASE)
                if matches:
                    deadline = matches[0]
                    info["æ‹›è˜æˆªæ­¢æ—¥æœŸ"] = deadline
                    print(f"â° æå–æˆªæ­¢æ—¥æœŸ: {deadline} (æ¨¡å¼{i+1})")
                    break
            
            # 9. æ–°å¢ï¼šæ‹›å‹Ÿæ–¹å‘æå–ï¼ˆç®€å•æ­£åˆ™ï¼‰
            direction_patterns = [
                r'æ‹›å‹Ÿæ–¹å‘[ï¼š:]\s*([^ã€‚\n]+)',
                r'æ–¹å‘[ï¼š:]\s*([^ã€‚\n]*æ–¹å‘[^ã€‚\n]*)',
                r'æŠ€æœ¯æ–¹å‘[ï¼š:]\s*([^ã€‚\n]+)',
                r'([^ã€‚\n]*æ–¹å‘[ã€ï¼Œ,][^ã€‚\n]*æ–¹å‘[^ã€‚\n]*)',
            ]
            
            for i, pattern in enumerate(direction_patterns):
                matches = re.findall(pattern, text, re.IGNORECASE)
                if matches:
                    direction = matches[0].strip()
                    if len(direction) > 10 and 'æ–¹å‘' in direction:  # ç¡®ä¿æ˜¯æœ‰æ„ä¹‰çš„æ–¹å‘æè¿°
                        info["æ‹›å‹Ÿæ–¹å‘"] = direction
                        print(f"ğŸ¯ æå–æ‹›å‹Ÿæ–¹å‘: {direction} (æ¨¡å¼{i+1})")
                        break
            
        except Exception as e:
            print(f"âš ï¸  ç»“æ„åŒ–æå–å¤±è´¥: {e}")
        
        return info
    
    def _prepare_html_for_llm(self, html: str) -> str:
        """ä¸ºLLMå‡†å¤‡HTMLï¼Œé‡ç‚¹ä¿ç•™å²—ä½æè¿°ç›¸å…³å†…å®¹"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # ç§»é™¤æ˜æ˜¾å™ªå£°
            for elem in soup(['script', 'style', 'nav', 'footer', 'header']):
                elem.decompose()
            
            # æŸ¥æ‰¾ä¸»è¦å²—ä½æè¿°åŒºåŸŸ
            job_content_selectors = [
                '[class*="job-detail"]',
                '[class*="job-description"]', 
                '[class*="position-detail"]',
                '[class*="job-content"]',
                '[class*="desc"]'
            ]
            
            main_content = ""
            for selector in job_content_selectors:
                elements = soup.select(selector)
                if elements:
                    main_content = "\n".join([elem.get_text(separator='\n') for elem in elements])
                    break
            
            if not main_content:
                main_content = soup.get_text(separator='\n')
            
            # æ¸…ç†å’Œè¿‡æ»¤
            lines = main_content.split('\n')
            cleaned_lines = []
            
            for line in lines:
                line = line.strip()
                if (len(line) > 5 and 
                    line not in cleaned_lines[-3:] and
                    not re.match(r'^[>\sâ€¢Â·\-\*\.]+$', line) and
                    'ä¸¾æŠ¥' not in line and 'å®¢æœ' not in line and 
                    'æ‰«ç ' not in line and 'å¾®ä¿¡' not in line):
                    cleaned_lines.append(line)
            
            content = '\n'.join(cleaned_lines)
            return content[:6000]
            
        except Exception as e:
            print(f"âš ï¸  HTMLé¢„å¤„ç†å¤±è´¥: {e}")
            return re.sub(r'<[^>]+>', ' ', html)[:6000]
    
    async def _call_llm_api(self, messages: list, max_retries: int = 3) -> Optional[str]:
        """è°ƒç”¨LLM API"""
        if not self.api_key:
            print(f"[ERROR] {self.provider} API key not configured")
            return None
            
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": self.model,
            "messages": messages,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens
        }
        
        if self.provider == "zhipu":
            data["stream"] = False
        
        for attempt in range(max_retries):
            try:
                async with httpx.AsyncClient(timeout=60.0) as client:
                    response = await client.post(
                        f"{self.base_url}/chat/completions",
                        headers=headers,
                        json=data
                    )
                    
                    if response.status_code == 200:
                        result = response.json()
                        content = result["choices"][0]["message"]["content"]
                        return content
                    else:
                        print(f"âš ï¸  APIé”™è¯¯: {response.status_code}")
                        
            except Exception as e:
                print(f"âš ï¸  APIè°ƒç”¨å¼‚å¸¸ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                
            if attempt < max_retries - 1:
                await asyncio.sleep(2 ** attempt)
        
        return None
    
    async def extract_for_notion_enhanced(self, html: str, url: str, job_data: Optional[Dict] = None) -> Optional[Dict[str, Any]]:
        """å¢å¼ºç‰ˆNotionæå–æ–¹æ³•"""
        if not html or not html.strip():
            return None
        
        print(f"ğŸ”„ å¼€å§‹å¢å¼ºæå–: {url}")
        
        # 1. ç»“æ„åŒ–æå–
        structured_info = self._extract_structured_info(html, url, job_data)
        
        # 2. LLMæå–å²—ä½æè¿°å’Œå…¬å¸åç§°ï¼Œä»¥åŠè¡¥å……æ‹›å‹Ÿæ–¹å‘
        processed_html = self._prepare_html_for_llm(html)
        
        prompt = f"""
è¯·ä»ä»¥ä¸‹æ‹›è˜é¡µé¢å†…å®¹ä¸­æå–ä¿¡æ¯ã€‚æ³¨æ„ï¼šåªæå–æ‹›è˜å²—ä½çš„ä¿¡æ¯ï¼Œä¸è¦æå–HRçš„ä¸ªäººä¿¡æ¯ã€‚

é¡µé¢å†…å®¹ï¼š
{processed_html}

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼Œä»¥JSONæ ¼å¼è¿”å›ï¼š

1. **å²—ä½æè¿°**ï¼šè¯¦ç»†çš„å·¥ä½œèŒè´£å’ŒæŠ€èƒ½è¦æ±‚ï¼ŒåŒ…æ‹¬ï¼š
   - å…·ä½“çš„å·¥ä½œå†…å®¹å’ŒèŒè´£
   - æŠ€èƒ½è¦æ±‚å’ŒæŠ€æœ¯æ ˆ
   - ä»»èŒè¦æ±‚å’Œæ¡ä»¶
   æ³¨æ„ï¼šåªè¦æ ¸å¿ƒå²—ä½å†…å®¹ï¼Œå»é™¤å…¬å¸ä»‹ç»ã€ç¦åˆ©å¾…é‡ã€è”ç³»æ–¹å¼ç­‰
   
2. **å…¬å¸åç§°**ï¼šæ‹›è˜å…¬å¸çš„å‡†ç¡®åç§°

3. **å‘å¸ƒæ—¥æœŸ**ï¼šå¦‚æœé¡µé¢ä¸­æ˜ç¡®æ˜¾ç¤ºå²—ä½å‘å¸ƒæ—¶é—´ï¼Œæå–æ ¼å¼ä¸ºYYYY-MM-DDï¼Œæ²¡æœ‰åˆ™ä¸ºç©ºå­—ç¬¦ä¸²
   é‡è¦æé†’ï¼š
   - åªè¦çœŸæ­£çš„å²—ä½å‘å¸ƒæ—¥æœŸï¼Œä¸è¦å…¬å¸æˆç«‹æ—¥æœŸ
   - ä¸è¦HRæ³¨å†Œæ—¶é—´ã€å…¬å¸åˆ›å»ºæ—¶é—´ã€æ›´æ–°æ—¶é—´
   - ä¸è¦ä»»ä½•éå²—ä½ç›¸å…³çš„æ—¥æœŸ
   - å¦‚æœä¸ç¡®å®šæ˜¯å¦ä¸ºå²—ä½å‘å¸ƒæ—¥æœŸï¼Œè¯·è®¾ä¸ºç©ºå­—ç¬¦ä¸²
   
4. **å‘å¸ƒæ—¥æœŸæ¥æº**ï¼šè¯´æ˜ä½ ä»é¡µé¢çš„å“ªä¸ªéƒ¨åˆ†æå–åˆ°å‘å¸ƒæ—¥æœŸï¼Œå¿…é¡»æ˜ç¡®æ˜¯å²—ä½å‘å¸ƒç›¸å…³ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°çœŸæ­£çš„å²—ä½å‘å¸ƒæ—¥æœŸåˆ™ä¸ºç©ºå­—ç¬¦ä¸²

5. **æ‹›å‹Ÿæ–¹å‘**ï¼šå¦‚æœé¡µé¢ä¸­æåˆ°å…·ä½“çš„æŠ€æœ¯æ–¹å‘æˆ–æ‹›å‹Ÿæ–¹å‘ï¼Œè¯·æå–å‡ºæ¥ã€‚å¦‚é¢„è®­ç»ƒæ–¹å‘ã€å¤§æ•°æ®æ–¹å‘ã€åˆ›æ–°æ–¹å‘ã€å¤šæ¨¡æ€æ–¹å‘ç­‰ã€‚æ²¡æœ‰åˆ™ä¸ºç©ºå­—ç¬¦ä¸²ã€‚

è¦æ±‚ï¼š
- ä¸“æ³¨äºæ‹›è˜å²—ä½çš„æ ¸å¿ƒä¿¡æ¯
- å²—ä½æè¿°è¦å®Œæ•´ä½†ç®€æ´ï¼Œçªå‡ºå…³é”®èŒè´£å’ŒæŠ€èƒ½
- å¯¹å‘å¸ƒæ—¥æœŸè¦ç‰¹åˆ«è°¨æ…ï¼Œå®å¯ä¸ºç©ºä¹Ÿä¸è¦é”™è¯¯çš„æ—¥æœŸ
- å¦‚æœå­—æ®µä¸å­˜åœ¨åˆ™è®¾ä¸ºç©ºå­—ç¬¦ä¸²
- åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦å…¶ä»–æ–‡å­—

è¿”å›æ ¼å¼ï¼š
{{
  "å²—ä½æè¿°": "è¯¦ç»†çš„å²—ä½èŒè´£å’ŒæŠ€èƒ½è¦æ±‚...",
  "å…¬å¸åç§°": "å…¬å¸åç§°", 
  "å‘å¸ƒæ—¥æœŸ": "YYYY-MM-DDæˆ–ç©ºå­—ç¬¦ä¸²",
  "å‘å¸ƒæ—¥æœŸæ¥æº": "æ˜ç¡®è¯´æ˜ä»é¡µé¢å“ªé‡Œæå–åˆ°å²—ä½å‘å¸ƒæ—¥æœŸï¼Œæ²¡æœ‰åˆ™ä¸ºç©º",
  "æ‹›å‹Ÿæ–¹å‘": "å…·ä½“çš„æŠ€æœ¯æ–¹å‘æˆ–æ‹›å‹Ÿæ–¹å‘ï¼Œæ²¡æœ‰åˆ™ä¸ºç©º"
}}
"""
        
        messages = [{"role": "user", "content": prompt}]
        
        try:
            content = await self._call_llm_api(messages)
            
            if content:
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    json_str = json_match.group()
                    llm_data = json.loads(json_str)
                    
                    # åˆå¹¶ç»“æ„åŒ–æå–å’ŒLLMæå–çš„ç»“æœ
                    raw_graduation_req = structured_info.get("æ¯•ä¸šæ—¶é—´è¦æ±‚", "")
                    raw_deadline = structured_info.get("æ‹›è˜æˆªæ­¢æ—¥æœŸ", "")
                    
                    # æ—¥æœŸæ ‡å‡†åŒ–
                    standardized_graduation = self.standardize_date_format(raw_graduation_req) if raw_graduation_req else ""
                    standardized_deadline = self.standardize_date_format(raw_deadline) if raw_deadline else ""
                    
                    # æ‹›å‹Ÿæ–¹å‘åˆå¹¶ï¼ˆä¼˜å…ˆä½¿ç”¨LLMæå–çš„ï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨æ­£åˆ™æå–çš„ï¼‰
                    recruitment_direction = llm_data.get("æ‹›å‹Ÿæ–¹å‘", "") or structured_info.get("æ‹›å‹Ÿæ–¹å‘", "")
                    
                    final_result = {
                        "å²—ä½åç§°": structured_info.get("å²—ä½åç§°", ""),
                        "å²—ä½æè¿°": llm_data.get("å²—ä½æè¿°", ""),
                        "å‘å¸ƒæ—¥æœŸ": llm_data.get("å‘å¸ƒæ—¥æœŸ", ""),
                        "å‘å¸ƒæ—¥æœŸæ¥æº": llm_data.get("å‘å¸ƒæ—¥æœŸæ¥æº", ""),
                        "å‘å¸ƒå¹³å°": structured_info.get("å‘å¸ƒå¹³å°", ""),
                        "HRæ´»è·ƒåº¦": structured_info.get("HRæ´»è·ƒåº¦", ""),
                        "å…¬å¸åç§°": llm_data.get("å…¬å¸åç§°", ""),
                        "è–ªèµ„": structured_info.get("è–ªèµ„", ""),
                        "ç»éªŒè¦æ±‚": structured_info.get("ç»éªŒè¦æ±‚", ""),
                        "å·¥ä½œåœ°ç‚¹": structured_info.get("å·¥ä½œåœ°ç‚¹", ""),
                        "å²—ä½é“¾æ¥": url,
                        "é¡µé¢æŠ“å–æ—¶é—´": structured_info.get("é¡µé¢æŠ“å–æ—¶é—´", ""),
                        
                        # æ–°å¢å­—æ®µ
                        "æ¯•ä¸šæ—¶é—´è¦æ±‚": raw_graduation_req,
                        "æ¯•ä¸šæ—¶é—´è¦æ±‚_æ ‡å‡†åŒ–": standardized_graduation,
                        "æ¯•ä¸šæ—¶é—´_åŒ¹é…çŠ¶æ€": self.check_graduation_eligibility(raw_graduation_req),
                        "æ‹›è˜æˆªæ­¢æ—¥æœŸ": raw_deadline,
                        "æ‹›è˜æˆªæ­¢æ—¥æœŸ_æ ‡å‡†åŒ–": standardized_deadline,
                        "æ‹›è˜æˆªæ­¢æ—¥æœŸ_çŠ¶æ€": self.check_deadline_status(standardized_deadline),
                        "æ‹›å‹Ÿæ–¹å‘": recruitment_direction,
                        
                        "æå–æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    }
                    
                    # è°ƒè¯•ä¿¡æ¯
                    print(f"[OK] Enhanced extraction completed:")
                    print(f"   å²—ä½åç§°: {final_result.get('å²—ä½åç§°', 'N/A')}")
                    print(f"   è–ªèµ„: {final_result.get('è–ªèµ„', 'N/A')}")
                    print(f"   åœ°ç‚¹: {final_result.get('å·¥ä½œåœ°ç‚¹', 'N/A')}")
                    print(f"   ç»éªŒ: {final_result.get('ç»éªŒè¦æ±‚', 'N/A')}")
                    print(f"   ğŸ“ æ¯•ä¸šæ—¶é—´è¦æ±‚: {final_result.get('æ¯•ä¸šæ—¶é—´è¦æ±‚', 'N/A')}")
                    print(f"   ğŸ“Š åŒ¹é…çŠ¶æ€: {final_result.get('æ¯•ä¸šæ—¶é—´_åŒ¹é…çŠ¶æ€', 'N/A')}")
                    print(f"   â° æ‹›è˜æˆªæ­¢æ—¥æœŸ: {final_result.get('æ‹›è˜æˆªæ­¢æ—¥æœŸ', 'N/A')} -> {final_result.get('æ‹›è˜æˆªæ­¢æ—¥æœŸ_æ ‡å‡†åŒ–', 'N/A')}")
                    print(f"   ğŸ“ˆ æˆªæ­¢çŠ¶æ€: {final_result.get('æ‹›è˜æˆªæ­¢æ—¥æœŸ_çŠ¶æ€', 'N/A')}")
                    print(f"   ğŸ¯ æ‹›å‹Ÿæ–¹å‘: {final_result.get('æ‹›å‹Ÿæ–¹å‘', 'N/A')}")
                    
                    return final_result
                    
        except Exception as e:
            print(f"âš ï¸  LLMæå–å¤±è´¥: {e}")
        
        # å¦‚æœLLMå¤±è´¥ï¼Œè¿”å›ç»“æ„åŒ–æå–çš„ç»“æœ
        raw_graduation_req = structured_info.get("æ¯•ä¸šæ—¶é—´è¦æ±‚", "")
        raw_deadline = structured_info.get("æ‹›è˜æˆªæ­¢æ—¥æœŸ", "")
        standardized_graduation = self.standardize_date_format(raw_graduation_req) if raw_graduation_req else ""
        standardized_deadline = self.standardize_date_format(raw_deadline) if raw_deadline else ""
        
        fallback_result = {
            "å²—ä½åç§°": structured_info.get("å²—ä½åç§°", ""),
            "å²—ä½æè¿°": "æš‚æ— è¯¦ç»†æè¿°",
            "å‘å¸ƒæ—¥æœŸ": "",
            "å‘å¸ƒæ—¥æœŸæ¥æº": "",
            "å‘å¸ƒå¹³å°": structured_info.get("å‘å¸ƒå¹³å°", ""),
            "HRæ´»è·ƒåº¦": structured_info.get("HRæ´»è·ƒåº¦", ""),
            "å…¬å¸åç§°": "",
            "è–ªèµ„": structured_info.get("è–ªèµ„", ""),
            "ç»éªŒè¦æ±‚": structured_info.get("ç»éªŒè¦æ±‚", ""),
            "å·¥ä½œåœ°ç‚¹": structured_info.get("å·¥ä½œåœ°ç‚¹", ""),
            "å²—ä½é“¾æ¥": url,
            "é¡µé¢æŠ“å–æ—¶é—´": structured_info.get("é¡µé¢æŠ“å–æ—¶é—´", ""),
            
            # æ–°å¢å­—æ®µï¼ˆä»…ç»“æ„åŒ–æå–ï¼‰
            "æ¯•ä¸šæ—¶é—´è¦æ±‚": raw_graduation_req,
            "æ¯•ä¸šæ—¶é—´è¦æ±‚_æ ‡å‡†åŒ–": standardized_graduation,
            "æ¯•ä¸šæ—¶é—´_åŒ¹é…çŠ¶æ€": self.check_graduation_eligibility(raw_graduation_req),
            "æ‹›è˜æˆªæ­¢æ—¥æœŸ": raw_deadline,
            "æ‹›è˜æˆªæ­¢æ—¥æœŸ_æ ‡å‡†åŒ–": standardized_deadline,
            "æ‹›è˜æˆªæ­¢æ—¥æœŸ_çŠ¶æ€": self.check_deadline_status(standardized_deadline),
            "æ‹›å‹Ÿæ–¹å‘": structured_info.get("æ‹›å‹Ÿæ–¹å‘", ""),
            
            "æå–æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        return fallback_result

# æµ‹è¯•å¢å¼ºç‰ˆæå–å™¨
async def test_enhanced_extractor():
    """æµ‹è¯•å¢å¼ºç‰ˆæå–å™¨"""
    import glob
    
    print("ğŸ§ª æµ‹è¯•å¢å¼ºç‰ˆNotionæå–å™¨")
    print("=" * 80)
    
    # æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
    data_patterns = ["../../data/raw_boss_playwright_*.jsonl", "data/raw_boss_playwright_*.jsonl"]
    data_files = []
    for pattern in data_patterns:
        data_files.extend(glob.glob(pattern, recursive=True))
    
    if not data_files:
        print("[ERROR] No data files found")
        return
    
    data_file = max(data_files, key=os.path.getmtime)
    print(f"ğŸ“ ä½¿ç”¨æ•°æ®æ–‡ä»¶: {data_file}")
    
    extractor = EnhancedNotionExtractor()
    results = []
    
    with open(data_file, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= 3:  # æµ‹è¯•å‰3ä¸ª
                break
            
            try:
                job_data = json.loads(line.strip())
                html = job_data.get('html', '')
                url = job_data.get('url', '')
                
                if html:
                    print(f"\n{'='*20} æµ‹è¯•å²—ä½ {i+1}/3 {'='*20}")
                    result = await extractor.extract_for_notion_enhanced(html, url, job_data)
                    
                    if result:
                        results.append(result)
                    
                    await asyncio.sleep(2)
                    
            except Exception as e:
                print(f"âš ï¸  å¤„ç†å¤±è´¥: {e}")
    
    # ä¿å­˜ç»“æœ
    if results:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"enhanced_notion_jobs_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å¢å¼ºç»“æœå·²ä¿å­˜: {output_file}")
        print(f"ğŸ“Š æ–°å¢å­—æ®µç»Ÿè®¡:")
        
        new_fields = ["æ¯•ä¸šæ—¶é—´è¦æ±‚", "æ¯•ä¸šæ—¶é—´_åŒ¹é…çŠ¶æ€", "æ‹›è˜æˆªæ­¢æ—¥æœŸ", "æ‹›è˜æˆªæ­¢æ—¥æœŸ_çŠ¶æ€", "æ‹›å‹Ÿæ–¹å‘"]
        for field in new_fields:
            non_empty = sum(1 for job in results if job.get(field, '').strip())
            print(f"   {field}: {non_empty}/{len(results)}")
        
        # æ˜¾ç¤ºåŒ¹é…çŠ¶æ€ç»Ÿè®¡
        match_statuses = {}
        for job in results:
            status = job.get('æ¯•ä¸šæ—¶é—´_åŒ¹é…çŠ¶æ€', 'æœªçŸ¥')
            match_statuses[status] = match_statuses.get(status, 0) + 1
        
        print(f"\nğŸ¯ æ¯•ä¸šæ—¶é—´åŒ¹é…çŠ¶æ€åˆ†å¸ƒ:")
        for status, count in match_statuses.items():
            print(f"   {status}: {count}ä¸ª")

if __name__ == "__main__":
    asyncio.run(test_enhanced_extractor())
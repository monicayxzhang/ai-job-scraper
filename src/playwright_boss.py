import asyncio
import json
import random
import time
from datetime import datetime
from typing import List, Dict, Optional
from playwright.async_api import async_playwright, Page, Browser
import os

async def setup_page(page: Page) -> None:
    """é…ç½®é¡µé¢ä»¥é¿å…æ£€æµ‹"""
    # è®¾ç½®ç”¨æˆ·ä»£ç† - ä¿®å¤APIè°ƒç”¨
    await page.set_extra_http_headers({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    })
    
    # è®¾ç½®è§†å£
    await page.set_viewport_size({"width": 1920, "height": 1080})
    
    # æ·»åŠ ä¸€äº›åæ£€æµ‹è„šæœ¬
    await page.add_init_script("""
        // è¦†ç›–webdriverå±æ€§
        Object.defineProperty(navigator, 'webdriver', {
            get: () => undefined,
        });
        
        // è¦†ç›–pluginsé•¿åº¦
        Object.defineProperty(navigator, 'plugins', {
            get: () => [1, 2, 3, 4, 5],
        });
        
        // è¦†ç›–è¯­è¨€
        Object.defineProperty(navigator, 'languages', {
            get: () => ['zh-CN', 'zh', 'en'],
        });
        
        // è¦†ç›–æƒé™
        const originalQuery = window.navigator.permissions.query;
        window.navigator.permissions.query = (parameters) => (
            parameters.name === 'notifications' ?
                Promise.resolve({ state: Notification.permission }) :
                originalQuery(parameters)
        );
    """)

async def human_like_delay():
    """æ¨¡æ‹Ÿäººç±»æ“ä½œçš„éšæœºå»¶è¿Ÿ"""
    await asyncio.sleep(random.uniform(1.5, 3.5))

async def scroll_page(page: Page):
    """æ¨¡æ‹Ÿäººç±»æ»šåŠ¨è¡Œä¸º"""
    # éšæœºæ»šåŠ¨
    for _ in range(random.randint(2, 5)):
        await page.mouse.wheel(0, random.randint(200, 800))
        await asyncio.sleep(random.uniform(0.5, 1.5))

async def extract_job_info_from_page(page: Page) -> List[Dict[str, str]]:
    """ä»é¡µé¢æå–å²—ä½ä¿¡æ¯"""
    jobs = []
    
    try:
        # ç­‰å¾…å²—ä½åˆ—è¡¨åŠ è½½ - ä¿®å¤é€‰æ‹©å™¨
        await page.wait_for_selector('.rec-job-list', timeout=10000)
        
        # æå–å²—ä½ä¿¡æ¯ - ä½¿ç”¨æ­£ç¡®çš„é€‰æ‹©å™¨
        job_elements = await page.query_selector_all('.job-card-wrap')
        
        print(f"ğŸ“„ é¡µé¢æ‰¾åˆ° {len(job_elements)} ä¸ªå²—ä½å…ƒç´ ")
        
        for job_element in job_elements:
            try:
                # æå–åŸºæœ¬ä¿¡æ¯ - æ ¹æ®å®é™…HTMLç»“æ„è°ƒæ•´
                job_name_elem = await job_element.query_selector('.job-name')
                company_name_elem = await job_element.query_selector('.boss-name')
                salary_elem = await job_element.query_selector('.job-salary')
                location_elem = await job_element.query_selector('.company-location')
                tag_elements = await job_element.query_selector_all('.tag-list li')
                
                # è·å–å²—ä½é“¾æ¥
                job_url = ""
                if job_name_elem:
                    job_url = await job_name_elem.get_attribute('href')
                    if job_url and not job_url.startswith('http'):
                        job_url = f"https://www.zhipin.com{job_url}"
                
                # æå–æ–‡æœ¬å†…å®¹
                job_name = await job_name_elem.inner_text() if job_name_elem else ""
                company_name = await company_name_elem.inner_text() if company_name_elem else ""
                salary = await salary_elem.inner_text() if salary_elem else ""
                location = await location_elem.inner_text() if location_elem else ""
                
                # æå–æ ‡ç­¾ä¿¡æ¯ï¼ˆç»éªŒè¦æ±‚ã€å­¦å†ç­‰ï¼‰
                tags = []
                for tag_elem in tag_elements:
                    tag_text = await tag_elem.inner_text()
                    if tag_text:
                        tags.append(tag_text.strip())
                
                if job_name and company_name:  # ç¡®ä¿æœ‰åŸºæœ¬ä¿¡æ¯
                    job_info = {
                        "job_name": job_name.strip(),
                        "company_name": company_name.strip(),
                        "salary_desc": salary.strip(),
                        "location": location.strip(),
                        "tags": ", ".join(tags),
                        "url": job_url
                    }
                    jobs.append(job_info)
                    
            except Exception as e:
                print(f"âš ï¸  æå–å•ä¸ªå²—ä½ä¿¡æ¯å¤±è´¥: {e}")
                continue
                
    except Exception as e:
        print(f"âŒ æå–å²—ä½ä¿¡æ¯å¤±è´¥: {e}")
    
    return jobs

async def get_job_detail_html(page: Page, job_url: str) -> Optional[str]:
    """è·å–å²—ä½è¯¦æƒ…é¡µHTML"""
    try:
        print(f"ğŸ” è®¿é—®å²—ä½è¯¦æƒ…: {job_url}")
        
        # é™ä½ç­‰å¾…è¦æ±‚ï¼Œå¢åŠ è¶…æ—¶æ—¶é—´
        await page.goto(job_url, wait_until='domcontentloaded', timeout=45000)
        await human_like_delay()
        
        # ç­‰å¾…é¡µé¢åŠ è½½ï¼Œä½¿ç”¨æ›´å®½æ¾çš„æ¡ä»¶
        try:
            await page.wait_for_selector('.job-detail', timeout=5000)
        except:
            # å¦‚æœç‰¹å®šå…ƒç´ åŠ è½½å¤±è´¥ï¼Œä»ç„¶å°è¯•è·å–é¡µé¢å†…å®¹
            print("âš ï¸  å²—ä½è¯¦æƒ…å…ƒç´ åŠ è½½è¶…æ—¶ï¼Œå°è¯•è·å–é¡µé¢å†…å®¹")
        
        # è·å–é¡µé¢HTML
        html = await page.content()
        return html
        
    except Exception as e:
        print(f"âš ï¸  è·å–è¯¦æƒ…é¡µå¤±è´¥ {job_url}: {e}")
        return None

async def fetch_boss_jobs_playwright(keyword: str, city: str = "101010100", max_pages: int = 2, max_jobs_test: Optional[int] = None) -> List[Dict[str, str]]:
    """ä½¿ç”¨Playwrightä»Bossç›´è˜è·å–å²—ä½ä¿¡æ¯
        
    Args:
        keyword: æœç´¢å…³é”®è¯
        city: åŸå¸‚ä»£ç 
        max_pages: æœ€å¤§é¡µæ•°
        max_jobs_test: æµ‹è¯•æ¨¡å¼ä¸‹æ¯é¡µæœ€å¤šå¤„ç†çš„å²—ä½æ•°é‡ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰å²—ä½
    """
    print(f"ğŸš€ å¯åŠ¨Playwrightçˆ¬è™«: {keyword}")
    if max_jobs_test:
        print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼: æ¯é¡µæœ€å¤šå¤„ç† {max_jobs_test} ä¸ªå²—ä½")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs("data", exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    raw_file = f"data/raw_boss_playwright_{timestamp}.jsonl"
    
    jobs = []
    
    async with async_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨ï¼ˆä½¿ç”¨Chromiumï¼‰
        browser = await p.chromium.launch(
            headless=False,  # è®¾ä¸ºTrueå¯ä»¥æ— ç•Œé¢è¿è¡Œ
            args=[
                '--no-sandbox',
                '--disable-blink-features=AutomationControlled',
                '--disable-infobars',
                '--disable-extensions',
                '--disable-dev-shm-usage',
                '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            ]
        )
        
        try:
            # åˆ›å»ºé¡µé¢
            page = await browser.new_page()
            await setup_page(page)
            
            # æ„é€ æœç´¢URL
            from urllib.parse import quote
            encoded_keyword = quote(keyword)
            base_url = f"https://www.zhipin.com/web/geek/job?query={encoded_keyword}&city={city}"
            
            for page_num in range(1, max_pages + 1):
                print(f"\nğŸ” æ­£åœ¨çˆ¬å–ç¬¬ {page_num} é¡µ...")
                
                # è®¿é—®æœç´¢é¡µé¢
                search_url = f"{base_url}&page={page_num}"
                print(f"è®¿é—®URL: {search_url}")
                
                try:
                    # å¢åŠ è¶…æ—¶æ—¶é—´ï¼Œé™ä½ç­‰å¾…æ ‡å‡†
                    await page.goto(search_url, wait_until='domcontentloaded', timeout=60000)
                    print("âœ… é¡µé¢åŠ è½½æˆåŠŸ")
                except Exception as e:
                    print(f"âš ï¸  é¡µé¢åŠ è½½å¤±è´¥ï¼Œå°è¯•é‡æ–°åŠ è½½: {e}")
                    try:
                        # å†æ¬¡å°è¯•ï¼Œä½¿ç”¨æ›´å®½æ¾çš„ç­‰å¾…æ¡ä»¶
                        await page.goto(search_url, wait_until='load', timeout=45000)
                        print("âœ… é‡æ–°åŠ è½½æˆåŠŸ")
                    except Exception as e2:
                        print(f"âŒ é‡æ–°åŠ è½½ä¹Ÿå¤±è´¥: {e2}")
                        # ä¿å­˜é”™è¯¯é¡µé¢
                        error_file = f"data/error_page_{page_num}_{timestamp}.html"
                        try:
                            content = await page.content()
                            with open(error_file, "w", encoding="utf-8") as f:
                                f.write(content)
                            print(f"å·²ä¿å­˜é”™è¯¯é¡µé¢åˆ°: {error_file}")
                        except:
                            pass
                        continue
                
                await human_like_delay()
                
                # æ£€æŸ¥æ˜¯å¦æœ‰åçˆ¬è™«é¡µé¢
                page_title = await page.title()
                page_content = await page.content()
                
                if "å¼‚å¸¸" in page_title or "éªŒè¯" in page_content or "åŠ è½½ä¸­" in page_content:
                    print("âš ï¸  é‡åˆ°åçˆ¬è™«é¡µé¢ï¼Œå°è¯•ç­‰å¾…...")
                    await asyncio.sleep(10)
                    
                    # å°è¯•åˆ·æ–°
                    await page.reload(wait_until='networkidle')
                    await human_like_delay()
                
                # æ¨¡æ‹Ÿäººç±»è¡Œä¸º
                await scroll_page(page)
                await human_like_delay()
                
                # æå–å½“å‰é¡µé¢çš„å²—ä½ä¿¡æ¯
                page_jobs = await extract_job_info_from_page(page)
                
                if not page_jobs:
                    print(f"âš ï¸  ç¬¬ {page_num} é¡µæ²¡æœ‰æ‰¾åˆ°å²—ä½ä¿¡æ¯")
                    
                    # ä¿å­˜è°ƒè¯•ä¿¡æ¯
                    debug_file = f"data/debug_playwright_page_{page_num}_{timestamp}.html"
                    with open(debug_file, "w", encoding="utf-8") as f:
                        f.write(await page.content())
                    print(f"å·²ä¿å­˜è°ƒè¯•é¡µé¢åˆ°: {debug_file}")
                    continue

                # åº”ç”¨æµ‹è¯•æ¨¡å¼é™åˆ¶
                if max_jobs_test and len(page_jobs) > max_jobs_test:
                    page_jobs = page_jobs[:max_jobs_test]
                    print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼: é™åˆ¶ä¸ºå‰ {max_jobs_test} ä¸ªå²—ä½")
                
                print(f"âœ… ç¬¬ {page_num} é¡µæå–åˆ° {len(page_jobs)} ä¸ªå²—ä½")
                
                # å¤„ç†æ¯ä¸ªå²—ä½
                for i, job_info in enumerate(page_jobs):
                    print(f"å¤„ç†å²—ä½ {i+1}/{len(page_jobs)}: {job_info['job_name']}")
                    
                    # è·å–è¯¦æƒ…é¡µHTMLï¼ˆå¯é€‰ï¼Œè€—æ—¶è¾ƒé•¿ï¼‰
                    job_html = ""
                    if job_info.get("url"):
                        # åœ¨æµ‹è¯•æ¨¡å¼ä¸‹ï¼Œè·å–æ‰€æœ‰å²—ä½çš„è¯¦æƒ…é¡µï¼ˆå› ä¸ºæ•°é‡å·²ç»è¢«é™åˆ¶äº†ï¼‰
                        job_html = await get_job_detail_html(page, job_info["url"])
                        await human_like_delay()
                    
                    # ä¿å­˜å²—ä½æ•°æ®
                    job_record = {
                        "url": job_info.get("url", ""),
                        "html": job_html,
                        "api_data": job_info,
                        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "source": "Bossç›´è˜Playwright"
                    }
                    
                    jobs.append(job_record)
                    
                    # å®æ—¶ä¿å­˜
                    with open(raw_file, "a", encoding="utf-8") as f:
                        f.write(json.dumps(job_record, ensure_ascii=False) + "\n")
                    
                    print(f"âœ… ä¿å­˜: {job_info['job_name']} - {job_info['company_name']}")
                
                # é¡µé¢é—´å»¶è¿Ÿ
                print(f"â³ ç­‰å¾…åç»§ç»­ä¸‹ä¸€é¡µ...")
                await asyncio.sleep(random.uniform(3, 6))
        
        finally:
            await browser.close()
    
    print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼")
    print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {raw_file}")
    print(f"ğŸ“Š æ€»å…±è·å– {len(jobs)} ä¸ªå²—ä½")
    
    return jobs

# æµ‹è¯•å‡½æ•°
if __name__ == "__main__":
    async def test():
        try:
            results = await fetch_boss_jobs_playwright(
                keyword="å¤§æ¨¡å‹ ç®—æ³•", 
                city="101010100",  # åŒ—äº¬
                max_pages=1
            )
            
            print(f"\nğŸ“‹ å²—ä½é¢„è§ˆ:")
            for i, job in enumerate(results[:5]):
                api_data = job.get("api_data", {})
                print(f"{i+1}. {api_data.get('job_name', 'N/A')} - {api_data.get('company_name', 'N/A')}")
                print(f"   è–ªèµ„: {api_data.get('salary_desc', 'N/A')}")
                print(f"   åœ°ç‚¹: {api_data.get('location', 'N/A')}")
                print(f"   URL: {job.get('url', 'N/A')}")
                print()
                
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    asyncio.run(test())